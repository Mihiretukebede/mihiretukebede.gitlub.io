[
  {
    "path": "posts/2023-03-30-quantile-regression/",
    "title": "Quantile regression tutorial",
    "description": "This is a quick demonstration of quantile regression in R",
    "author": [
      {
        "name": "Mihiretu Kebede(PhD)",
        "url": {}
      }
    ],
    "date": "2023-03-30",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nWhat is qauntile regression?\r\nRead Datasets\r\n\r\nQauntile regression\r\nHow about if we want to access the coefficients?\r\n\r\nVizualize\r\nContact\r\n\r\n\r\nMotivation\r\nAround a year ago, a subscriber on my YouTube channel requested that I create a video on “Quantile regression”. Although I had no prior experience with this regression technique, I accepted the assignment and began researching the topic. After gaining a basic understanding of the technique, I created a video which was well-received by my audience, and has since become one of my most viewed videos. You can watch the video below. Several viewers have also requested that I make the code available, and I apologize for the delay in doing so due to time constraints. Better than never, I will share you the code today. Thanks for your patience.\r\n\r\n\r\n\r\nWhat is qauntile regression?\r\nQuantile regression is a statistical technique used to model the relationship between a response variable and one or more predictor variables. Unlike traditional regression models that estimate the mean of the response variable given the predictor variables, quantile regression estimates the conditional quantiles of the response variable.\r\nAn equal probability portion of a distribution is represented by a quantile. When the median is the 50th percentile, for instance, 50% of the distribution’s values fall below the median and 50% fall above it. We can estimate any conditional quantile of the response variable in quantile regression, not only the median.\r\nQuantile regression’s primary benefit is that it enables us to model the relationship between the predictor variables across different parts of the distribution of the response variable. This can be useful when the relationship between the predictor variables and the response variable is not constant across the distribution, for example, when the relationship is stronger in the tails of the distribution.\r\nQuantile regression can be a good choice if the data violate some of the key assumptions in ordinary least square regression (OLS): domesticity and normality. It is also robust to outliers or influential data points.\r\nWithout spending more time, I will proceed directly to the codes. I need few libraries and I will use the life_expectancy data set. I will use quantreg package to do the models(life expactancy as a function of income) and plotly to visualize the coefficients in 3D.\r\n\r\n\r\nlibrary(pacman)\r\np_load(dplyr,janitor, ggplot2, quantreg,sjPlot, plotly)\r\n\r\n\r\nRead Datasets\r\n\r\n\r\nlife_exp  <- read.csv(\"life_expectancy.csv\", na.strings = \"\")\r\nhead(life_exp)\r\n\r\n      Country Year     Status Life.expectancy Adult.Mortality\r\n1 Afghanistan 2015 Developing            65.0             263\r\n2 Afghanistan 2014 Developing            59.9             271\r\n3 Afghanistan 2013 Developing            59.9             268\r\n4 Afghanistan 2012 Developing            59.5             272\r\n5 Afghanistan 2011 Developing            59.2             275\r\n6 Afghanistan 2010 Developing            58.8             279\r\n  infant.deaths Alcohol percentage.expenditure Hepatitis.B Measles\r\n1            62    0.01              71.279624          65    1154\r\n2            64    0.01              73.523582          62     492\r\n3            66    0.01              73.219243          64     430\r\n4            69    0.01              78.184215          67    2787\r\n5            71    0.01               7.097109          68    3013\r\n6            74    0.01              79.679367          66    1989\r\n   BMI under.five.deaths Polio Total.expenditure Diphtheria HIV.AIDS\r\n1 19.1                83     6              8.16         65      0.1\r\n2 18.6                86    58              8.18         62      0.1\r\n3 18.1                89    62              8.13         64      0.1\r\n4 17.6                93    67              8.52         67      0.1\r\n5 17.2                97    68              7.87         68      0.1\r\n6 16.7               102    66              9.20         66      0.1\r\n        GDP Population thinness..1.19.years thinness.5.9.years\r\n1 584.25921   33736494                 17.2               17.3\r\n2 612.69651     327582                 17.5               17.5\r\n3 631.74498   31731688                 17.7               17.7\r\n4 669.95900    3696958                 17.9               18.0\r\n5  63.53723    2978599                 18.2               18.2\r\n6 553.32894    2883167                 18.4               18.4\r\n  Income.composition.of.resources Schooling\r\n1                           0.479      10.1\r\n2                           0.476      10.0\r\n3                           0.470       9.9\r\n4                           0.463       9.8\r\n5                           0.454       9.5\r\n6                           0.448       9.2\r\n\r\nI will now do some data cleaning. The variable names are a bit tedious and to avoid using the back ticks. I will simply clean up the column names using the janitor packages very useful clean_names() function\r\n\r\n\r\nlife_expe_data  <- clean_names(life_exp)\r\nnames(life_expe_data)\r\n\r\n [1] \"country\"                        \r\n [2] \"year\"                           \r\n [3] \"status\"                         \r\n [4] \"life_expectancy\"                \r\n [5] \"adult_mortality\"                \r\n [6] \"infant_deaths\"                  \r\n [7] \"alcohol\"                        \r\n [8] \"percentage_expenditure\"         \r\n [9] \"hepatitis_b\"                    \r\n[10] \"measles\"                        \r\n[11] \"bmi\"                            \r\n[12] \"under_five_deaths\"              \r\n[13] \"polio\"                          \r\n[14] \"total_expenditure\"              \r\n[15] \"diphtheria\"                     \r\n[16] \"hiv_aids\"                       \r\n[17] \"gdp\"                            \r\n[18] \"population\"                     \r\n[19] \"thinness_1_19_years\"            \r\n[20] \"thinness_5_9_years\"             \r\n[21] \"income_composition_of_resources\"\r\n[22] \"schooling\"                      \r\n\r\nNow the variable names are clean, we can do the rest of the coding as follows.I now remove the missing values. You can do better for your data by using imputation techniques to deal with missing values. But that is not my goal here today. Perhaps, I may do a blog or a YouTube video in the future. I just need you to demand for it or encourage me to do it.\r\n\r\n\r\nlife_exp_narm  <- life_expe_data[complete.cases(life_expe_data),]\r\ndim(life_exp_narm) \r\n\r\n[1] 1649   22\r\n\r\nlife_exp_narm <- life_exp_narm |> \r\n  filter(income_composition_of_resources>0) #let's remove all with income index around 0. \r\n\r\n\r\nWe have about 22 variables and 1649 observations. Not bad for modelling! Since, income_composition_of_resources is a really long and boring variable name, I am changing it.\r\n\r\n\r\nnames(life_exp_narm)[names(life_exp_narm) == \"income_composition_of_resources\"] <- \"income_index\"\r\n\r\n\r\n\r\n\r\nsummary(life_exp_narm$income_index)\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n 0.2790  0.5290  0.6770  0.6505  0.7540  0.9360 \r\n\r\nsd(life_exp_narm$income_index)\r\n\r\n[1] 0.1490056\r\n\r\n\r\n\r\nsummary(life_exp_narm$life_expectancy)\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  44.00   64.60   71.80   69.39   75.00   89.00 \r\n\r\nsd(life_exp_narm$life_expectancy)\r\n\r\n[1] 8.876531\r\n\r\nYou can see that our main predictor variable has a mean 0.65 and SD of 0.149. It maybe a good idea to take a look at it by visualizing it. And the mean life expectancy was 69.4(SD=8.88)\r\n\r\n\r\nggplot(life_exp_narm, aes(x=income_index, \r\n                          y=life_expectancy, col=status)) + geom_point() +\r\ngeom_smooth(method = \"lm\", col=\"blue\") +\r\n  xlab(\"Income index\") + ylab(\"Life expectancy\")\r\n\r\n\r\n\r\nQauntile regression\r\nBefore doing the quantile regression. It might be a good idea to take a look at the visualization by setting different tau values in abline of the base r plot showing the relationship between life_expectancy and income_index. The\r\nOLS regression is do the regression from the mean while the quantil regression n quantile regression, we can estimate any conditional quantile of the response variable, not just the mean but could be the medeian (50%), 90%, 75% or anywhere until 99%.If this is not clear, I highly recommended watching the video I dropped earlier.\r\nLet’s now visualize the mean and the median ablines\r\n\r\n\r\nplot(life_expectancy  ~ income_index, data = life_exp_narm) \r\nabline(lm(life_expectancy  ~ income_index, data = life_exp_narm), col=\"blue\", lwd = 3)\r\nabline(rq(life_expectancy  ~ income_index, tau=0.5, data = life_exp_narm), col=\"red\",  lwd = 4) # Notcie the tau=0.5, \r\n\r\n\r\n\r\nYou can see the plot blue line is the OLS regression line but the red line is if we model the regression using the median. If you have to run only two models and choose the model that best fits the data, you have to choose by AIC or using loss function(eg. the model with minimum mean squared error or mean absolute error, etc will be the best fit). NB: choosing the right model is not my goal here.\r\nNow let’s add 10th and 90th percentiles from the above vizualization and evaluate which line looks better fit for the relationship between the two variables.\r\n\r\n\r\nplot(life_expectancy  ~ income_index, data = life_exp_narm) \r\nabline(lm(life_expectancy  ~ income_index, data = life_exp_narm), col=\"blue\", lwd = 3)\r\nabline(rq(life_expectancy  ~ income_index, tau=0.5, data = life_exp_narm), col=\"red\",  lwd = 4)\r\nabline(rq(life_expectancy  ~ income_index, tau=0.1, data = life_exp_narm), col=\"black\",  lwd = 5)\r\nabline(rq(life_expectancy  ~ income_index, tau=0.90, data = life_exp_narm), col=\"turquoise4\",  lwd = 6)\r\n\r\n\r\n\r\nWe can actually visualize multiple quantreg models and see it compared to the OLS model.\r\n\r\n\r\nggplot(life_exp_narm, aes(life_expectancy, income_index))+\r\n    geom_point()+\r\n    geom_smooth(method = lm, se = F, color = \"darkturquoise\")+ #linear model\r\n    geom_quantile(color = \"red\", quantiles = 0.5)+\r\n    geom_quantile(color = \"black\", alpha = 0.2, #Quantreg based on median\r\n                  quantiles = seq(.05, .95, by = 0.05)) #multiple models from the 5th percentile to 95th percentile\r\n\r\n\r\n\r\nLet’s now run four different models at the mean(OLS), and quantile regressions at the median, 10th percentile and 95th percentiles. Notice the tau values. I also added one more variable.\r\n\r\n\r\nols <-  rq(life_expectancy~income_index+schooling, data = life_exp_narm) #Similar to lm()\r\nquant_reg_med  <- rq(life_expectancy~income_index+schooling,tau = 0.5, data = life_exp_narm)\r\nquant_reg_first  <- rq(life_expectancy~income_index+schooling,tau = 0.1, data = life_exp_narm)\r\nquant_reg_last  <- rq(life_expectancy~income_index+schooling,tau = 0.95, data = life_exp_narm)\r\n\r\n\r\n\r\n\r\nsummary(ols)\r\n\r\n\r\nCall: rq(formula = life_expectancy ~ income_index + schooling, data = life_exp_narm)\r\n\r\ntau: [1] 0.5\r\n\r\nCoefficients:\r\n             Value    Std. Error t value  Pr(>|t|)\r\n(Intercept)  38.59204  0.56239   68.62125  0.00000\r\nincome_index 62.04691  1.98135   31.31544  0.00000\r\nschooling    -0.75195  0.10388   -7.23903  0.00000\r\n\r\n\r\n\r\nsummary(quant_reg_med)\r\n\r\n\r\nCall: rq(formula = life_expectancy ~ income_index + schooling, tau = 0.5, \r\n    data = life_exp_narm)\r\n\r\ntau: [1] 0.5\r\n\r\nCoefficients:\r\n             Value    Std. Error t value  Pr(>|t|)\r\n(Intercept)  38.59204  0.56239   68.62125  0.00000\r\nincome_index 62.04691  1.98135   31.31544  0.00000\r\nschooling    -0.75195  0.10388   -7.23903  0.00000\r\n\r\n\r\n\r\nsummary(quant_reg_first)\r\n\r\n\r\nCall: rq(formula = life_expectancy ~ income_index + schooling, tau = 0.1, \r\n    data = life_exp_narm)\r\n\r\ntau: [1] 0.1\r\n\r\nCoefficients:\r\n             Value    Std. Error t value  Pr(>|t|)\r\n(Intercept)  25.92063  1.81747   14.26195  0.00000\r\nincome_index 84.00142  5.34145   15.72633  0.00000\r\nschooling    -1.34938  0.22578   -5.97661  0.00000\r\n\r\n\r\n\r\nsummary(quant_reg_last)\r\n\r\n\r\nCall: rq(formula = life_expectancy ~ income_index + schooling, tau = 0.95, \r\n    data = life_exp_narm)\r\n\r\ntau: [1] 0.95\r\n\r\nCoefficients:\r\n             Value     Std. Error t value   Pr(>|t|) \r\n(Intercept)   46.56308   0.40214  115.78932   0.00000\r\nincome_index  57.37912   2.25777   25.41402   0.00000\r\nschooling     -0.69824   0.10643   -6.56032   0.00000\r\n\r\n\r\n\r\nplot_models(ols, quant_reg_med, quant_reg_first, quant_reg_last,\r\n           show.values = TRUE,\r\n           m.labels = c(\"OLS\", \"Median\", \"10th percentile\",\r\n                       \"95th percentile\",\r\n                       legend.title = \"Model\")\r\n           )\r\n\r\n\r\n\r\nYou can see the OLS(based one mean) and quanitile regression based on the median are identical.But the one with 10th percentile and 95th percentile are different from the coefficients of the OLS.\r\nFor effectively modeling numerous quantreg models, you may do somthing like the following.\r\n\r\n\r\ntaus<-seq(from = .05, to = .95, by = 0.05) #Taus ranging from 0.05 to 0.95 with a step value of 0.05\r\nquant_all  <- rq(life_expectancy~income_index,tau = taus, \r\n                 data = life_exp_narm)\r\n\r\n\r\n\r\n\r\nnames(quant_all) #To access the cntents of our model\r\n\r\n [1] \"coefficients\"  \"x\"             \"y\"             \"residuals\"    \r\n [5] \"dual\"          \"fitted.values\" \"formula\"       \"terms\"        \r\n [9] \"xlevels\"       \"call\"          \"tau\"           \"rho\"          \r\n[13] \"method\"        \"model\"        \r\n\r\nquant_all$tau #To see the taus \r\n\r\n [1] 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65\r\n[14] 0.70 0.75 0.80 0.85 0.90 0.95\r\n\r\nWith that we have got several models. But how we access a specific model for example when tau=0.8 or tau=0.1?\r\n\r\n\r\nsummary(quant_all)[which(taus == 0.8)]\r\n\r\n[[1]]\r\n\r\nCall: rq(formula = life_expectancy ~ income_index, tau = taus, data = life_exp_narm)\r\n\r\ntau: [1] 0.8\r\n\r\nCoefficients:\r\n             Value    Std. Error t value  Pr(>|t|)\r\n(Intercept)  44.77157  0.62620   71.49715  0.00000\r\nincome_index 42.89216  0.79348   54.05567  0.00000\r\n\r\nsummary(quant_all)[which(taus == 0.1)]\r\n\r\n[[1]]\r\n\r\nCall: rq(formula = life_expectancy ~ income_index, tau = taus, data = life_exp_narm)\r\n\r\ntau: [1] 0.1\r\n\r\nCoefficients:\r\n             Value    Std. Error t value  Pr(>|t|)\r\n(Intercept)  20.52315  1.70906   12.00844  0.00000\r\nincome_index 65.74074  2.18924   30.02897  0.00000\r\n\r\nIf we want to compare the AIC values of these models, as usual we can use AIC()\r\n\r\n\r\naic_df <- data.frame(AIC = AIC(quant_all), model = paste(\"tau=\",taus))\r\nhead(aic_df)\r\n\r\n        AIC     model\r\n1 10945.777 tau= 0.05\r\n2 10550.411  tau= 0.1\r\n3 10230.887 tau= 0.15\r\n4  9978.270  tau= 0.2\r\n5  9771.004 tau= 0.25\r\n6  9618.262  tau= 0.3\r\n\r\nggplot(aic_df, aes(x = model, y = AIC)) +\r\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\r\n  labs(title = \"AIC values for different tau values\",\r\n       x = \"Tau\",\r\n       y = \"AIC\") +\r\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n\r\n\r\n\r\nIt seems the model around the median point is better fit. In general, we ought to pick the model with the lowest AIC score. AIC (Akaike Information Criterion) is a metric used to assess a statistical model’s suitability for a particular set of data. It is based on the model’s complexity as well as its goodness of fit. The model is thought to be better the lower the AIC value.\r\nHowever, it’s crucial to remember that the AIC should not be the only factor considered when choosing a model. When choosing a model, it is important to take into account a number of additional elements, including interpretability, applicability, and theoretical relevance. It’s also possible that none of the models have an AIC that is noticeably superior to the others, in which case other factors might be more crucial in selecting a model.\r\nHow about if we want to access the coefficients?\r\n\r\n\r\nQR.coef <- coef(quant_all)\r\n\r\n\r\nOr in a more complicated way like the following\r\n\r\n\r\nlprq <-  function(x, y, h, m=19 , tau=.5)\r\n  {\r\n    xx <- seq(min(x),max(x),length=m)\r\n    fv <- xx\r\n    dv <- xx\r\n    for(i in 1:length(xx)) {\r\n      z <- x - xx[i]\r\n      wx <- dnorm(z/h)\r\n      r <- rq(y~z, weights=wx, tau=tau, ci=FALSE)\r\n      fv[i] <- r$coef[1.]\r\n      dv[i] <- r$coef[2.]\r\n    }\r\n    data.frame(dv = dv)\r\n}\r\n\r\n#Create a matrix to save the QQR estimates\r\ntaus<-seq(from = .05, to = .95, by = 0.05)\r\n\r\nQQR.coef <- as.data.frame(matrix(0, ncol = 19, nrow = 19))\r\n\r\n# Run QQR in a loop and save estimates in matrix \"QQR.coef\"\r\n#Note: 0.05 in below loop is the bandwidth that can be adjusted\r\n\r\no <-life_exp_narm$life_expectancy\r\np <-life_exp_narm$income_index\r\n\r\nfor (i in 1:19){\r\n  x<-lprq(o, p,0.05,tau=taus[i])\r\n  QQR.coef[,i]<-x\r\n}  \r\n\r\n\r\nLet’s now save all coefficients in a matrix\r\n\r\n\r\nbeta <- as.matrix(QQR.coef)\r\n\r\n\r\nVizualize\r\n\r\n\r\np <- plot_ly( z = ~beta, x = ~taus, y = ~taus, opacity = 0.6) %>%\r\n  add_markers()\r\n\r\np %>% add_surface(z = ~beta, x = ~taus, y = ~taus, showscale = FALSE) %>%\r\n  layout(showlegend = FALSE)\r\n\r\n\r\n\r\nThat is all for today. Again I highly recommend watching the video and reading quantreg package functions. I hope, you like it. I hope I will see you in the next post.\r\nContact\r\nPlease mention @RPy_DataScience if you tweet this post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-30-quantile-regression/quantreg.png",
    "last_modified": "2023-03-31T17:16:15+02:00",
    "input_file": "quantile-regression.knit.md",
    "preview_width": 1023,
    "preview_height": 588
  },
  {
    "path": "posts/2020-08-27-text-machine-learning/",
    "title": "XGboost, Naive Bayes and SVM Machine learning algorithms for facilitating title-abstract screening in systematic reviews: predicting inclusion/exclusion of abstracts",
    "description": "Machine learning methods for document classification.",
    "author": [
      {
        "name": "Mihiretu Kebede(PhD)",
        "url": {}
      }
    ],
    "date": "2023-03-29",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\nIntroduction\r\nSeveral thousands of papers are being published everyday. A paper published about a decade ago wrote “the total number of science papers published since 1665 passed 50 million. About 2.5 million new scientific papers are published each year.” In the introduction section of one of my previous systematic review, I wrote “The number of published research doubles every 9 years and its growth particularly in medicine and health care is exponential.”\r\nDoing systematic reviews on a certain topic requires an extensive database search, title/abstract screening, full-text screening, quality assessment, data extraction, qualitative, quantitative synthesis and other important steps. Of these steps, at least title/abstract screening and full text reviews is required to be done by two review authors.\r\nGoing through the title and abstract of thousands of papers is time-taking, and laborious. Yet, policy makers medical practitioners require the latest evidence for making urgent decisions. Thanks to the development of machine learning and artificial intelligence over the past decade, systematic reviews can be used to automate or semi-automate some of the steps of systematic reviews. Systematic reviewers are already using some web-based text mining and machine learning tools such as Abstrackr, Robotreviewer, RobotAnalyst, etc. However, it is worth a try to implement text classification algorithms in R.\r\nIn a paper “Why Should I Trust You?”: Explaining the Predictions of Any Classifier\", Riberio TL et.al described machine learning models as mostly “black boxes”. The biggest challenge of machine learning is interpretability and applying it to local context or simply to an individual or observation with a specific characteristics. They developed LIME which was later changed into Python and R packages. Lime can help explain several models implemented in about six different packages including caret, h2o, and xgboost.\r\nIn this blog post, I want to try one of the many available methods available to check whether machine learning methods correctly discriminates SARS papers from COVID19 papers. I know the two viruses are highly related. I am only using the SARS and COVID for demonstrations.\r\n\r\n\r\nlibrary(dplyr) #for data management\r\nlibrary(ggplot2) #for plotting\r\nlibrary(bib2df) #for converting bib file to data frame\r\nlibrary(xgboost) # for building extreme gradient boosting \r\nlibrary(lime) #for explaining the machine learning models\r\n\r\n\r\nData\r\nI used the covid19 data that I used in my previous blog post. In addition, I have added new reference data by searching SARS papers published from January 1/2003 until September 12/2003.\r\nAs I did in my last blog, I read my files using the beautiful bib2df package. You can also use bibliometrix or revtools packages to convert bibtex files to data frame in R. With revtools, you can also convert .ris files to data frame. Awesome!\r\n\r\n\r\n# library(bib2df)\r\n# library(dplyr)\r\n# \r\n# covid <- bib2df(\"covid19.bib\") %>% # import references\r\n#   filter(!is.na(ABSTRACT)) %>% # select only papers with no missing abstract\r\n#   select(\"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"AUTHOR\", \"BIBTEXKEY\") # Select few variables \r\n#   \r\n# \r\n# # 2003 corona virus\r\n# covexclude <- bib2df(\"covexclude.bib\") %>% \r\n#   filter(!is.na(ABSTRACT)) %>% \r\n#   select(\"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"AUTHOR\", \"BIBTEXKEY\")\r\n# \r\n# cov2003 <- bib2df(\"cov2003.bib\") %>% \r\n#   filter(!is.na(ABSTRACT)) %>% \r\n#   select(\"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"AUTHOR\", \"BIBTEXKEY\")\r\n# \r\n# # Now import the TB data\r\n# sars <- bib2df(\"sars.bib\") %>% \r\n#   filter(!is.na(ABSTRACT)) %>% \r\n#   select(\"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"AUTHOR\", \"BIBTEXKEY\")\r\n\r\n\r\nAssume we are only looking for COVID19 papers to include in our review. That means we will need to exclude all SARS papers published in 2003. I used a simple include/exclude decisions.\r\nWe will now create a new binary factor factor variable called “Include”. We will then assign 1 for all covid papers and 0 for all SARS papers. To make the decisions look more in real world setting, I excluded 40 COVID papers published in August 22/2020. This has nothing to do with the contents of the papers. I am arbitrarily excluding the papers to make it look like a real world title/abstract screening process rather than a random decision.\r\nAfter that I merged the two data frames with a simple rbind() code. I had a problem rendering UTF-8 characters in my github page for this blog post. So I have instead read everything using bib2df and then from there I wrote them in xlsx file. After that I simply read my excel file in R and did the whole analysis using the excel file.\r\n\r\n\r\n# Exclude all covid papers\r\n# covid$Include <- 1 # coded them as included\r\n# \r\n# covexclude$Include <- 0 # excluded\r\n# \r\n# cov2003$Include <- 1 # included\r\n# sars$Include <- 0 # excluded\r\n# \r\n# # Now merge the tw data frames\r\n# covid_sars <- rbind(covid, covexclude, sars, cov2003) # combine\r\n# \r\n\r\n# write.xlsx(covid_sars, file = \"covidsars.xlsx\",\r\n#            sheetName = \"covidsars\", append = FALSE) \r\n\r\n\r\n\r\n\r\nlibrary(readxl)\r\nlibrary(dplyr)\r\ncovid_sars <- read_excel(\"covidsars.xlsx\")\r\nglimpse(covid_sars)\r\n\r\nRows: 983\r\nColumns: 7\r\n$ ...1     <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", ~\r\n$ title    <chr> \"Factors determining the diffusion of COVID-19 and ~\r\n$ abstract <chr> \"This study has two goals. The first is to explain ~\r\n$ keywords <chr> \"Air Pollutants,Air Pollution,Betacoronavirus,Citie~\r\n$ author   <chr> \"Coccia, Mario\", \"Ataguba, Ochega A and Ataguba, Jo~\r\n$ label    <chr> \"Coccia2020\", \"Ataguba2020\", \"Sigala2020\", \"Lechner~\r\n$ Include  <chr> \"included\", \"included\", \"included\", \"included\", \"in~\r\n\r\n#of characters\r\ncovid_sars$abstract_length <- nchar(covid_sars$abstract)\r\nsummary(covid_sars$abstract_length) \r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n     11     812    1239    1244    1620    4259 \r\n\r\n# Select only records with more than 300 characters \r\n\r\ncovid_sars <- covid_sars %>% \r\n  filter(abstract_length > 200)\r\n\r\n\r\nCheck number of included/excluded papers\r\n\r\n\r\nlibrary(dplyr)\r\ntable(covid_sars$Include)\r\n\r\n\r\n    included not included \r\n         506          464 \r\n\r\n# check the type of this variable and convert it to factor \r\n\r\ntypeof(covid_sars$Include) # it is double. We have to convert it to factor variable\r\n\r\n[1] \"character\"\r\n\r\ncovid_sars$Include[covid_sars$Include==\"1\"]<-\"included\"\r\ncovid_sars$Include[covid_sars$Include==\"0\"]<-\"not included\"\r\n\r\ntable(covid_sars$Include) %>% prop.table() #51% excluded, 49% included\r\n\r\n\r\n    included not included \r\n   0.5216495    0.4783505 \r\n\r\nSplit the data\r\nimageMachine learning algorithms require our data to be split into training and testing. The training data will be used to train the model and the testing data sets will be used to make predictions by deploying the model developed using the training sets. There are different ways of splitting our data to training and testing split. You may use base R, Resample or Caret package to easily perform this task. Here, I will use my favorite R packager: the Caret package. We will use 70/30 split which means 70% of my data will be assigned to the training set and 30% the data will be used for test sets.\r\nI will need to stratify my data split by my dependent variable. That means my train and test data will have exactly similar proportion for the outcome variable responses as the original data sets. That is 51% for excluded (coded as 0) and 49% for included (coded as 1). The data is nearly balanced. We don’t have the curse of class imbalance here, awesome!\r\n\r\n\r\nlibrary(caret)\r\nset.seed(3456) # for reproducibility\r\n\r\ntrainIndex <- createDataPartition(covid_sars$Include, p = .7, \r\n                                  list = FALSE, \r\n                                  times = 1)\r\ntrain <- covid_sars[ trainIndex,]\r\ntest  <- covid_sars[-trainIndex,]\r\n\r\n\r\nprop.table(table(covid_sars$Include)) # outcome proportion for th original data\r\n\r\n\r\n    included not included \r\n   0.5216495    0.4783505 \r\n\r\nprop.table(table(train$Include)) # outcome proportion for training data\r\n\r\n\r\n    included not included \r\n   0.5220588    0.4779412 \r\n\r\nprop.table(table(test$Include)) # outcome proportion for testing data\r\n\r\n\r\n    included not included \r\n   0.5206897    0.4793103 \r\n\r\n# Cool! They have exactly similar proportions with respect to the outcome variable. \r\n\r\n\r\nThe nice part\r\n\r\n\r\n\r\nlibrary(text2vec)\r\n\r\nget_matrix <- function(text) {\r\n  it <- itoken(text, progressbar = FALSE)\r\n  create_dtm(it, vectorizer = hash_vectorizer())\r\n}\r\n\r\ndtm_train = get_matrix(train$abstract)\r\ndtm_test = get_matrix(test$abstract)\r\n\r\n\r\n\r\n\r\n# Create boosting model for binary classification (-> logistic loss)\r\n# Other parameters are quite standard\r\n\r\nlibrary(xgboost) # I will use extreme gradient boosting algorithm for building the model. But, I will also try other algorithms in the future. \r\n\r\nparam <- list(max_depth = 10, \r\n              eta = 0.1, \r\n              objective = \"binary:logistic\", \r\n              eval_metric = \"error\", \r\n              nthread = 1) # I will set the paprametres nearly similarly as it was described in the example from the package description. \r\n\r\nxgb_model <- xgb.train(\r\n  param, \r\n  xgb.DMatrix(dtm_train, label = train$Include==\"included\"),\r\n  nrounds = 50\r\n)\r\n\r\n\r\nUse the model to predict the test set\r\nWe Can Predict The Future Prediction GIF - WeCanPredictTheFuture Future Prediction - Discover & Share GIFs\r\n\r\nlibrary(caret)\r\npredictions <- predict(xgb_model, dtm_test) \r\n# prediction probabilities\r\n\r\npredict <- ifelse(predictions > 0.5, \"included\", \"not included\") # assign prediction probabilities greater than 0.5 as included and less than 0.5 as not included\r\n\r\n\r\nconfusionMatrix(as.factor(predict), as.factor(test$Include))\r\n\r\nConfusion Matrix and Statistics\r\n\r\n              Reference\r\nPrediction     included not included\r\n  included          143            9\r\n  not included        8          130\r\n                                          \r\n               Accuracy : 0.9414          \r\n                 95% CI : (0.9078, 0.9655)\r\n    No Information Rate : 0.5207          \r\n    P-Value [Acc > NIR] : <2e-16          \r\n                                          \r\n                  Kappa : 0.8825          \r\n                                          \r\n Mcnemar's Test P-Value : 1               \r\n                                          \r\n            Sensitivity : 0.9470          \r\n            Specificity : 0.9353          \r\n         Pos Pred Value : 0.9408          \r\n         Neg Pred Value : 0.9420          \r\n             Prevalence : 0.5207          \r\n         Detection Rate : 0.4931          \r\n   Detection Prevalence : 0.5241          \r\n      Balanced Accuracy : 0.9411          \r\n                                          \r\n       'Positive' Class : included        \r\n                                          \r\n\r\nIt resulted 93.5% accuracy. In real world setting this may not happen because we will work on highly related titles and abstracts. My experiment brings two somehow related documents (SARS and covid19). ALthough we are explcitly looking for covid papers, I assigned some of the covid papers to be excluded just to add some confusion.\r\nLet’s pick two abstracts(Abstract #89 and #271) and see what are the most important terms of the abstract that xgboost used for its predictions. We will use lime package to explain the predictions\r\nAbstract number 89\r\nWe need to use lime:: to avoid conflicts with dplyrpackage\r\n\r\n\r\nab_to_explain89 <- head(test[89,]$abstract, 6)\r\n\r\nexplainer89 <- lime::lime(ab_to_explain89, model = xgb_model, \r\n                  preprocess = get_matrix)\r\n\r\nexplanation89 <- lime::explain(ab_to_explain89, explainer89, n_labels=1,\r\n                       n_features = 7) # Set number most important features to 7\r\n\r\n\r\n\r\n\r\nexplanation89[, 2:10]\r\n\r\n# A tibble: 7 x 9\r\n   case label label~1 model~2 model~3 model~4 feature featu~5 featur~6\r\n  <int> <chr>   <dbl>   <dbl>   <dbl>   <dbl> <chr>   <chr>      <dbl>\r\n1     1 0       0.950   0.962   0.989   0.951 by      by      -0.0135 \r\n2     1 0       0.950   0.962   0.989   0.951 and     and     -0.00645\r\n3     1 0       0.950   0.962   0.989   0.951 of      of      -0.00592\r\n4     1 0       0.950   0.962   0.989   0.951 is      is      -0.00551\r\n5     1 0       0.950   0.962   0.989   0.951 their   their   -0.00281\r\n6     1 0       0.950   0.962   0.989   0.951 are     are     -0.00204\r\n7     1 0       0.950   0.962   0.989   0.951 to      to      -0.00194\r\n# ... with abbreviated variable names 1: label_prob, 2: model_r2,\r\n#   3: model_intercept, 4: model_prediction, 5: feature_value,\r\n#   6: feature_weight\r\n\r\n\r\n\r\nlime::plot_features(explanation89)\r\n\r\n\r\n\r\nThe word sars is most important word that xgb model used to predict the exclusion of this this abstract.\r\n\r\n\r\nlime::plot_text_explanations(explanation89)\r\n\r\n\r\n\r\nAbstract 283\r\n\r\n\r\nlibrary(lime)\r\n\r\nab_to_explain <- head(test[283,]$abstract, 6)\r\n\r\nexplainer <- lime(ab_to_explain, model = xgb_model, \r\n                  preprocess = get_matrix, \r\n                  tokenization =default_tokenize)\r\n\r\nexplanation <- explain(ab_to_explain, explainer, n_labels=1,\r\n                       n_features = 7) # Set number most important features to 7\r\n\r\n\r\n\r\n\r\nexplanation[, 2:10]\r\n\r\n# A tibble: 7 x 9\r\n   case label label~1 model~2 model~3 model~4 feature featu~5 featur~6\r\n  <int> <chr>   <dbl>   <dbl>   <dbl>   <dbl> <chr>   <chr>      <dbl>\r\n1     1 1       0.935   0.997 -0.0123   0.931 corona  corona   0.848  \r\n2     1 1       0.935   0.997 -0.0123   0.931 of      of       0.0416 \r\n3     1 1       0.935   0.997 -0.0123   0.931 and     and      0.0250 \r\n4     1 1       0.935   0.997 -0.0123   0.931 is      is       0.0193 \r\n5     1 1       0.935   0.997 -0.0123   0.931 an      an       0.00880\r\n6     1 1       0.935   0.997 -0.0123   0.931 total   total   -0.00763\r\n7     1 1       0.935   0.997 -0.0123   0.931 were    were     0.00890\r\n# ... with abbreviated variable names 1: label_prob, 2: model_r2,\r\n#   3: model_intercept, 4: model_prediction, 5: feature_value,\r\n#   6: feature_weight\r\n\r\n\r\n\r\nplot_features(explanation)\r\n\r\n\r\n\r\n\r\n\r\nplot_text_explanations(explanation)\r\n\r\n\r\n\r\nThe word corona is used for predicting “Include” with 87% prediction probability.\r\nThe above modelling is dirty.It didn’t use only the relevant words. I will use quanteda package to remove all irrelavnt words.\r\n\r\n\r\nlibrary (quanteda)\r\n\r\n\r\nCreate corpus\r\n\r\n\r\nnames(train) \r\n\r\n[1] \"...1\"            \"title\"           \"abstract\"       \r\n[4] \"keywords\"        \"author\"          \"label\"          \r\n[7] \"Include\"         \"abstract_length\"\r\n\r\ntrain_data <- train[, 6]  # I need only some of my variables: title, include, and label variable I don't need the rest of the variables for now. The label variable helps me to identify the papers it is written like Authoryyyy. I will use it to attach as a document identifier. \r\n\r\n# Now build the corpus using abstracts of the papers\r\ntrain_corpus <- corpus(train$abstract, \r\n                     docvars = data.frame(abstract_label = names(train_data))) # I added the docvars to save the additional variables other than the abstract\r\n\r\n\r\nSimilarly for the test set\r\n\r\n\r\nnames(test) \r\n\r\n[1] \"...1\"            \"title\"           \"abstract\"       \r\n[4] \"keywords\"        \"author\"          \"label\"          \r\n[7] \"Include\"         \"abstract_length\"\r\n\r\ntest1 <- test %>% \r\n  filter(label!=\"NA\")\r\n\r\ntest_data <- test1[,6] \r\n\r\ntest_corpus <- corpus(test1$abstract)\r\n\r\n\r\nAdd document identifier for both test and training set data\r\n\r\n\r\ndocid <- paste(train$label)\r\ndocnames(train_corpus) <- docid\r\nprint(train_corpus)\r\n\r\nCorpus consisting of 680 documents and 1 docvar.\r\nAtaguba2020.1 :\r\n\"The coronavirus disease 2019 (COVID-19) pandemic has affecte...\"\r\n\r\nSigala2020.1 :\r\n\"The paper aims to critically review past and emerging litera...\"\r\n\r\nLechner2020.1 :\r\n\"Amidst the coronavirus pandemic, universities across the cou...\"\r\n\r\nVanDorp2020.1 :\r\n\"SARS-CoV-2 is a SARS-like coronavirus of likely zoonotic ori...\"\r\n\r\nBarilla2020.1 :\r\n\": ACE2 receptor has a broad expression pattern in the cellul...\"\r\n\r\nZhang2020d.1 :\r\n\"The nucleocapsid protein is significant in the formation of ...\"\r\n\r\n[ reached max_ndoc ... 674 more documents ]\r\n\r\nAttach the document identifier also for the test set\r\n\r\n\r\ndocidtest <- paste(test1$label)\r\ndocnames(test_corpus) <- docidtest\r\nprint(test_corpus)\r\n\r\nCorpus consisting of 290 documents.\r\nCoccia2020.1 :\r\n\"This study has two goals. The first is to explain the geo-en...\"\r\n\r\nCagliani2020.1 :\r\n\"In December 2019, a novel human-infecting coronavirus (SARS-...\"\r\n\r\nOkba2020.1 :\r\n\"Middle East respiratory syndrome coronavirus (MERS-CoV) is a...\"\r\n\r\nDonthu2020.1 :\r\n\"The COVID-19 outbreak is a sharp reminder that pandemics, li...\"\r\n\r\nWister2020.1 :\r\n\"The COVID-19 global crisis is reshaping Canadian society in ...\"\r\n\r\nActer2020.1 :\r\n\"According to data compiled by researchers at Johns Hopkins U...\"\r\n\r\n[ reached max_ndoc ... 284 more documents ]\r\n\r\nTokenize\r\nI will create tokens and then later document feature matrix for both train and test abstract copuses. We do the same tokenization process as we did in my previous blog post. We will remove numbers, remove punctuation, or remove a customized list of stop words\r\n\r\n\r\ncustom_stop_words <- c(\"background\", \"introduction\",\"aims\", \"objectives\", \"materials\", \"methods\", \"results\", \"conclusions\",\"textless\", \"study\") \r\n                                      \r\ntrain_tokens <- tokens(train_corpus, remove_punct = TRUE,\r\n                  remove_numbers = TRUE)\r\ntrain_tokens <- tokens_select(train_tokens, pattern = stopwords('en'), selection = 'remove') # remove irrelevant words\r\n\r\ntrain_tokens <- tokens_select(train_tokens, pattern = custom_stop_words, selection = 'remove') # remove customized list of stop words\r\n\r\n\r\nI do the same for the test abstract corpuses\r\n\r\n\r\ntest_tokens <- tokens(test_corpus, remove_punct = TRUE,\r\n                  remove_numbers = TRUE)\r\ntest_tokens <- tokens_select(test_tokens, pattern = stopwords('en'), selection = 'remove') # remove irrelevant words\r\n\r\ntest_tokens <- tokens_select(test_tokens, pattern = custom_stop_words, selection = 'remove') # remove customized list of stop words\r\n\r\n\r\nConstruct document feature matrix for both train and test abstract tokens\r\n\r\n\r\ntrain_dfmat <- dfm(train_tokens) \r\ntest_dfmat <- dfm(test_tokens)\r\n\r\n\r\nHave a quick look of the two document feature matrices\r\n\r\n\r\nhead(train_dfmat)\r\n\r\nDocument-feature matrix of: 6 documents, 11,352 features (99.14% sparse) and 1 docvar.\r\n               features\r\ndocs            coronavirus disease covid-19 pandemic affected many\r\n  Ataguba2020.1           2       2        6        3        1    7\r\n  Sigala2020.1            0       0        4        1        0    0\r\n  Lechner2020.1           2       0        0        2        0    0\r\n  VanDorp2020.1           1       0        1        1        0    0\r\n  Barilla2020.1           1       1        2        1        0    1\r\n  Zhang2020d.1            1       0        1        0        0    0\r\n               features\r\ndocs            countries increasing morbidity mortality\r\n  Ataguba2020.1         7          1         2         1\r\n  Sigala2020.1          0          0         0         0\r\n  Lechner2020.1         0          0         0         0\r\n  VanDorp2020.1         1          0         0         0\r\n  Barilla2020.1         0          0         0         0\r\n  Zhang2020d.1          0          0         0         0\r\n[ reached max_nfeat ... 11,342 more features ]\r\n\r\nhead(test_dfmat)\r\n\r\nDocument-feature matrix of: 6 documents, 6,869 features (98.40% sparse) and 0 docvars.\r\n                features\r\ndocs             two goals first explain geo-environmental\r\n  Coccia2020.1     1     1     1       1                 1\r\n  Cagliani2020.1   0     0     0       0                 0\r\n  Okba2020.1       0     0     0       0                 0\r\n  Donthu2020.1     0     0     0       0                 0\r\n  Wister2020.1     0     0     0       0                 0\r\n  Acter2020.1      1     0     0       0                 0\r\n                features\r\ndocs             determinants accelerated diffusion covid-19\r\n  Coccia2020.1              1           3         3        9\r\n  Cagliani2020.1            0           0         0        0\r\n  Okba2020.1                0           0         0        0\r\n  Donthu2020.1              0           0         0        1\r\n  Wister2020.1              0           0         0        4\r\n  Acter2020.1               0           0         0        1\r\n                features\r\ndocs             generating\r\n  Coccia2020.1            1\r\n  Cagliani2020.1          0\r\n  Okba2020.1              0\r\n  Donthu2020.1            0\r\n  Wister2020.1            0\r\n  Acter2020.1             0\r\n[ reached max_nfeat ... 6,859 more features ]\r\n\r\ndim(train_dfmat) # 646 abstracts 11191 features/terms\r\n\r\n[1]   680 11352\r\n\r\ndim(test_dfmat) # 276 abstracts 6668 features/terms\r\n\r\n[1]  290 6869\r\n\r\n# Or simply pass the dfm object in ndoc() or nfeat() functions\r\nndoc(train_dfmat)\r\n\r\n[1] 680\r\n\r\nnfeat(train_dfmat)\r\n\r\n[1] 11352\r\n\r\nndoc(test_dfmat)\r\n\r\n[1] 290\r\n\r\nnfeat(test_dfmat)\r\n\r\n[1] 6869\r\n\r\nVizualize the test and train document feature matrices\r\n\"When you visualize, then you materialize.\" Denis Waitley\r\nVizualizing the data is very importnat. Let’s see how the wordclouds look for train and test abstracts.\r\n\r\n\r\nlibrary(quanteda.textplots)\r\nlibrary(RColorBrewer)\r\npal <- brewer.pal(5, \"Dark2\")\r\n\r\ntextplot_wordcloud(train_dfmat, min_count = 40,max_words = 400,\r\n     color = pal)\r\n\r\n\r\n\r\nSimilarly, for the test_dfmat\r\n\r\n\r\nlibrary(quanteda.textplots)\r\n textplot_wordcloud(test_dfmat, min_count = 20,max_words = 400,\r\n     color = pal)\r\n\r\n\r\n\r\nBoth plots of the test and train data are comparable.\r\nPrediction using Naive Bayes model\r\nI need the following packages\r\n\r\n\r\nlibrary(quanteda)\r\nlibrary(quanteda.textmodels)\r\nlibrary(caret)\r\n\r\n\r\n\r\n\r\nnaive_bayes <- textmodel_nb(train_dfmat,train$Include)\r\nsummary(naive_bayes)\r\n\r\n\r\nCall:\r\ntextmodel_nb.dfm(x = train_dfmat, y = train$Include)\r\n\r\nClass Priors:\r\n(showing first 2 elements)\r\n    included not included \r\n         0.5          0.5 \r\n\r\nEstimated Feature Scores:\r\n             coronavirus  disease  covid-19  pandemic  affected\r\nincluded        0.003960 0.004630 0.0138695 0.0043808 0.0005356\r\nnot included    0.003552 0.004461 0.0006497 0.0004331 0.0006281\r\n                  many countries increasing morbidity mortality\r\nincluded     0.0008609 0.0011478  0.0005165 0.0002104 0.0006887\r\nnot included 0.0007797 0.0005847  0.0001299 0.0001083 0.0007797\r\n             interestingly   actions  policies   adopted    linked\r\nincluded         9.565e-05 1.530e-04 2.678e-04 1.530e-04 0.0002487\r\nnot included     4.331e-05 8.663e-05 2.166e-05 2.166e-05 0.0001733\r\n                social determinants   health       sdh  critical\r\nincluded     0.0008035    7.652e-05 0.003539 1.148e-04 0.0007652\r\nnot included 0.0001949    1.083e-04 0.003617 2.166e-05 0.0004765\r\n             inequalities  directly    within    sector distancing\r\nincluded        3.826e-05 0.0002296 0.0005739 1.722e-04  2.870e-04\r\nnot included    2.166e-05 0.0001733 0.0009529 6.497e-05  6.497e-05\r\n                  good   hygiene  avoiding     large gatherings\r\nincluded     0.0002487 0.0001722 1.339e-04 0.0006887  3.826e-05\r\nnot included 0.0002166 0.0001299 2.166e-05 0.0005847  2.166e-05\r\n\r\nSince we have already our model, we can use it for predicting the test sets. Unfortunately, the features in test_dfmat which are also in train_dfmat. Quanteda package has one really nice function called dfm_match() to select only features of the testing set that also occur in the training set.\r\n\r\n\r\nmatched_dfmat <- dfm_match(test_dfmat, features = featnames(train_dfmat))\r\n\r\n\r\n\r\n\r\nactual_class <- test$Include\r\npredicted_class <- predict(naive_bayes, newdata = matched_dfmat)\r\ntab_class <- table(predicted_class, actual_class )\r\ntab_class\r\n\r\n               actual_class\r\npredicted_class included not included\r\n   included          134           13\r\n   not included       17          126\r\n\r\n\r\n\r\nconfusionMatrix(tab_class, mode = \"everything\")\r\n\r\nConfusion Matrix and Statistics\r\n\r\n               actual_class\r\npredicted_class included not included\r\n   included          134           13\r\n   not included       17          126\r\n                                          \r\n               Accuracy : 0.8966          \r\n                 95% CI : (0.8556, 0.9291)\r\n    No Information Rate : 0.5207          \r\n    P-Value [Acc > NIR] : <2e-16          \r\n                                          \r\n                  Kappa : 0.793           \r\n                                          \r\n Mcnemar's Test P-Value : 0.5839          \r\n                                          \r\n            Sensitivity : 0.8874          \r\n            Specificity : 0.9065          \r\n         Pos Pred Value : 0.9116          \r\n         Neg Pred Value : 0.8811          \r\n              Precision : 0.9116          \r\n                 Recall : 0.8874          \r\n                     F1 : 0.8993          \r\n             Prevalence : 0.5207          \r\n         Detection Rate : 0.4621          \r\n   Detection Prevalence : 0.5069          \r\n      Balanced Accuracy : 0.8969          \r\n                                          \r\n       'Positive' Class : included        \r\n                                          \r\n\r\nOur models predicted the test data set with 89.5% accuracy. This is wonderful! The other model performance parameters are also very good. How about Support Vector Machines?\r\n\r\n\r\ntrain_svm <- textmodel_svm(train_dfmat,train$Include, weight=\"uniform\") # There are three weighting options. I don't have problem of class imbalance, let me just use the default \"uniform\"\r\n\r\n\r\nPredict the test set using SVM model\r\n\r\n\r\nActual <- test$Include\r\npredicted_class_svm <- predict(train_svm, newdata = matched_dfmat)\r\ntab_class_svm <- table(predicted_class_svm, Actual )\r\ntab_class_svm\r\n\r\n                   Actual\r\npredicted_class_svm included not included\r\n       included          143           11\r\n       not included        8          128\r\n\r\nNow the confusion matrix\r\n\r\n\r\nconfusionMatrix(tab_class_svm, mode = \"everything\")\r\n\r\nConfusion Matrix and Statistics\r\n\r\n                   Actual\r\npredicted_class_svm included not included\r\n       included          143           11\r\n       not included        8          128\r\n                                          \r\n               Accuracy : 0.9345          \r\n                 95% CI : (0.8996, 0.9601)\r\n    No Information Rate : 0.5207          \r\n    P-Value [Acc > NIR] : <2e-16          \r\n                                          \r\n                  Kappa : 0.8686          \r\n                                          \r\n Mcnemar's Test P-Value : 0.6464          \r\n                                          \r\n            Sensitivity : 0.9470          \r\n            Specificity : 0.9209          \r\n         Pos Pred Value : 0.9286          \r\n         Neg Pred Value : 0.9412          \r\n              Precision : 0.9286          \r\n                 Recall : 0.9470          \r\n                     F1 : 0.9377          \r\n             Prevalence : 0.5207          \r\n         Detection Rate : 0.4931          \r\n   Detection Prevalence : 0.5310          \r\n      Balanced Accuracy : 0.9339          \r\n                                          \r\n       'Positive' Class : included        \r\n                                          \r\n\r\nSVM prediction is quite remarkable. Almost 92.5% accurate prediction. This is cool. Look at the other performance measures: sensitivity, specificity, PVP, NPV, recall, precision. They are all more than 90%!\r\nOne limitation of implementing these models using quanteda.textmodels is we cannot benefit the great advantages of lime package, we cannot use lime to explain our naive_bayes model locally (within abstracts) as we did it our prediction using XGB because lime currently doesn’t support quanteda.textmodels package. Currently lime package only explains models developed using mlr, xgboost, h2o, keras, or MASSpackages.\r\nFinal remarks\r\nI have tried three different models and the level of accuracy has improved through my model choices. I see lots of beautiful things that we can take advantage of NLP for systematic reviews. However, the process is quite complex, computationally expensive and we don’t know which model works best unless we experiment several models with broad range of model parameters. I can see its tremendous potential even after such a tiny scratch on its surface. We don’t know how much I will gain in improving the prediction accuracies if I process my abstracts using *tf-idf*, n-grams, the mighty cross-validation processes, and redo my analysis with several models including deep learning, etc.\r\nNext stop\r\nVisualizing odds ratios/risk ratios in forest plots. See you!\r\nTrains GIFs | TenorContact\r\n@MihiretuKebede1\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-08-27-text-machine-learning/machinelearning.png",
    "last_modified": "2023-03-29T22:31:12+02:00",
    "input_file": "2020-08-27-text-machine-learning.knit.md",
    "preview_width": 2121,
    "preview_height": 1362
  },
  {
    "path": "posts/2020-12-09-2020-12-09-documentclassification-using-regularized-models/",
    "title": "Classifying abstracts using regularized models: covid-19 or sars-2003?",
    "description": "Classifying abstracts using regularized models: covid-19 or sars-2003?",
    "author": [
      {
        "name": "Mihiretu Kebede(PhD)",
        "url": {}
      }
    ],
    "date": "2023-03-23",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\nIntroduction\r\nClassifying documents to a certain list of categories can provide valuable insights and possibly make the documents more manageable. Document classification is one of the application areas of machine learning. Natural Language Processing(NLP) and machine learning methods can be used to automate classification of documents such as emails(ham/spam categories), articles, books, response to survey questions, sentiments, product reviews(negative/positive), etc.\r\nThere are handful of very good algorithms that can automagically handle text data. In this blog post, I would like to experiment regularized logistic regression machine learning models for classifying abstracts.\r\nI am interested in predicting abstracts that will fall into “SARS_2003” and “COVID_19” categories. For this task, I will use the same data sets that I have used in my previous blog posts. I saved the references files in excel from my previous post. I will simply load this excel file and play with the abstract using qunateda and glmnet packages.\r\nData\r\nI load the data and see a summary of number of characters used to write the abstracts. For this analysis, I will use only abstracts having 200 characters. I will just filter out the abstracts on the lower side of the extreme.\r\nCheck number of covid/sars papers\r\nThe covid abstracts were originally labeled as “included” and sars abstracts as “excluded”. I will simply create a category variable and change the labels to “covid” or “sars”. About 52% of the abstracts have “covid” labels the remaining is “sars”.\r\n\r\n\r\ncovid_sars$category <- covid_sars$Include\r\ncovid_sars$category[covid_sars$category==\"included\"]<-\"covid\"\r\ncovid_sars$category[covid_sars$category==\"not included\"]<-\"sars\"\r\n\r\ntable(covid_sars$category)\r\n\r\n\r\ncovid  sars \r\n  506   464 \r\n\r\nround(table(covid_sars$category) %>% prop.table()*100, 1)\r\n\r\n\r\ncovid  sars \r\n 52.2  47.8 \r\n\r\nSplit the data\r\nBefore we do any steps in a text analysis, it is recommended to split the data. Splitting the data after tokenization is not a good approach. So, we will instead split the data into train and test set, tokenize the train and test sets separately, build models, match the variables of the test data set with the train. Finally, we will predict the test data and then evaluate our predictions.\r\n\r\n\r\nlibrary(caret) #For splitting \r\n\r\nset.seed(1234)\r\n\r\ntrainIndex <- createDataPartition(covid_sars$category, p = .8, \r\n                                  list = FALSE, \r\n                                  times = 1)\r\n\r\ntrain <- covid_sars[trainIndex,]\r\ntest  <- covid_sars[-trainIndex,]\r\n\r\ntable(train$category)\r\n\r\n\r\ncovid  sars \r\n  405   372 \r\n\r\ntable(test$category)\r\n\r\n\r\ncovid  sars \r\n  101    92 \r\n\r\nnrow(train)\r\n\r\n[1] 777\r\n\r\nText analysis\r\nIf you are interested in text analysis, I reccommend visiting Quanteda website. Quanteda is a great package for text analysis. One of the great advantage of quanteda is it is super fast, have so many powerful functions. I have tried five different R packages for text data. Based on my personal taste, I would rank them as follows: Quanteda, txt2vec, tidytext, tm.\r\nCreate Corpus\r\n\r\n\r\nrequire(quanteda)\r\n\r\n#Train data\r\ntr <- train[, 6] # the sixth variable is unique label. I will use it as identifier. \r\ntraincorpus <- corpus(train$abstract,\r\n                      docvars = data.frame(trainvars=names(tr)))\r\n#Test data\r\nts <- test[, 6]\r\n\r\ntestcorpus <- corpus(test$abstract,\r\n                     docvars = data.frame(testvars=names(ts)))\r\nsummary(traincorpus,2)\r\n\r\nCorpus consisting of 777 documents, showing 2 documents:\r\n\r\n  Text Types Tokens Sentences trainvars\r\n text1   165    337        11     label\r\n text2    94    164         5     label\r\n\r\n# Connect the labels with the corpuses\r\ndocid_train <- train$label\r\n\r\ndocnames(traincorpus) <- docid_train\r\nhead(traincorpus,1)\r\n\r\nCorpus consisting of 1 document and 1 docvar.\r\nAtaguba2020.1 :\r\n\"The coronavirus disease 2019 (COVID-19) pandemic has affecte...\"\r\n\r\ndocid_test <- test$label\r\ndocnames(testcorpus) <- docid_test\r\n\r\nsummary(testcorpus, 2)\r\n\r\nCorpus consisting of 193 documents, showing 2 documents:\r\n\r\n           Text Types Tokens Sentences testvars\r\n   Coccia2020.1   264    782        12    label\r\n Cagliani2020.1   182    337        16    label\r\n\r\nsummary(traincorpus, 4)\r\n\r\nCorpus consisting of 777 documents, showing 4 documents:\r\n\r\n          Text Types Tokens Sentences trainvars\r\n Ataguba2020.1   165    337        11     label\r\n  Sigala2020.1    94    164         5     label\r\n Lechner2020.1   103    160         7     label\r\n    Okba2020.1   118    176         6     label\r\n\r\nTokenize\r\n\r\n\r\ntraintokens <- tokens(traincorpus,\r\n                      remove_punct = TRUE,\r\n                      remove_url = TRUE,\r\n                      remove_numbers = TRUE)\r\n\r\ntraintokens <- tokens_remove(traintokens, \r\n                             pattern=stopwords('en'))\r\n\r\ntesttokens <- tokens(testcorpus,\r\n                     remove_punct = TRUE,\r\n                     remove_url = TRUE,\r\n                     remove_numbers = TRUE)\r\n\r\ntesttokens <- tokens_remove(testtokens, \r\n                             pattern=stopwords('en'))\r\n\r\n\r\nConstruct the DFM objects\r\n\r\n\r\ndfmat_train <- dfm(traintokens)\r\n\r\ndfmat_test <- dfm(testtokens)\r\n\r\nhead(dfmat_train,2)\r\n\r\nDocument-feature matrix of: 2 documents, 12,119 features (99.21% sparse) and 1 docvar.\r\n               features\r\ndocs            coronavirus disease covid-19 pandemic affected many\r\n  Ataguba2020.1           2       2        6        3        1    7\r\n  Sigala2020.1            0       0        4        1        0    0\r\n               features\r\ndocs            countries increasing morbidity mortality\r\n  Ataguba2020.1         7          1         2         1\r\n  Sigala2020.1          0          0         0         0\r\n[ reached max_nfeat ... 12,109 more features ]\r\n\r\nhead(dfmat_test,2)\r\n\r\nDocument-feature matrix of: 2 documents, 5,484 features (97.01% sparse) and 1 docvar.\r\n                features\r\ndocs             study two goals first explain geo-environmental\r\n  Coccia2020.1       3   1     1     1       1                 1\r\n  Cagliani2020.1     0   0     0     0       0                 0\r\n                features\r\ndocs             determinants accelerated diffusion covid-19\r\n  Coccia2020.1              1           3         3        9\r\n  Cagliani2020.1            0           0         0        0\r\n[ reached max_nfeat ... 5,474 more features ]\r\n\r\nThe training data has 12,119 features and is 99.2% sparse, while the test data has 5,484 features and 97% sparsity. I will not do anything to reduce the sparsity. But, you may have to do it if you have a large number of observations. Quanteda’s dfm_trim() can do that for you.\r\nTF-IDF weighting is known to improve prediction performance. I will use that here too.\r\n\r\n\r\ndfmat_train_tfidf <- dfm_tfidf(dfmat_train)\r\ndfmat_test_tfidf <- dfm_tfidf(dfmat_test)\r\n\r\n\r\nLet’s inspect the two tfidf data that were created above.\r\n\r\n\r\nhead(dfmat_train_tfidf, 2)\r\n\r\nDocument-feature matrix of: 2 documents, 12,119 features (99.21% sparse) and 1 docvar.\r\n               features\r\ndocs            coronavirus   disease covid-19  pandemic affected\r\n  Ataguba2020.1     0.86811 0.8990239 2.287311 1.8627242 1.182851\r\n  Sigala2020.1      0       0         1.524874 0.6209081 0       \r\n               features\r\ndocs                many countries increasing morbidity mortality\r\n  Ataguba2020.1 7.450424  6.911317   1.475448  3.319944  1.126993\r\n  Sigala2020.1  0         0          0         0         0       \r\n[ reached max_nfeat ... 12,109 more features ]\r\n\r\nhead(dfmat_test_tfidf,2)\r\n\r\nDocument-feature matrix of: 2 documents, 5,484 features (97.01% sparse) and 1 docvar.\r\n                features\r\ndocs                study      two    goals     first  explain\r\n  Coccia2020.1   1.659491 0.870584 1.808436 0.7057737 1.984527\r\n  Cagliani2020.1 0        0        0        0         0       \r\n                features\r\ndocs             geo-environmental determinants accelerated diffusion\r\n  Coccia2020.1            2.285557     2.285557    5.953582  5.425308\r\n  Cagliani2020.1          0            0           0         0       \r\n                features\r\ndocs             covid-19\r\n  Coccia2020.1   3.642693\r\n  Cagliani2020.1 0       \r\n[ reached max_nfeat ... 5,474 more features ]\r\n\r\nModel building\r\nWhy not logistic regression?\r\nMy data has two class labels(covid vs sars) and all numerical features. Why not logistic regression, then? Well, linear models provide great approaches to predictive modeling given that the assumptions are met! These assumptions (for example: hemoscidascity of variance) are violated when we have more number of features than observations (For example in genetics studies and text analysis, this is often the case). Applying linear models to such data results in biased coefficients, weaker prediction performance scores, overfitting or high out of sample prediction error problems. Hence, penalizing the estimated model coefficients was devised. This method of penalizing linear models is called “Regularization”. There are three most commonly used approaches to regularization for logistic regression: Ridge, LASSO, and Elastic net.\r\nIn Ridge penalty, the estimated model coefficients are penalized by adding the following parameter. SSE \\[\\begin{equation}\r\n\\ SSE = \\sum^n_{i=1} \\left(y_i - \\hat{y}_i\\right)^2\r\n\\end{equation}\\]\r\n\\[\\begin{equation} \\ SSE + \\lambda \\sum^p_{j=1} \\beta_j^2 \\\r\n\\end{equation}\\]\r\nThis is called L^2 norm. From the above equation if lambda equals 0, the model will be equal to the ordinary least squares model. As approaches to infinity, the penalty will force the model coefficients to be closer to zero but not completely to 0. Ridge penalty is known to systematically handling highly correlated features. In Lasso penalty the model coefficients are penalized by a L1 norm as follows. \\[\\begin{equation}SSE + \\lambda \\sum^p_{j=1} | \\beta_j |\r\n\\end{equation}\\]\r\nLasso penalty unlike ridge pushes all the coefficients all the way to zero. The advantage of Lasso is it improves model performance while also automating feature selection. Only the features that are important will be retianed on the final model.\r\nElastic net combines both Lasso and Ridge penalty parameters. Elastic net takes advantages of both Lasso and Ridge penalty: effective regularization by automated feature selection as well as effectively handling correlated features.\r\nImplementations\r\nProbably the most popular package to implement regualrized models is the glmnet package. This package is lightening fast to tune cross validated models. I watched one very nice webinar tutorial from Dr Trevor Hastie(one of the authors of this package). He mentioned that it is fast because it is programmed in Fortran. I invite you to watch that great webinar here I head I heard there are also other packages like H2O and elastic net. I have never tried any of them.\r\nFor regularized models, we have two main tuning parameters: alpha and lambda. In ridge and Lasso the lambda is the only tuning parameter but alpha is set to be 0 and 1, respectively. For tuning lambda, the cv.glmnet() function provides 100 different data driven lambda values and there is no need to do anything else. Since elastic net combines both Lasso and Ridge penalty, we will have two tuning parameters: alpha and lambda. Alpha can take a number values between between 0 and 1, while lambda can have 100 different data driven lambda values by just using cv.glmnet() function.\r\nAll models require the data to be in a matrix form. The good thing with quanteda is the document feature matrix is already a matrix object we don’t need to change the structure of our data. Notice the time required to tune the cross-validated algorithms.\r\nRidge\r\n\r\n\r\n# Ridge regression\r\nlibrary(glmnet)\r\nridge_1 <- glmnet(x = dfmat_train, y = train$category, \r\n                    alpha = 0, family = \"binomial\")\r\n#tfidf\r\nridge_1_tfidf <- glmnet(x = dfmat_train_tfidf, y = train$category, \r\n                    alpha = 0, family = \"binomial\")\r\n\r\npar(mfrow = c(1, 2))\r\nplot(ridge_1, xvar=\"lambda\", main=\"Ridge penalty\\n\\n\")\r\nplot(ridge_1_tfidf, xvar=\"lambda\", main=\"Ridge penalty tfidf\\n\\n\")\r\n\r\n\r\n\r\n\r\n\r\nx <- Sys.time()\r\n\r\nset.seed(123)\r\nridge_min <- cv.glmnet(x=dfmat_train,\r\n                   y=train$category,\r\n                   family=\"binomial\", \r\n                   alpha=0,  # alpha = 0 for ridge regression\r\n                   parallel=TRUE, \r\n                   intercept=TRUE)\r\n\r\n\r\nAgain using the tf-idf weighted data\r\n\r\n\r\nset.seed(123)\r\nridge_min_tfidf <- cv.glmnet(x=dfmat_train_tfidf,\r\n                   y=train$category,\r\n                   family=\"binomial\", \r\n                   alpha=0,  # alpha = 0 for ridge regression\r\n                   parallel=TRUE, \r\n                   intercept=TRUE)\r\n\r\npar(mfrow = c(1, 2))\r\nplot(ridge_min, main=\"Ridge penalty\\n\\n\")\r\nplot(ridge_min_tfidf, main=\"Ridge penalty_tfidf\\n\\n\")\r\n\r\n\r\nSys.time() - x\r\n\r\nTime difference of 18.70159 secs\r\n\r\nLet’s plot the results\r\n\r\n\r\npar(mfrow = c(1, 2))\r\nplot(ridge_1, xvar = \"lambda\", main = \"Ridge penalty\\n\\n\") \r\nabline(v=log(ridge_min$lambda.min), col = \"red\", lty = \"dashed\")\r\nabline(v=log(ridge_min$lambda.1se), col = \"blue\", lty = \"dashed\")\r\n\r\nplot(ridge_1_tfidf, xvar = \"lambda\", main = \"Ridge penalty tfidf\\n\\n\") \r\nabline(v=log(ridge_min_tfidf$lambda.min), col = \"red\", lty = \"dashed\")\r\nabline(v=log(ridge_min_tfidf$lambda.1se), col = \"blue\", lty = \"dashed\")\r\n\r\n\r\n\r\nPredict the test data sets\r\nBefore we predict the test data, we need to do one very key step. We will predict the test data based on the data that the model was trained. So, features on the test data should match with the features on the training data. Otherwise, the prediction will not work. The model cannot understand anything outside of the features that were in the training data. This is very key step in text prediction. Quanteda provides a nice function for that: dfm_match(). It subsets the features of the test data that were part of the training data.\r\n\r\n\r\ndfmat_matched <- dfm_match(dfmat_test, \r\n                           features = featnames(dfmat_train))\r\n# Match the tfi-idf\r\ndfmat_matched_tfidf <- dfm_match(dfmat_test_tfidf, \r\n                                 features = featnames(dfmat_train_tfidf))\r\n\r\n\r\nFor prediction, I will use the best model from the cross validated models. The best model lies between the model having the minimum lamda value and the model that has a lambda value within 1 se. Here, I will use the minimum lamda value.\r\n\r\n\r\n# Predict \r\nridge_min\r\n\r\n\r\nCall:  cv.glmnet(x = dfmat_train, y = train$category, parallel = TRUE,      family = \"binomial\", alpha = 0, intercept = TRUE) \r\n\r\nMeasure: Binomial Deviance \r\n\r\n    Lambda Index Measure      SE Nonzero\r\nmin  3.060   100   1.023 0.01754   12119\r\n1se  3.518    97   1.040 0.01666   12119\r\n\r\nridge_min$lambda.1se\r\n\r\n[1] 3.518117\r\n\r\nridge_min$lambda.min\r\n\r\n[1] 3.059879\r\n\r\nactual_class <- as.factor(test$category)\r\npredicted_class.ridge <- predict(ridge_min, newx=dfmat_matched,s=\"lambda.min\", type=\"class\")\r\n\r\ntab_class.ridge <- table(predicted_class.ridge, actual_class)\r\n\r\nconfusionmatrix_ridge <- confusionMatrix(tab_class.ridge, mode=\"everything\", positive=\"covid\")\r\n\r\n\r\n##tfidf\r\nridge_min_tfidf\r\n\r\n\r\nCall:  cv.glmnet(x = dfmat_train_tfidf, y = train$category, parallel = TRUE,      family = \"binomial\", alpha = 0, intercept = TRUE) \r\n\r\nMeasure: Binomial Deviance \r\n\r\n    Lambda Index Measure      SE Nonzero\r\nmin  3.060   100   1.023 0.01754   12119\r\n1se  3.518    97   1.040 0.01666   12119\r\n\r\nridge_min_tfidf$lambda.1se\r\n\r\n[1] 3.518117\r\n\r\nridge_min_tfidf$lambda.min\r\n\r\n[1] 3.059879\r\n\r\nactual_class_tfidf <- as.factor(test$category)\r\npredicted_class.ridge_tfidf <- predict(ridge_min_tfidf, newx=dfmat_matched_tfidf, s=\"lambda.min\", type=\"class\")\r\n\r\ntab_class.ridge_tfidf <- table(predicted_class.ridge_tfidf, actual_class)\r\n\r\nconfusionmatrix_ridge_tfidf <- confusionMatrix(tab_class.ridge_tfidf, mode=\"everything\", positive=\"covid\")\r\n\r\nconfusionmatrix_ridge\r\n\r\nConfusion Matrix and Statistics\r\n\r\n                     actual_class\r\npredicted_class.ridge covid sars\r\n                covid    85   17\r\n                sars     16   75\r\n                                          \r\n               Accuracy : 0.829           \r\n                 95% CI : (0.7683, 0.8793)\r\n    No Information Rate : 0.5233          \r\n    P-Value [Acc > NIR] : <2e-16          \r\n                                          \r\n                  Kappa : 0.6571          \r\n                                          \r\n Mcnemar's Test P-Value : 1               \r\n                                          \r\n            Sensitivity : 0.8416          \r\n            Specificity : 0.8152          \r\n         Pos Pred Value : 0.8333          \r\n         Neg Pred Value : 0.8242          \r\n              Precision : 0.8333          \r\n                 Recall : 0.8416          \r\n                     F1 : 0.8374          \r\n             Prevalence : 0.5233          \r\n         Detection Rate : 0.4404          \r\n   Detection Prevalence : 0.5285          \r\n      Balanced Accuracy : 0.8284          \r\n                                          \r\n       'Positive' Class : covid           \r\n                                          \r\n\r\nconfusionmatrix_ridge_tfidf\r\n\r\nConfusion Matrix and Statistics\r\n\r\n                           actual_class\r\npredicted_class.ridge_tfidf covid sars\r\n                      covid    86   17\r\n                      sars     15   75\r\n                                          \r\n               Accuracy : 0.8342          \r\n                 95% CI : (0.7741, 0.8837)\r\n    No Information Rate : 0.5233          \r\n    P-Value [Acc > NIR] : <2e-16          \r\n                                          \r\n                  Kappa : 0.6673          \r\n                                          \r\n Mcnemar's Test P-Value : 0.8597          \r\n                                          \r\n            Sensitivity : 0.8515          \r\n            Specificity : 0.8152          \r\n         Pos Pred Value : 0.8350          \r\n         Neg Pred Value : 0.8333          \r\n              Precision : 0.8350          \r\n                 Recall : 0.8515          \r\n                     F1 : 0.8431          \r\n             Prevalence : 0.5233          \r\n         Detection Rate : 0.4456          \r\n   Detection Prevalence : 0.5337          \r\n      Balanced Accuracy : 0.8334          \r\n                                          \r\n       'Positive' Class : covid           \r\n                                          \r\n\r\nSee, all features are retained in the final models. Looking at the prediction performances of the two, we can see that the tfidf-weighted data has better performance. We will try Lasso if we can improve that.\r\nLasso penalty\r\n\r\n\r\n## Lasso model\r\nlasso_1 <- glmnet(x = dfmat_train, y = train$category, \r\n                    alpha = 1, family = \"binomial\", type.measure=\"class\") \r\n\r\nlasso_1_tfidf <- glmnet(x = dfmat_train, y = train$category, \r\n                    alpha = 1, family = \"binomial\", type.measure=\"class\") \r\n\r\npar(mfrow=c(1,2))\r\n\r\nplot(lasso_1, xvar=\"lambda\", main=\"Lasso penalty\\n\\n\")\r\nplot(lasso_1_tfidf, xvar=\"lambda\", main=\"Lasso penalty tfidf\\n\\n\")\r\n\r\n\r\n\r\n\r\n\r\nx <- Sys.time()\r\n#registerDoMC(cores=2) # parallelize to speed up\r\nset.seed(123)\r\nlasso <- cv.glmnet(x=dfmat_train,\r\n                   y=train$category,\r\n                   family=\"binomial\", \r\n                   alpha=1,  # alpha = 1: LASSO\r\n                   parallel=TRUE, nfolds = 10,\r\n                   intercept=TRUE) \r\n\r\n# tfidf\r\nset.seed(123)\r\nlasso_tfidf <- cv.glmnet(x=dfmat_train_tfidf,\r\n                   y=train$category,\r\n                   family=\"binomial\", \r\n                   alpha=1,  # alpha = 1: LASSO\r\n                   parallel=TRUE, nfolds = 10,\r\n                   intercept=TRUE)\r\n\r\nSys.time() -x \r\n\r\nTime difference of 3.338095 secs\r\n\r\nlasso\r\n\r\n\r\nCall:  cv.glmnet(x = dfmat_train, y = train$category, nfolds = 10, parallel = TRUE,      family = \"binomial\", alpha = 1, intercept = TRUE) \r\n\r\nMeasure: Binomial Deviance \r\n\r\n     Lambda Index Measure      SE Nonzero\r\nmin 0.02369    56  0.4940 0.02140      65\r\n1se 0.03132    50  0.5107 0.01893      16\r\n\r\nlasso_tfidf\r\n\r\n\r\nCall:  cv.glmnet(x = dfmat_train_tfidf, y = train$category, nfolds = 10,      parallel = TRUE, family = \"binomial\", alpha = 1, intercept = TRUE) \r\n\r\nMeasure: Binomial Deviance \r\n\r\n     Lambda Index Measure      SE Nonzero\r\nmin 0.02369    56  0.4940 0.02139      66\r\n1se 0.03132    50  0.5107 0.01893      16\r\n\r\n\r\n\r\n# Plot lasso without cv and with cv to mark lamda.min and lamda.1se\r\n\r\npar(mfrow=c(1,2))\r\nplot(lasso_1, xvar=\"lambda\", main=\"Lasso penalty \\n\\n\")\r\nabline(v=log(lasso$lambda.min), col=\"red\", lty=\"dashed\")\r\nabline(v=log(lasso$lambda.1se), col=\"blue\", lty=\"dashed\")\r\n\r\nplot(lasso_1_tfidf, xvar=\"lambda\", main=\"Lasso penalty tfidf \\n\\n\")\r\nabline(v=log(lasso_tfidf$lambda.min), col=\"red\", lty=\"dashed\")\r\nabline(v=log(lasso_tfidf$lambda.1se), col=\"blue\", lty=\"dashed\")\r\n\r\n\r\n\r\n\r\n\r\npar(mfrow=c(1,2))\r\nplot(lasso,main=\"Lasso penalty\\n\\n\")\r\nplot(lasso_tfidf,main=\"Lasso penalty tfidf\\n\\n\")\r\n\r\n\r\n\r\n\r\n\r\n# Predict \r\nlasso\r\n\r\n\r\nCall:  cv.glmnet(x = dfmat_train, y = train$category, nfolds = 10, parallel = TRUE,      family = \"binomial\", alpha = 1, intercept = TRUE) \r\n\r\nMeasure: Binomial Deviance \r\n\r\n     Lambda Index Measure      SE Nonzero\r\nmin 0.02369    56  0.4940 0.02140      65\r\n1se 0.03132    50  0.5107 0.01893      16\r\n\r\nlasso_tfidf\r\n\r\n\r\nCall:  cv.glmnet(x = dfmat_train_tfidf, y = train$category, nfolds = 10,      parallel = TRUE, family = \"binomial\", alpha = 1, intercept = TRUE) \r\n\r\nMeasure: Binomial Deviance \r\n\r\n     Lambda Index Measure      SE Nonzero\r\nmin 0.02369    56  0.4940 0.02139      66\r\n1se 0.03132    50  0.5107 0.01893      16\r\n\r\nlasso$lambda.1se\r\n\r\n[1] 0.03131881\r\n\r\nlasso$lambda.min\r\n\r\n[1] 0.02369153\r\n\r\nactual_class <- as.factor(test$category)\r\npredicted_class.lasso <- predict(lasso, newx=dfmat_matched,s=\"lambda.min\", type=\"class\")\r\n\r\ntab_class.lasso <- table(predicted_class.lasso, actual_class)\r\n\r\nconfusion_matrix_lasso <- confusionMatrix(tab_class.lasso, mode=\"everything\", positive=\"covid\")\r\n\r\n\r\n##tfidf\r\nlasso_tfidf$lambda.1se\r\n\r\n[1] 0.03131881\r\n\r\nlasso_tfidf$lambda.min\r\n\r\n[1] 0.02369153\r\n\r\nactual_class_tfidf <- as.factor(test$category)\r\npredicted_class.lasso_tfidf <- predict(lasso_tfidf,\r\n                                       newx=dfmat_matched_tfidf,s=\"lambda.min\",\r\n                                       type=\"class\")\r\n\r\ntab_class.lasso_tfidf <- table(predicted_class.lasso_tfidf, actual_class)\r\n\r\nconfusion_matrix_lasso_tfidf <- confusionMatrix(tab_class.lasso_tfidf, mode=\"everything\", positive=\"covid\")\r\n\r\nconfusion_matrix_lasso\r\n\r\nConfusion Matrix and Statistics\r\n\r\n                     actual_class\r\npredicted_class.lasso covid sars\r\n                covid   100   11\r\n                sars      1   81\r\n                                          \r\n               Accuracy : 0.9378          \r\n                 95% CI : (0.8939, 0.9675)\r\n    No Information Rate : 0.5233          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.8748          \r\n                                          \r\n Mcnemar's Test P-Value : 0.009375        \r\n                                          \r\n            Sensitivity : 0.9901          \r\n            Specificity : 0.8804          \r\n         Pos Pred Value : 0.9009          \r\n         Neg Pred Value : 0.9878          \r\n              Precision : 0.9009          \r\n                 Recall : 0.9901          \r\n                     F1 : 0.9434          \r\n             Prevalence : 0.5233          \r\n         Detection Rate : 0.5181          \r\n   Detection Prevalence : 0.5751          \r\n      Balanced Accuracy : 0.9353          \r\n                                          \r\n       'Positive' Class : covid           \r\n                                          \r\n\r\nconfusion_matrix_lasso_tfidf\r\n\r\nConfusion Matrix and Statistics\r\n\r\n                           actual_class\r\npredicted_class.lasso_tfidf covid sars\r\n                      covid   100   11\r\n                      sars      1   81\r\n                                          \r\n               Accuracy : 0.9378          \r\n                 95% CI : (0.8939, 0.9675)\r\n    No Information Rate : 0.5233          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.8748          \r\n                                          \r\n Mcnemar's Test P-Value : 0.009375        \r\n                                          \r\n            Sensitivity : 0.9901          \r\n            Specificity : 0.8804          \r\n         Pos Pred Value : 0.9009          \r\n         Neg Pred Value : 0.9878          \r\n              Precision : 0.9009          \r\n                 Recall : 0.9901          \r\n                     F1 : 0.9434          \r\n             Prevalence : 0.5233          \r\n         Detection Rate : 0.5181          \r\n   Detection Prevalence : 0.5751          \r\n      Balanced Accuracy : 0.9353          \r\n                                          \r\n       'Positive' Class : covid           \r\n                                          \r\n\r\nThe best model in the end retains 65 variables. If we used tf-idf weighting, the best model retains 69 of the 1219 variables. That is great way to reduce the irrelevant features. We can have a look at some of these variables using the vippackage. VIP ranks based on their importance scores.\r\n\r\n\r\nlibrary(vip)\r\nvip(lasso_tfidf, 10)\r\n\r\n\r\n\r\nLet’s see if this this variables are also the main features in the ridge model.\r\n\r\n\r\nvip(ridge_min_tfidf, 10)\r\n\r\n\r\n\r\nElastic net\r\nIn my experience with text data, I found elastic net regression having problems with the matrix format of the dfm objects resulted from quanteda. I will have to convert the matrix formats to data frames.\r\n\r\n\r\nda_train <- cbind(category=train$category, \r\n                  convert(dfmat_train, to=\"data.frame\")) \r\n\r\nda_train_tfidf <- cbind(category=train$category, \r\n                        convert(dfmat_train_tfidf, to=\"data.frame\")) \r\n\r\n\r\n\r\n\r\nncol(dfmat_train)\r\n\r\n[1] 12119\r\n\r\nncol(da_train)\r\n\r\n[1] 12121\r\n\r\nda_train <- da_train[,-2] #the document identifier variable should be removed\r\nncol(da_train)\r\n\r\n[1] 12120\r\n\r\n## The tfidf version \r\nncol(dfmat_train_tfidf)\r\n\r\n[1] 12119\r\n\r\nncol(da_train_tfidf)\r\n\r\n[1] 12121\r\n\r\nda_train_tfidf <- da_train_tfidf[,-2] #the document identifier variable should be removed\r\nncol(da_train_tfidf)\r\n\r\n[1] 12120\r\n\r\n\r\n\r\nda_train_xmatrix <- da_train[,-1]  %>% as.data.frame() %>% as.matrix() \r\nda_train_xdf <- da_train  %>% as.data.frame()\r\n\r\n#for the tf-idf pre-processed data \r\n\r\nda_train_xmatrix_tfidf <- da_train_tfidf[,-1]  %>% as.data.frame() %>% as.matrix() \r\n\r\nda_train_xdf_tfidf <- da_train_tfidf  %>% as.data.frame()\r\n\r\n\r\n\r\n\r\nda_test_match <- cbind(category=test$category, convert(dfmat_matched, to=\"data.frame\")) \r\n\r\nda_test_match <- da_test_match[,-2] \r\nncol(da_test_match)\r\n\r\n[1] 12120\r\n\r\nda_test_xmatrix <- da_test_match[,-1]  %>% as.data.frame() %>% as.matrix() \r\nncol(dfmat_matched)\r\n\r\n[1] 12119\r\n\r\nncol(da_test_xmatrix)\r\n\r\n[1] 12119\r\n\r\n# Do the same for the tfidf data\r\n\r\nda_test_match_tfidf <- cbind(category=test$category, convert(dfmat_matched_tfidf, to=\"data.frame\")) \r\n\r\nda_test_match_tfidf <- da_test_match_tfidf[,-2] #the document identifier variable should be removed\r\nncol(da_test_match_tfidf)\r\n\r\n[1] 12120\r\n\r\nda_test_xmatrix_tfidf <- da_test_match_tfidf[,-1]  %>% as.data.frame() %>% as.matrix() # remove the dependent variable\r\n\r\nncol(dfmat_matched_tfidf)\r\n\r\n[1] 12119\r\n\r\nncol(da_test_xmatrix_tfidf)\r\n\r\n[1] 12119\r\n\r\n\r\n\r\n# Fit elastic net regression with 10 different alpha values from 0 to 1\r\n\r\nx <- Sys.time()\r\n\r\nset.seed(223)\r\ny=ifelse(da_train_xdf$category==\"covid\", \"1\", \"0\") # convert to numeric labels\r\n                       \r\ncv_glmnet_10_roc <- train(x = da_train_xdf[,-1], \r\n                          y = y, type.measure=\"auc\", method=\"glmnet\",\r\n                          family=\"binomial\", \r\n                          traControl=trainControl(method=\"cv\", number=10),\r\n                          parallel=TRUE,\r\n                          tuneLength=10) # I will use 10 different alpha values between 0 and 1\r\n\r\nx-Sys.time()\r\n\r\nTime difference of -6.203993 mins\r\n\r\n#tfidf\r\nx <- Sys.time()\r\n\r\nset.seed(223)\r\ncv_glmnet_10_roc_tfidf <- train(x = da_train_xdf_tfidf[,-1], \r\n                                y = y, type.measure=\"auc\", method=\"glmnet\",\r\n                                family=\"binomial\",\r\n                              traControl=trainControl(method=\"cv\",number=10), \r\n                                parallel=TRUE,\r\n                                tuneLength=10) \r\n\r\nx-Sys.time()\r\n\r\nTime difference of -5.264415 mins\r\n\r\nLet’s visualize the two models\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\nggplot(cv_glmnet_10_roc)\r\n\r\n\r\n#Tf-idf\r\nggplot(cv_glmnet_10_roc_tfidf)\r\n\r\n\r\n\r\n\r\n\r\n## Predict using the belastic model cv_glmnet_50\r\npredicted_class.elastic_10 <- predict(cv_glmnet_10_roc, \r\n                                      da_test_xmatrix, \r\n                                      cv_glmnet_10_roc$lamda.min,\r\n                                      type=\"raw\")\r\n\r\npredicted_class.elastic_10 <- as.factor(ifelse(predicted_class.elastic_10==0, \"sars\", \"covid\"))\r\n\r\nconfusion_mat_elastic_net <- confusionMatrix(predicted_class.elastic_10, \r\n                                      actual_class, mode=\"everything\",\r\n                                      positive=\"covid\") \r\n\r\n#Predict the tfidf weighted data\r\n\r\npredicted_class.elastic_10_tfidf <- predict(cv_glmnet_10_roc_tfidf,\r\n                                            da_test_xmatrix_tfidf,\r\n                                            cv_glmnet_10_tfidf$lamda.min, \r\n                                            type=\"raw\")\r\n\r\npredicted_class.elastic_10_tfidf <- as.factor(ifelse(predicted_class.elastic_10_tfidf==0, \"sars\", \"covid\"))\r\nconfusion_mat_elastic_net_tfidf <- confusionMatrix(predicted_class.elastic_10_tfidf, \r\n                                            actual_class, \r\n                                            mode=\"everything\", positive=\"covid\") \r\n\r\nconfusion_mat_elastic_net\r\n\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction covid sars\r\n     covid   100   11\r\n     sars      1   81\r\n                                          \r\n               Accuracy : 0.9378          \r\n                 95% CI : (0.8939, 0.9675)\r\n    No Information Rate : 0.5233          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.8748          \r\n                                          \r\n Mcnemar's Test P-Value : 0.009375        \r\n                                          \r\n            Sensitivity : 0.9901          \r\n            Specificity : 0.8804          \r\n         Pos Pred Value : 0.9009          \r\n         Neg Pred Value : 0.9878          \r\n              Precision : 0.9009          \r\n                 Recall : 0.9901          \r\n                     F1 : 0.9434          \r\n             Prevalence : 0.5233          \r\n         Detection Rate : 0.5181          \r\n   Detection Prevalence : 0.5751          \r\n      Balanced Accuracy : 0.9353          \r\n                                          \r\n       'Positive' Class : covid           \r\n                                          \r\n\r\nconfusion_mat_elastic_net_tfidf\r\n\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction covid sars\r\n     covid   100   11\r\n     sars      1   81\r\n                                          \r\n               Accuracy : 0.9378          \r\n                 95% CI : (0.8939, 0.9675)\r\n    No Information Rate : 0.5233          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.8748          \r\n                                          \r\n Mcnemar's Test P-Value : 0.009375        \r\n                                          \r\n            Sensitivity : 0.9901          \r\n            Specificity : 0.8804          \r\n         Pos Pred Value : 0.9009          \r\n         Neg Pred Value : 0.9878          \r\n              Precision : 0.9009          \r\n                 Recall : 0.9901          \r\n                     F1 : 0.9434          \r\n             Prevalence : 0.5233          \r\n         Detection Rate : 0.5181          \r\n   Detection Prevalence : 0.5751          \r\n      Balanced Accuracy : 0.9353          \r\n                                          \r\n       'Positive' Class : covid           \r\n                                          \r\n\r\nFinal remarks\r\nNotice, Lasso and Elastic net models gave us superior prediction performances. A model with a sensitivity of 99%, and a specificity of 88%, and precision of more than 90% is extraordinarily superior to me! Sensitivity and specificity are not affected by prevalence. But, precision(positive predictive value) and negative predictive values are influenced by prevalence.It is possible to calculate confidence intervals for sensitivity and specificity. But, I will not do that here. Since the prevalence of covid abstracts is high(52%), the precission of my predictive models are all high. This may not be the case if your model is dealing with rare cases.\r\nThis book by Bradley Boehmke & Brandon Greenwell is a good reference to learn about machine learning. It is freely available. But, you can also buy a hard copy. It is one of my favorite machine learning books.\r\nContact\r\n@MihiretuKebede1\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-09-2020-12-09-documentclassification-using-regularized-models/2020-12-09-documentclassification-using-regularized-models_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2023-03-23T22:36:51+01:00",
    "input_file": "2020-12-09-documentclassification-using-regularized-models.knit.md",
    "preview_width": 1950,
    "preview_height": 1200
  },
  {
    "path": "posts/2021-06-07-scraping-individual-participant-data-from-scatter-plots/",
    "title": "Scraping individual participant data from scatter plots",
    "description": "In this blog post, I demonstrated how to easily scrap individual participant data by digitizing a scatter plot of an old publication.",
    "author": [
      {
        "name": "Mihiretu Kebede(PhD)",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nHow do we extract values?\r\nContact\r\n\r\n\r\nIntroduction\r\nA scatter plot aka scatter diagram is one of the most commonly used graph to display the relationship between two or more variables. By looking at scatter plots, we can quickly have an insight on whether two or more variables are linearly, negatively/positively or how strongly they are correlated to each other.\r\nScatter plots are often reported in scientific publications. Though many scientific publications report scatter plots to display relationships, correlation statistics may not be reported along with the scatter plots. Fortunately, scatter plots opens door to open science. With the help of new tools such as WebPlotDigitizer web based point and click software, plotdigitizer python python package or the digitize R packages, we can easily digitize scatter plots, scrap individual participant data and estimate correlation values. If you are working in systematic reviews, you may not find all relevant data from the reported papers. You either need to contact the authors or find out mechanisms of estimating values from reported data. I will be back on another blog post on estimating some values from reported data.\r\nHow do we extract values?\r\nI will step by step demonstrate how you can easily extract individual participant data from a scatter plot of an old publication. For detail explanations, please check the following YouTube video. <https://www.youtube.com/watch?v=3NI4CyJzJhM&t=344s>. I often do R or Python data science tutorials and live coding sessions in my YouTube channel. Please consider subscribing. That will encourage me for sharing more contents.\r\n\r\n\r\n\r\n\r\nThe scatter plot that I will use is from the 1994 publication by Strain G. and colleagues. The results are interesting and I invite you reading the paper.\r\n\r\n\r\n\r\nFor this blog post, I will be using Fig 1 which shows the relationship of fasting insulin level to BMI in non-dieting weight-stable subjects.\r\n\r\n\r\n\r\nNow, we have what we need and let’s go straight to extracting individual participant data from the scatter plot.\r\nWhat we need to do is take a screen shot of the scatter plot, save it as png or JPG, read it in R, calibrate the x and y axes .\r\nRead and calibrate the figure (mark the beginning and the end of x and y axis)\r\n\r\n\r\nlibrary(digitize)\r\nfig <- ReadAndCal('F:/github/githubwebsite/_posts/2021-06-07-scraping-individual-participant-data-from-scatter-plots/scatterPlotdigitize.JPG')\r\n\r\n\r\n\r\n\r\n\r\n\r\nMark data points\r\n\r\n\r\ndata.points = DigitData(col='red')\r\n\r\n\r\n\r\n\r\n\r\nExtract the data point in a data frame\r\n\r\n\r\ndf <- Calibrate(data.points, fig, 0, 100, 0, 100) #determin where x and y axis values start and end\r\n\r\n\r\n\r\n\r\n\r\nNow we have extracted the x which was BMI and the y (insulin level). We can easily recalculate the correlation and Beta coefficient values.\r\n\r\n\r\ncor(df$x, df$y, method = 'pearson')\r\n\r\n\r\n\r\n\r\n\r\nEstimate the regression coefficient and compare with the reported value.\r\n\r\n\r\nsummary(lm(y~x, data=df))\r\n\r\n\r\nI estimated the correlation value to be 0.709 and the Beta coefficient to be about 0.9605. The tiny difference is due to my calibration. If you zoom and carefully calibrate, you can have approximately similar values.\r\n\r\n\r\n\r\nThank you for reading this post. I hope you find this helpful. For more, subscribe to my YouTube channel and follow me on Twitter @RPy_DataScience.You can also follow me by liking R_Py Data Science Facebook page.\r\nContact\r\nPlease mention @RPy_DataScience if you tweet this post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-07-scraping-individual-participant-data-from-scatter-plots/digitize.jpg",
    "last_modified": "2023-03-28T23:39:52+02:00",
    "input_file": "scraping-individual-participant-data-from-scatter-plots.knit.md"
  },
  {
    "path": "posts/NA-corr-plot-python-vs-r/",
    "title": "Corr plot and Pair plots matrix: Python vs R",
    "description": "Correlation heat map in R and Python",
    "author": [
      {
        "name": "Mihiretu Kebede(PhD)",
        "url": {}
      }
    ],
    "date": "2021-05-17",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nContact\r\n\r\n\r\nIntroduction\r\nCorrelation is often used to test the relationship between two or more variables. Pearson product moment correlation is the most commonly used method of calculating correlation coefficient. It mainly measures the linear relationship between variables. Spearman and Kendal rank correlations are a non-parametric measures of correlation.\r\nPearson correlation requires few assumptions to be met: linearity, normality, and absence of outliers. These assumptions should be met before doing the correlation. You can use Shapiro wilk test to to check whether a variable is normally is distributed using a simple command in r: shapiro.test(data$var). This test gives you W statistic and a p value. A p value of greater than 0.05 means you can assume “normally” in your variable.\r\n\\[\r\nr_{x y}=\\frac{n \\sum x_{i} y_{i}-\\sum x_{i} \\sum y_{i}}{\\sqrt{n \\sum x_{i}^{2}-\\left(\\sum x_{i}\\right)^{2}} \\sqrt{n \\sum y_{i}^{2}-\\left(\\sum y_{i}\\right)^{2}}}\r\n\\] rxy = Pearson r correlation coefficient between ,\r\nn = number of observations\r\nxi = observed value of x for ith observation,\r\nyi = observed value of y or ith observation\r\nWithout spending much time on the theory, let’s go directly to creating a correlation matrix and then we will visualize with a heat map in R and Python. Let’s use the cancer classification data.\r\nFor detail explanations. Please check the following video. https://www.youtube.com/watch?v=Z7ggbnj0dM0&t=2s\r\n\r\n\r\n\r\n\r\n\r\n\r\nlibrary(reticulate)\r\nr_data  <- read.csv(\"cancer_classification.csv\")\r\ndim(r_data)\r\n\r\n[1] 569  31\r\n\r\ncolnames(r_data) \r\n\r\n [1] \"mean.radius\"             \"mean.texture\"           \r\n [3] \"mean.perimeter\"          \"mean.area\"              \r\n [5] \"mean.smoothness\"         \"mean.compactness\"       \r\n [7] \"mean.concavity\"          \"mean.concave.points\"    \r\n [9] \"mean.symmetry\"           \"mean.fractal.dimension\" \r\n[11] \"radius.error\"            \"texture.error\"          \r\n[13] \"perimeter.error\"         \"area.error\"             \r\n[15] \"smoothness.error\"        \"compactness.error\"      \r\n[17] \"concavity.error\"         \"concave.points.error\"   \r\n[19] \"symmetry.error\"          \"fractal.dimension.error\"\r\n[21] \"worst.radius\"            \"worst.texture\"          \r\n[23] \"worst.perimeter\"         \"worst.area\"             \r\n[25] \"worst.smoothness\"        \"worst.compactness\"      \r\n[27] \"worst.concavity\"         \"worst.concave.points\"   \r\n[29] \"worst.symmetry\"          \"worst.fractal.dimension\"\r\n[31] \"benign_0__mal_1\"        \r\n\r\nNow, let’s read this in Python. I am using reticulate package to run r and python scripts in one RMD file.\r\n\r\nimport pandas as pd\r\npy_data  = pd.read_csv(\"cancer_classification.csv\")\r\n\r\nThe data has 31 variables and 569 observations. I will only select the first six variables for my correlation plot.\r\nNotice in R counting starts from 1 while in Python it starts from 0.\r\n\r\n\r\nfilter_r_data  <- r_data[, c(1:6)]\r\nlength(filter_r_data)\r\n\r\n[1] 6\r\n\r\ndim(filter_r_data)\r\n\r\n[1] 569   6\r\n\r\nNow in Python:\r\n\r\nfilter_py_data = py_data.iloc[:,0:6]\r\nlen(filter_py_data)\r\nfilter_py_data.shape\r\n\r\nNext is producing the pearson correlation matrix in R and Python\r\n\r\n\r\nr_corr_matrix <-  cor(filter_r_data, method=\"pearson\")\r\nr_corr_matrix\r\n\r\n                 mean.radius mean.texture mean.perimeter mean.area\r\nmean.radius        1.0000000   0.32378189      0.9978553 0.9873572\r\nmean.texture       0.3237819   1.00000000      0.3295331 0.3210857\r\nmean.perimeter     0.9978553   0.32953306      1.0000000 0.9865068\r\nmean.area          0.9873572   0.32108570      0.9865068 1.0000000\r\nmean.smoothness    0.1705812  -0.02338852      0.2072782 0.1770284\r\nmean.compactness   0.5061236   0.23670222      0.5569362 0.4985017\r\n                 mean.smoothness mean.compactness\r\nmean.radius           0.17058119        0.5061236\r\nmean.texture         -0.02338852        0.2367022\r\nmean.perimeter        0.20727816        0.5569362\r\nmean.area             0.17702838        0.4985017\r\nmean.smoothness       1.00000000        0.6591232\r\nmean.compactness      0.65912322        1.0000000\r\n\r\nNow, in Python\r\n\r\npy_cor_matrix = filter_py_data.corr(method=\"pearson\")\r\n\r\nWe have now produced our pearson correlation matrix in both R and Python. Next is visualizing them using corr heat maps. I use correplot package in R and matplotlib and seaborn libraries for producing for producing the correlation heatmaps in both languages\r\nThe corrplot() function has numerous additional arguments which you can play with. I will use few of them here.\r\n\r\n\r\nlibrary(corrplot)\r\ncorrplot(r_corr_matrix, method =\"shade\", addCoef.col=TRUE )\r\n\r\n\r\n\r\nNow, in Python\r\n\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\n\r\n\r\nsns.heatmap(py_cor_matrix, annot=True, cmap=\"viridis\")\r\nplt.show()\r\n\r\n\r\nI personally like the plot by seaborn. But, it is a personal taste.\r\nContact\r\nPlease mention @MihiretuKebede1 if you tweet this post.\r\n\r\n\r\n\r\n",
    "preview": "posts/NA-corr-plot-python-vs-r/2021-05-13-corr-plot-python-vs-r_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2023-03-23T21:40:22+01:00",
    "input_file": "2021-05-13-corr-plot-python-vs-r.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-13-corr-plot-python-vs-r/",
    "title": "Corr plot and Pair plots matrix: Python vs R",
    "description": "Correlation heat map in R and Python",
    "author": [
      {
        "name": "Mihiretu Kebede(PhD)",
        "url": {}
      }
    ],
    "date": "2021-05-13",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nContact\r\n\r\nIntroduction\r\nCorrelation is often used to test the relationship between two or more variables. Pearson product moment correlation is the most commonly used method of calculating correlation coefficient. It mainly measures the linear relationship between variables. Spearman and Kendal rank correlations are a non-parametric measures of correlation.\r\nPearson correlation requires few assumptions to be met: linearity, normality, and absence of outliers. These assumptions should be met before doing the correlation. You can use Shapiro wilk test to to check whether a variable is normally is distributed using a simple command in r: shapiro.test(data$var). This test gives you W statistic and a p value. A p value of greater than 0.05 means you can assume “normally” in your variable.\r\n\\[\r\nr_{x y}=\\frac{n \\sum x_{i} y_{i}-\\sum x_{i} \\sum y_{i}}{\\sqrt{n \\sum x_{i}^{2}-\\left(\\sum x_{i}\\right)^{2}} \\sqrt{n \\sum y_{i}^{2}-\\left(\\sum y_{i}\\right)^{2}}}\r\n\\] rxy = Pearson r correlation coefficient between ,\r\nn = number of observations\r\nxi = observed value of x for ith observation, yi = observed value of y or ith observation\r\nWithout spending much time on the theory, let’s go directly to creating a correlation matrix and then we will visualize with a heat map in R and Python. Let’s use the cancer classification data.\r\nFor detail explanations. Please check the following video.\r\n\r\n\r\n\r\n\r\nr_data  <- read.csv(\"cancer_classification.csv\")\r\ndim(r_data)\r\n\r\n\r\n[1] 569  31\r\n\r\ncolnames(r_data) \r\n\r\n\r\n [1] \"mean.radius\"             \"mean.texture\"           \r\n [3] \"mean.perimeter\"          \"mean.area\"              \r\n [5] \"mean.smoothness\"         \"mean.compactness\"       \r\n [7] \"mean.concavity\"          \"mean.concave.points\"    \r\n [9] \"mean.symmetry\"           \"mean.fractal.dimension\" \r\n[11] \"radius.error\"            \"texture.error\"          \r\n[13] \"perimeter.error\"         \"area.error\"             \r\n[15] \"smoothness.error\"        \"compactness.error\"      \r\n[17] \"concavity.error\"         \"concave.points.error\"   \r\n[19] \"symmetry.error\"          \"fractal.dimension.error\"\r\n[21] \"worst.radius\"            \"worst.texture\"          \r\n[23] \"worst.perimeter\"         \"worst.area\"             \r\n[25] \"worst.smoothness\"        \"worst.compactness\"      \r\n[27] \"worst.concavity\"         \"worst.concave.points\"   \r\n[29] \"worst.symmetry\"          \"worst.fractal.dimension\"\r\n[31] \"benign_0__mal_1\"        \r\n\r\nNow, let’s read this in Python. I am using reticulate package to run r and python scripts in one RMD file.\r\n\r\nimport pandas as pd\r\npy_data  = pd.read_csv(\"cancer_classification.csv\")\r\n\r\nThe data has 31 variables and 569 observations. I will only select the first six variables for my correlation plot.\r\nNotice in R counting starts from 1 while in Python it starts from 0.\r\n\r\n\r\nfilter_r_data  <- r_data[, c(1:6)]\r\nlength(filter_r_data)\r\n\r\n\r\n[1] 6\r\n\r\ndim(filter_r_data)\r\n\r\n\r\n[1] 569   6\r\n\r\nNow in Python:\r\n\r\nfilter_py_data = py_data.iloc[:,0:6]\r\nlen(filter_py_data)\r\n569\r\nfilter_py_data.shape\r\n(569, 6)\r\n\r\nNext is producing the pearson correlation matrix in R and Python\r\n\r\n\r\nr_corr_matrix <-  cor(filter_r_data, method=\"pearson\")\r\nr_corr_matrix\r\n\r\n\r\n                 mean.radius mean.texture mean.perimeter mean.area\r\nmean.radius        1.0000000   0.32378189      0.9978553 0.9873572\r\nmean.texture       0.3237819   1.00000000      0.3295331 0.3210857\r\nmean.perimeter     0.9978553   0.32953306      1.0000000 0.9865068\r\nmean.area          0.9873572   0.32108570      0.9865068 1.0000000\r\nmean.smoothness    0.1705812  -0.02338852      0.2072782 0.1770284\r\nmean.compactness   0.5061236   0.23670222      0.5569362 0.4985017\r\n                 mean.smoothness mean.compactness\r\nmean.radius           0.17058119        0.5061236\r\nmean.texture         -0.02338852        0.2367022\r\nmean.perimeter        0.20727816        0.5569362\r\nmean.area             0.17702838        0.4985017\r\nmean.smoothness       1.00000000        0.6591232\r\nmean.compactness      0.65912322        1.0000000\r\n\r\nNow, in Python\r\n\r\npy_cor_matrix = filter_py_data.corr(method=\"pearson\")\r\n\r\nWe have now produced our pearson correlation matrix in both R and Python. Next is visualizing them using corr heat maps. I use correplot package in R and matplotlib and seaborn libraries for producing for producing the correlation heatmaps in both languages\r\nThe corrplot() function has numerous additional arguments which you can play with. I will use few of them here.\r\n\r\n\r\nlibrary(corrplot)\r\ncorrplot(r_corr_matrix, method =\"shade\", addCoef.col=TRUE )\r\n\r\n\r\n\r\n\r\nNow, in Python\r\n\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\n\r\n\r\nsns.heatmap(py_cor_matrix, annot=True, cmap=\"viridis\")\r\nplt.show()\r\n\r\n\r\nI personally like the plot by seaborn. But, it is a personal taste.\r\nContact\r\nPlease mention @MihiretuKebede1 if you tweet this post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-13-corr-plot-python-vs-r/corr-plot-python-vs-r_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-05-13T23:21:28+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-09-30-2020-09-30-plotting-model-coefficients-in-a-forest-plot/",
    "title": "Plotting regression model coefficients in a forest plot",
    "description": "How to plot model coefficients in a forest plot.",
    "author": [
      {
        "name": "Mihiretu Kebede(PhD)",
        "url": {}
      }
    ],
    "date": "2021-02-04",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\nThe detail description of this blog is also available in my YouTube channel.. Each content of this blog were based on my three YouTube videos embedded below. I highly recommend watching them.\r\nPart 1\r\n\r\n\r\n\r\nPart 2\r\n\r\n\r\n\r\nPart 3\r\n\r\n\r\n\r\nMotivation\r\nA friend of mine asked me to plot regression coefficients or odds ratios/risk ratios on a forest plots. Since my favorite part of data analysis is visualization, I happily took the challenge. R is a great language for powerful visualizations. As I promised in my previous blog post, I will describe how to visualize model coefficients/OR/RR in R. and their confidence intervals. For today, I will perform linear regression and logistic regression models. Then, I will extract coefficients(for linear regression) and ORs (for logistic regression). Many may prefer to present their analysis in tables. Compared to long and messy tables, plots can help us to quickly see patterns and grasp the the idea.\r\nA great novelist may use fancy words to describe the beauty of Afework Tekele’s “Meskel Flower”, but it can never be as memorable as the 1959’s Afework Tekle’s master piece.\r\n\r\n\r\n\r\nFigure 1: Meskel Flower by Afework Tekle, 1959\r\n\r\n\r\n\r\nData\r\nThe data that I will use for this blog post is from the Global Health Observatory (GHO) data repository. The data is publicly available data and has collected some economic and health related variables a long the years for 193 WHO member states. I will also use this data for my future post about missing value imputation methods. You are free to download a copy of this data from my GitHub repository or from the original source.\r\nI will use the readrpackage to read the csv file and use glm and lm to do the linear regression and logistic regression modeling. I will mention other relevant packages and their use along the way.\r\nLoad the data\r\n\r\n\r\nlibrary(readr) #for reading the csv file\r\nlibrary(dplyr) #for data manipulation\r\nlife_expe <- read_csv(\"life_expectancy.csv\")\r\nnames(life_expe) \r\n\r\n [1] \"Country\"                        \r\n [2] \"Year\"                           \r\n [3] \"Status\"                         \r\n [4] \"Life expectancy\"                \r\n [5] \"Adult Mortality\"                \r\n [6] \"infant deaths\"                  \r\n [7] \"Alcohol\"                        \r\n [8] \"percentage expenditure\"         \r\n [9] \"Hepatitis B\"                    \r\n[10] \"Measles\"                        \r\n[11] \"BMI\"                            \r\n[12] \"under-five deaths\"              \r\n[13] \"Polio\"                          \r\n[14] \"Total expenditure\"              \r\n[15] \"Diphtheria\"                     \r\n[16] \"HIV/AIDS\"                       \r\n[17] \"GDP\"                            \r\n[18] \"Population\"                     \r\n[19] \"thinness  1-19 years\"           \r\n[20] \"thinness 5-9 years\"             \r\n[21] \"Income composition of resources\"\r\n[22] \"Schooling\"                      \r\n\r\nLook at the variable names. They are not syntactically valid. That is why R puts variables inside the back tick and forward tick signs. I will use my favorite data cleaning package janitor, to quickly clean that messy variable naming with just one magically powerful function.\r\n\r\n\r\nlibrary(janitor)\r\nlife_expe_data <- clean_names(life_expe)\r\nnames(life_expe_data) # All variables names are automagically cleaned!\r\n\r\n [1] \"country\"                        \r\n [2] \"year\"                           \r\n [3] \"status\"                         \r\n [4] \"life_expectancy\"                \r\n [5] \"adult_mortality\"                \r\n [6] \"infant_deaths\"                  \r\n [7] \"alcohol\"                        \r\n [8] \"percentage_expenditure\"         \r\n [9] \"hepatitis_b\"                    \r\n[10] \"measles\"                        \r\n[11] \"bmi\"                            \r\n[12] \"under_five_deaths\"              \r\n[13] \"polio\"                          \r\n[14] \"total_expenditure\"              \r\n[15] \"diphtheria\"                     \r\n[16] \"hiv_aids\"                       \r\n[17] \"gdp\"                            \r\n[18] \"population\"                     \r\n[19] \"thinness_1_19_years\"            \r\n[20] \"thinness_5_9_years\"             \r\n[21] \"income_composition_of_resources\"\r\n[22] \"schooling\"                      \r\n\r\nglimpse(life_expe_data)\r\n\r\nRows: 2,938\r\nColumns: 22\r\n$ country                         <chr> \"Afghanistan\", \"Afghanistan\"~\r\n$ year                            <dbl> 2015, 2014, 2013, 2012, 2011~\r\n$ status                          <chr> \"Developing\", \"Developing\", ~\r\n$ life_expectancy                 <dbl> 65.0, 59.9, 59.9, 59.5, 59.2~\r\n$ adult_mortality                 <dbl> 263, 271, 268, 272, 275, 279~\r\n$ infant_deaths                   <dbl> 62, 64, 66, 69, 71, 74, 77, ~\r\n$ alcohol                         <dbl> 0.01, 0.01, 0.01, 0.01, 0.01~\r\n$ percentage_expenditure          <dbl> 71.279624, 73.523582, 73.219~\r\n$ hepatitis_b                     <dbl> 65, 62, 64, 67, 68, 66, 63, ~\r\n$ measles                         <dbl> 1154, 492, 430, 2787, 3013, ~\r\n$ bmi                             <dbl> 19.1, 18.6, 18.1, 17.6, 17.2~\r\n$ under_five_deaths               <dbl> 83, 86, 89, 93, 97, 102, 106~\r\n$ polio                           <dbl> 6, 58, 62, 67, 68, 66, 63, 6~\r\n$ total_expenditure               <dbl> 8.16, 8.18, 8.13, 8.52, 7.87~\r\n$ diphtheria                      <dbl> 65, 62, 64, 67, 68, 66, 63, ~\r\n$ hiv_aids                        <dbl> 0.1, 0.1, 0.1, 0.1, 0.1, 0.1~\r\n$ gdp                             <dbl> 584.25921, 612.69651, 631.74~\r\n$ population                      <dbl> 33736494, 327582, 31731688, ~\r\n$ thinness_1_19_years             <dbl> 17.2, 17.5, 17.7, 17.9, 18.2~\r\n$ thinness_5_9_years              <dbl> 17.3, 17.5, 17.7, 18.0, 18.2~\r\n$ income_composition_of_resources <dbl> 0.479, 0.476, 0.470, 0.463, ~\r\n$ schooling                       <dbl> 10.1, 10.0, 9.9, 9.8, 9.5, 9~\r\n\r\nThe above result shows the data has about 22 columns(variables) and 2928 rows(observations). Let me do a quick exploratory data analysis .\r\n\r\n\r\nlibrary(DataExplorer)\r\nplot_missing(life_expe_data) \r\n\r\n\r\n\r\nThe missing value exploration plot shows, we don’t have missing value of more than 25% for each variable. We don’t have any variable that will be dropped using a complete deletion. In fact, the missing values are tolerable and performing imputation strategies is practically possible. I will be back on another blog post to perform missing value imputations using tree-based methods or KNN algorithms. For now, I will simply drop all observations with missing values!\r\n\r\n\r\nlife_exp_narm <- life_expe_data[complete.cases(life_expe_data), ] # This deletes all rows with missing values\r\n\r\n\r\nThere is another important function from the DataExplorerpackage to do a quick grasp of the distributions of my variables.\r\n\r\n\r\nplot_histogram(life_exp_narm) # Almost all variables are skewed\r\n\r\n\r\n\r\nModeling\r\nSince, the goal of this post is to plot model coefficients and confidence intervals in a forest plot, I will not go deeper in explaining models.I will use life expectancy as a dependent variable and use some predictors to do my linear models. Before that, I will do one quick visualization.\r\n\r\n\r\nlibrary(ggplot2) #for awesome plotting\r\n\r\n# let me rename the income variable\r\nnames(life_exp_narm)[names(life_exp_narm) == \"income_composition_of_resources\"] <- \"income_index\"\r\n\r\n\r\nggplot(life_exp_narm, aes(x=income_index, y=life_expectancy, col=status)) +\r\n  geom_point() \r\n\r\n\r\n\r\nThe plot shows, as income index increases, life expectancy increases. In addition, the developed countries have higher income level and high life expectancy. However, there is really one very important thing visible in this plot. The income_index variable has a lot of zeros values.\r\n\r\n\r\nsummary(life_exp_narm$income_index)\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n 0.0000  0.5090  0.6730  0.6316  0.7510  0.9360 \r\n\r\nsum(life_exp_narm$income_index==0) \r\n\r\n[1] 48\r\n\r\nSee, 48 observations have 0 value on income_index. That must be missing data. I will filter out the observations with 0 values in this variable.\r\n\r\n\r\nlife_exp_new <- life_exp_narm %>% \r\n  filter(income_index>0) # all with 0 values on income_index variable are removed\r\n\r\n\r\nNow, we can visualize again with a little more tweaking.\r\n\r\n\r\nlife_exp_new %>% \r\n  ggplot(aes(x=income_index, y=life_expectancy, col=status)) +\r\n  geom_point() + geom_smooth(method=\"lm\", col=\"blue\") \r\n\r\n\r\n\r\nWe have now somehow clean data to start our modeling process. We may need to see the relations ship of income index with other variables. For now, I will skip that and directly proceed to my modeling.\r\nDifferent ways, similar results\r\nOne of the great things, you can enjoy with R is there are many ways of doing the same thing.\r\n\r\n\r\nlm1 <- lm(life_expectancy ~ total_expenditure + log(income_index) + adult_mortality + infant_deaths + alcohol + thinness_1_19_years + thinness_5_9_years + schooling + infant_deaths + status,data=life_exp_new)\r\n\r\n# We can do the same task using glm but defining the family argument as \"gaussian\"\r\nlm2 <- glm(life_expectancy ~ total_expenditure + log(income_index) + adult_mortality + infant_deaths + alcohol + thinness_1_19_years + thinness_5_9_years + schooling + infant_deaths + status,\r\n           data=life_exp_new, family = gaussian)\r\n\r\n\r\nSee the model outputs\r\nThe two approach produce similar outputs. But,lm has a shorter code than glm. So, many ppl prefer to use lm() for linear regression.\r\n\r\n\r\nlibrary(jtools) #for nice table model output\r\nsumm(lm1,confint = TRUE, digits = 3, vifs = TRUE) # add vif to see if variance inflation factor is greater than 2\r\n\r\nMODEL INFO:\r\nObservations: 1601\r\nDependent Variable: life_expectancy\r\nType: OLS linear regression \r\n\r\nMODEL FIT:\r\nF(9,1591) = 883.694, p = 0.000\r\nR² = 0.833\r\nAdj. R² = 0.832 \r\n\r\nStandard errors: OLS\r\n----------------------------------------------------------------------\r\n                              Est.     2.5%    97.5%    t val.       p\r\n------------------------- -------- -------- -------- --------- -------\r\n(Intercept)                 94.878   92.149   97.606    68.214   0.000\r\ntotal_expenditure            0.227    0.144    0.311     5.365   0.000\r\nlog(income_index)           29.194   27.340   31.047    30.897   0.000\r\nadult_mortality             -0.021   -0.023   -0.020   -23.933   0.000\r\ninfant_deaths               -0.001   -0.002    0.001    -0.653   0.514\r\nalcohol                     -0.246   -0.310   -0.181    -7.494   0.000\r\nthinness_1_19_years          0.081   -0.024    0.186     1.507   0.132\r\nthinness_5_9_years          -0.186   -0.290   -0.083    -3.546   0.000\r\nschooling                   -0.512   -0.674   -0.351    -6.220   0.000\r\nstatusDeveloping            -2.289   -2.940   -1.637    -6.890   0.000\r\n----------------------------------------------------------------------\r\n \r\n---------------------------------\r\n                              VIF\r\n------------------------- -------\r\n(Intercept)                      \r\ntotal_expenditure           1.138\r\nlog(income_index)           6.779\r\nadult_mortality             1.540\r\ninfant_deaths               1.369\r\nalcohol                     2.131\r\nthinness_1_19_years         7.147\r\nthinness_5_9_years          6.961\r\nschooling                   6.389\r\nstatusDeveloping            1.716\r\n---------------------------------\r\n\r\nsumm(lm2,confint = TRUE, digits = 3, vifs=TRUE)\r\n\r\nMODEL INFO:\r\nObservations: 1601\r\nDependent Variable: life_expectancy\r\nType: Linear regression \r\n\r\nMODEL FIT:\r\n<U+03C7>²(9) = 105053.208, p = 0.000\r\nPseudo-R² (Cragg-Uhler) = 0.834\r\nPseudo-R² (McFadden) = 0.249\r\nAIC = 8687.410, BIC = 8746.572 \r\n\r\nStandard errors: MLE\r\n----------------------------------------------------------------------\r\n                              Est.     2.5%    97.5%    t val.       p\r\n------------------------- -------- -------- -------- --------- -------\r\n(Intercept)                 94.878   92.151   97.604    68.214   0.000\r\ntotal_expenditure            0.227    0.144    0.310     5.365   0.000\r\nlog(income_index)           29.194   27.342   31.046    30.897   0.000\r\nadult_mortality             -0.021   -0.023   -0.020   -23.933   0.000\r\ninfant_deaths               -0.001   -0.002    0.001    -0.653   0.514\r\nalcohol                     -0.246   -0.310   -0.181    -7.494   0.000\r\nthinness_1_19_years          0.081   -0.024    0.186     1.507   0.132\r\nthinness_5_9_years          -0.186   -0.289   -0.083    -3.546   0.000\r\nschooling                   -0.512   -0.674   -0.351    -6.220   0.000\r\nstatusDeveloping            -2.289   -2.940   -1.638    -6.890   0.000\r\n----------------------------------------------------------------------\r\n \r\n---------------------------------\r\n                              VIF\r\n------------------------- -------\r\n(Intercept)                      \r\ntotal_expenditure           1.138\r\nlog(income_index)           6.779\r\nadult_mortality             1.540\r\ninfant_deaths               1.369\r\nalcohol                     2.131\r\nthinness_1_19_years         7.147\r\nthinness_5_9_years          6.961\r\nschooling                   6.389\r\nstatusDeveloping            1.716\r\n---------------------------------\r\n\r\nEstimated dispersion parameter = 13.209 \r\n\r\nSeeing the vif value above 2 is evidence of multicollinearity in our model. Let’s just use the HMISC package to find all correlation values for the variables in the model.\r\n\r\n\r\ndat <- life_exp_narm %>% \r\n  select(total_expenditure, income_index, adult_mortality , infant_deaths ,\r\n             alcohol, thinness_1_19_years, thinness_5_9_years, schooling)\r\n\r\n#library(corrplot)\r\nlibrary(Hmisc)\r\ncor <- rcorr(as.matrix(dat))\r\nr <- cor$r %>% as.table()\r\nr\r\n\r\n                    total_expenditure income_index adult_mortality\r\ntotal_expenditure          1.00000000   0.18365319     -0.08522653\r\nincome_index               0.18365319   1.00000000     -0.44220329\r\nadult_mortality           -0.08522653  -0.44220329      1.00000000\r\ninfant_deaths             -0.14695112  -0.13475386      0.04245024\r\nalcohol                    0.21488509   0.56107433     -0.17553509\r\nthinness_1_19_years       -0.20987232  -0.45367885      0.27223004\r\nthinness_5_9_years        -0.21786479  -0.43848372      0.28672288\r\nschooling                  0.24378345   0.78474058     -0.42117052\r\n                    infant_deaths     alcohol thinness_1_19_years\r\ntotal_expenditure     -0.14695112  0.21488509         -0.20987232\r\nincome_index          -0.13475386  0.56107433         -0.45367885\r\nadult_mortality        0.04245024 -0.17553509          0.27223004\r\ninfant_deaths          1.00000000 -0.10621692          0.46341526\r\nalcohol               -0.10621692  1.00000000         -0.40375499\r\nthinness_1_19_years    0.46341526 -0.40375499          1.00000000\r\nthinness_5_9_years     0.46190792 -0.38620819          0.92791344\r\nschooling             -0.21437190  0.61697481         -0.49119921\r\n                    thinness_5_9_years   schooling\r\ntotal_expenditure          -0.21786479  0.24378345\r\nincome_index               -0.43848372  0.78474058\r\nadult_mortality             0.28672288 -0.42117052\r\ninfant_deaths               0.46190792 -0.21437190\r\nalcohol                    -0.38620819  0.61697481\r\nthinness_1_19_years         0.92791344 -0.49119921\r\nthinness_5_9_years          1.00000000 -0.47248203\r\nschooling                  -0.47248203  1.00000000\r\n\r\nOr more beautifully we can plot he correlation values using the pairs.panels function from the psych package. We can have a good correlation matrix of our variables.\r\n\r\n\r\nlibrary(psych)\r\npairs.panels(dat)\r\n\r\n\r\n\r\nThere is a high correlation(r=.93) between thinness_1_19_years and thinness_5_9_years, schooling and income_index (r=0.78), schooling and alchohol. To deal with this multicollinearity problem, I will just drop one of the collinear variables because they just add a redundenent information in my model. Ideally, one can do a stepwise regression, or best subsets regression and then choose the model that has the highest R-squared value. But, we are simply dropping one of the variables. Let me drop thinness_5_9_years (the variable thinness_1_19_years actually carry the other variable: thinness_5_9_years), and income_index variables from my linear model and redo my analysis.\r\n\r\n\r\nlm3 <- lm(life_expectancy ~ total_expenditure + \r\n             schooling + adult_mortality + infant_deaths + thinness_1_19_years +\r\n            alcohol + status,\r\n          data=life_exp_new)\r\n\r\nsumm(lm3,confint = TRUE, digits = 3, vifs=TRUE) \r\n\r\nMODEL INFO:\r\nObservations: 1601\r\nDependent Variable: life_expectancy\r\nType: OLS linear regression \r\n\r\nMODEL FIT:\r\nF(7,1593) = 623.404, p = 0.000\r\nR² = 0.733\r\nAdj. R² = 0.731 \r\n\r\nStandard errors: OLS\r\n----------------------------------------------------------------------\r\n                              Est.     2.5%    97.5%    t val.       p\r\n------------------------- -------- -------- -------- --------- -------\r\n(Intercept)                 59.180   57.252   61.107    60.223   0.000\r\ntotal_expenditure           -0.027   -0.130    0.076    -0.509   0.611\r\nschooling                    1.558    1.440    1.677    25.746   0.000\r\nadult_mortality             -0.033   -0.035   -0.031   -31.411   0.000\r\ninfant_deaths                0.001   -0.002    0.003     0.566   0.571\r\nthinness_1_19_years         -0.214   -0.280   -0.148    -6.373   0.000\r\nalcohol                     -0.146   -0.227   -0.065    -3.532   0.000\r\nstatusDeveloping            -1.725   -2.548   -0.902    -4.111   0.000\r\n----------------------------------------------------------------------\r\n \r\n---------------------------------\r\n                              VIF\r\n------------------------- -------\r\n(Intercept)                      \r\ntotal_expenditure           1.090\r\nschooling                   2.153\r\nadult_mortality             1.288\r\ninfant_deaths               1.345\r\nthinness_1_19_years         1.736\r\nalcohol                     2.110\r\nstatusDeveloping            1.709\r\n---------------------------------\r\n\r\nNow, I don’t have much evidence of multicollinearity (see the vif values). BINGO!\r\nThere are many ways of visualizing my model outputs.\r\n\r\n\r\nlibrary(broom)\r\nlibrary(cli)\r\nmodel_output <- tidy(lm3)\r\nout_conf <- tidy(lm3, conf.int = TRUE)\r\n\r\nlibrary(forestmangr)\r\nlm_model_out <- round_df(out_conf, digits=2)\r\nlm_model_out <- lm_model_out[-1,] #remove the intercept \r\n\r\n# Now plot them\r\nggplot(lm_model_out, aes(x=reorder(term, estimate), y=estimate)) +\r\n         geom_errorbar(aes(ymin=conf.low, ymax=conf.high), \r\n                       width = 0.2,size  = 1,\r\n                       position = \"dodge\", color=\"turquoise4\") +\r\n  geom_hline(yintercept = 0, color = \"red\", size = 1) +\r\n  geom_point() + coord_flip() \r\n\r\n\r\n\r\nRetrieve coefficients and their 95% CI\r\n\r\n\r\ncoef <- coef(lm3)\r\nConfidenceInterval <- confint(lm3)\r\ncoef_confint <- cbind(coef, ConfidenceInterval) %>% as.data.frame()\r\ncoef_confint <- coef_confint %>% mutate(variable=rownames(coef_confint))\r\n\r\n\r\nlibrary(plyr) \r\ncoef_confint <- rename(coef_confint,c(\"coef\" = \"Beta\",\r\n                                      `2.5 %` = \"lower_bound\", \r\n                                      `97.5 %` = \"upper_bound\"))\r\n\r\n# We don't need to plot the intercept. We can remove it from our data\r\n\r\n\r\n# Reorder variables\r\ncol_order <- c(\"variable\", \"Beta\", \"lower_bound\", \"upper_bound\")\r\ncoef_confint <- coef_confint[, col_order] #reorder variables in the data frame\r\n\r\ncoef_confint <- coef_confint %>% \r\n  mutate_if(is.numeric, round, digits = 2) # round numeric into two significant digits \r\n\r\n\r\nVisualize the coefficients once again\r\nNow, I have everything to visualize my coefficients and their respective confidence intervals in a forest plot.\r\n\r\n\r\nplot_lm <- coef_confint[-1,] %>%  #remove row number 1 (The intercept) \r\n  ggplot(aes(x=reorder(variable, Beta), y=Beta)) +\r\n  geom_point(shape = 15,\r\n             size  = 4, width = 0.1,\r\n             position = \"dodge\", color=\"black\") + \r\n  geom_errorbar(aes(ymin  = lower_bound,\r\n                    ymax  = upper_bound),\r\n                width = 0.2,\r\n                size  = 1,\r\n                position = \"dodge\", color=\"turquoise4\") +\r\n  theme(axis.title = element_text(face = \"bold\")) +\r\n  xlab(\"Variables\") + ylab(\"Beta coeffecients with 95% CI\") +\r\n  coord_flip(ylim = c(-2.5, 1.6)) + \r\n  geom_hline(yintercept = 0, color = \"red\", size = 1) +\r\n   theme(axis.title = element_text(size = 17)) + \r\n  theme(axis.text = element_text(size = 14)) \r\n\r\nplot_lm\r\n\r\n\r\n\r\nThe y-axis tick labels can be improved to correctly define the names. I can do that by bz renaming mz axis tick-labels in my ggplot code.\r\n\r\n\r\nplot_lm + scale_x_discrete(breaks = c(\"statusDeveloping\", \r\n                                    \"thinness_1_19_years\", \"alcohol\",\r\n                                    \"adult_mortality\",\"total_expenditure\", \r\n                                    \"infant_deaths\", \"schooling\"),\r\n                           labels = c(\"Developing countries\", \"Thinness 1-19 yrs\", \r\n                                      \"Alcohol consumption\", \r\n                                      \"Adult MR\", \"Total expenditure\",\r\n                                      \"IMR\", \r\n                                      \"Average school yrs\")) \r\n\r\n\r\n\r\nThere are also a handful of specialized r packages which are dedicated to do this job with a much shorter lines of codes. In the following part, I will use some of the packages to plot my model coefficients. Of the many packages available, broom.mixed, and sJPlot, coefplot,dotwhisker, modelsummary, etc are some of them. I will try broom.mixed and sJPlot\r\n\r\n\r\nlibrary(broom.mixed)\r\n\r\nplot_summs(lm3, scale = TRUE, size=3) \r\n\r\n\r\nplot_summs(lm1, scale = TRUE, plot.distributions = TRUE, \r\n           inner_ci_level = .9,\r\n           color.class = \"darkgreen\") \r\n\r\n\r\n\r\nOne advantage of broom.mixedis it helps you visualize the distribution of the variable and besides the coefficient and the confidence interval. That helps to have a rough overview of normality.\r\nThe variable names in the plots can be edited. But, for the sake of time, I am not going to do that.\r\n\r\n\r\nlibrary(sjPlot)\r\nplot_model(lm3, show.values = TRUE, value.offset = 0.3,\r\n           axis.labels=c(\"Developing countries\", \"Alcohol consumption\",\r\n                         \"Thinness 1-19 yrs\",\"IMR\", \"Adult MR\",\"Total expenditure\",\r\n                         \"Average school yrs\"))\r\n\r\n\r\n\r\nWith SjPlot, it is much easier to label the coefficients in the plot.\r\nOne more tip\r\nThis is not the same regression as we did above. I used a new set of predictors. The following package can do the modeling calculation, tabulation and plotting all together. For linear regression, you can use coef_plot, for logistic regression or_plot, and hr_plot for hazard ratios, etc. I recently discovered this package in stack overflow.\r\n\r\n\r\nlibrary(finalfit) \r\n\r\nexplanatory = c( \"alcohol\",\r\n                \"thinness_1_19_years\", \r\n                \"hiv_aids\", \r\n                \"total_expenditure\", \"schooling\")\r\n\r\ndependent = \"life_expectancy\"\r\n\r\n\r\nlife_exp_new %>%\r\n coefficient_plot(dependent, explanatory, table_text_size=3, \r\n                  title_text_size=12,\r\n                   plot_opts=list(xlab(\"Beta, 95% CI\"), \r\n                                  theme(axis.title = element_text(size=12))))\r\n\r\n\r\n\r\nBut, is linear regression model the right choice for our data. Did my model pass all key assumptions. I don’t think so. Before I do anything I need to check whether the linear regression assumptions are fulfilled. To reject or accept our model, it must pass all the relevant assumptions. gvlma is a good package for checking that.\r\n\r\n\r\nlibrary(gvlma)\r\ngvlma(lm3)\r\n\r\n\r\nCall:\r\nlm(formula = life_expectancy ~ total_expenditure + schooling + \r\n    adult_mortality + infant_deaths + thinness_1_19_years + alcohol + \r\n    status, data = life_exp_new)\r\n\r\nCoefficients:\r\n        (Intercept)    total_expenditure            schooling  \r\n         59.1795303           -0.0267339            1.5582409  \r\n    adult_mortality        infant_deaths  thinness_1_19_years  \r\n         -0.0325335            0.0006165           -0.2137415  \r\n            alcohol     statusDeveloping  \r\n         -0.1458388           -1.7250085  \r\n\r\n\r\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\r\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\r\nLevel of Significance =  0.05 \r\n\r\nCall:\r\n gvlma(x = lm3) \r\n\r\n                     Value   p-value                   Decision\r\nGlobal Stat        1729.57 0.000e+00 Assumptions NOT satisfied!\r\nSkewness            409.04 0.000e+00 Assumptions NOT satisfied!\r\nKurtosis           1245.83 0.000e+00 Assumptions NOT satisfied!\r\nLink Function        37.21 1.059e-09 Assumptions NOT satisfied!\r\nHeteroscedasticity   37.49 9.187e-10 Assumptions NOT satisfied!\r\n\r\nAll of the linear regression assumptions are violated. The above model is incorrect. Therefore, I have to decide whether to transform my data using transformation methods or go for other models. Let me check some details about the dependent variable using the psych package,\r\n\r\n\r\nhist(life_exp_narm$life_expectancy)\r\n\r\n\r\nlibrary(psych)\r\ndescribe(life_exp_narm$life_expectancy) #skewness -0.63, negatively skewed\r\n\r\n   vars    n mean  sd median trimmed  mad min max range  skew\r\nX1    1 1649 69.3 8.8   71.7   69.91 7.56  44  89    45 -0.63\r\n   kurtosis   se\r\nX1     0.03 0.22\r\n\r\nMy dependent variable is negatively(left) skewed. For left(negatively) skewed distribution, square root, cube root or logarithmic transformation can help me achieve normality. I will check that one by one using qqplot. But, for a nicer version of qqplots, I prefer to use the ggpubr package.\r\n\r\n\r\nlibrary(ggpubr)\r\nggqqplot(sqrt(life_exp_new$life_expectancy))\r\n\r\n\r\n\r\nThe qqplots above show many of the points fall outside the reference line indicating the data is not normally distributed. Let me transform all of them using square root, cube root and logarithm and investigate qqplots. After that, I will run shapiro wilk normality test.\r\n\r\n\r\nlibrary(ggpubr)\r\nlibrary(patchwork)\r\np1 <- ggqqplot(sqrt(life_exp_new$life_expectancy))\r\np2 <- ggqqplot((life_exp_new$life_expectancy)^1/3)\r\np3 <- ggqqplot(log(life_exp_new$life_expectancy))\r\n\r\np1 + p2 + p3\r\n\r\n\r\nshapiro.test(sqrt(life_exp_new$life_expectancy))\r\n\r\n\r\n    Shapiro-Wilk normality test\r\n\r\ndata:  sqrt(life_exp_new$life_expectancy)\r\nW = 0.94587, p-value < 2.2e-16\r\n\r\nshapiro.test((life_exp_new$life_expectancy)^1/3)\r\n\r\n\r\n    Shapiro-Wilk normality test\r\n\r\ndata:  (life_exp_new$life_expectancy)^1/3\r\nW = 0.9605, p-value < 2.2e-16\r\n\r\nshapiro.test(log(life_exp_new$life_expectancy))\r\n\r\n\r\n    Shapiro-Wilk normality test\r\n\r\ndata:  log(life_exp_new$life_expectancy)\r\nW = 0.92811, p-value < 2.2e-16\r\n\r\nApparently, the p-values for Shapiro Wilk normality test for all transformed data are rather much lower than 0.05. The null hypothesis (H0) of Shapiro test is “data is normally distributed”. Therefore, we will need to reject the null hypothesis and conclude the data is not normally distributed. A pity!\r\nWith that we can reject our model(lm3) and it is not relevant for our data. I am now done with linear models here. I am angry at them. What else can I do? There are many many alternatives. For example: Logistic regression. But, I should get my data ready for that.\r\nLogistic regression\r\nI will use glm function to run logistic regression. However, I need to dichotomize my life expectancy variable into high and low life expectancy categories and attach numeric value label my binary category(something like: high,low/1,0. I need to split my outcome variable into two categories: high and low. Since, the data doesn’t follow normal distribution as evidenced above, I should use median split which means every life expectancy value above the median will have “high” label else “low” label.\r\n\r\n\r\nsummary(life_exp_new$life_expectancy) #median is 71.8\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  44.00   64.60   71.80   69.39   75.00   89.00 \r\n\r\nMedian split\r\n\r\n\r\nlife_exp_new$life_exp_cat <- ifelse(life_exp_new$life_expectancy <= 71.8, \"0\", \"1\")\r\n\r\nlife_exp_new$life_exp_cat <- factor(life_exp_new$life_exp_cat, levels = c(0,1),\r\n                    labels = c(\"low\", \"high\"))\r\n\r\ntable(life_exp_new$life_exp_cat) \r\n\r\n\r\n low high \r\n 802  799 \r\n\r\n\r\n\r\nlogistic_reg <- glm(life_exp_cat ~ total_expenditure + \r\n             schooling + adult_mortality + infant_deaths + thinness_1_19_years +\r\n            alcohol + status, data=life_exp_new, family = binomial)\r\n\r\nsummary(logistic_reg)\r\n\r\n\r\nCall:\r\nglm(formula = life_exp_cat ~ total_expenditure + schooling + \r\n    adult_mortality + infant_deaths + thinness_1_19_years + alcohol + \r\n    status, family = binomial, data = life_exp_new)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-3.1008  -0.4497  -0.0110   0.5094   2.2990  \r\n\r\nCoefficients:\r\n                      Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)         -7.0055557  0.8329181  -8.411  < 2e-16 ***\r\ntotal_expenditure    0.1157071  0.0398235   2.905  0.00367 ** \r\nschooling            0.7555904  0.0566505  13.338  < 2e-16 ***\r\nadult_mortality     -0.0128377  0.0010746 -11.946  < 2e-16 ***\r\ninfant_deaths       -0.0009023  0.0009796  -0.921  0.35704    \r\nthinness_1_19_years -0.0633472  0.0215176  -2.944  0.00324 ** \r\nalcohol             -0.0547746  0.0278457  -1.967  0.04917 *  \r\nstatusDeveloping    -0.4520548  0.3583898  -1.261  0.20718    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 2219.5  on 1600  degrees of freedom\r\nResidual deviance: 1097.5  on 1593  degrees of freedom\r\nAIC: 1113.5\r\n\r\nNumber of Fisher Scoring iterations: 6\r\n\r\nExtract Odds ratios and confidence intervals\r\n\r\n\r\nrequire(MASS)\r\nor_CI <- round(exp(cbind(coef(logistic_reg), confint(logistic_reg))), digits=3) %>% \r\n  as.data.frame()\r\n\r\nor_CI <- or_CI %>% \r\n  mutate(variable=rownames(or_CI)) # extract the variables from rownames\r\n  \r\n\r\nor_CI <- rename(or_CI, c(\"V1\" = \"AOR\",\r\n                        `2.5 %` = \"lower_bound\",\r\n                        `97.5 %` = \"upper_bound\"))\r\n\r\n# We don't need to plot the intercept. We can remove it from our data\r\n\r\n\r\n# Reorder variables\r\ncol_order <- c(\"variable\", \"AOR\", \"lower_bound\", \"upper_bound\")\r\nor_CI <- or_CI[, col_order] #reorder variables in the data frame\r\n\r\n\r\nNow, I can plot the odds ratios and confidence intervals exactly as I did for my linear model coefficients.\r\n\r\n\r\nplot_logit_model <- or_CI[-1,] %>%  #remove row number 1 (The intercept) \r\n  ggplot(aes(x = reorder(variable, AOR), y = AOR)) +\r\n  geom_point(shape = 15,\r\n             size  = 4, width = 0.1,\r\n             position = \"dodge\", color=\"black\") + \r\n  geom_errorbar(aes(ymin  = lower_bound,\r\n                    ymax  = upper_bound),\r\n                width = 0.2,\r\n                size  = 0.7,\r\n                position = \"dodge\", color=\"turquoise4\") +\r\n  theme(axis.title = element_text(face = \"bold\")) +\r\n  xlab(\"Variables\") + ylab(\"Adjusted odds ratios with 95% CI\") +\r\n  coord_flip(ylim = c(0, 2.5)) + \r\n  geom_hline(yintercept = 1, color = \"red\", size = 1) +\r\n   theme(axis.title = element_text(size = 17)) + \r\n  theme(axis.text = element_text(size = 14)) \r\nplot_logit_model\r\n\r\n\r\n\r\n\r\n\r\nplot_logit_model + scale_x_discrete(breaks = c(\"statusDeveloping\", \"alcohol\",\r\n                                    \"thinness_1_19_years\", \r\n                                    \"adult_mortality\",\"infant_deaths\",\r\n                                    \"total_expenditure\", \"schooling\"),\r\n                           \r\n                           labels = c(\"Developing countries\", \"Alcohol consumption\",\r\n                                      \"Thinness 1-19 yrs\", \r\n                                      \"Adult mortality\", \"Infant mortality\", \r\n                                      \"Total expenditure\",\r\n                                      \"Average school yrs\")) \r\n\r\n\r\n\r\nOr just simply using the sjPlot package\r\n\r\n\r\nplot_model(logistic_reg, show.values = TRUE, value.offset = .3, vline.color = \"red\")\r\n\r\n\r\n\r\nEvaluation of model performance\r\nA widely approach to assess model performance is to assess prediction accuracy via loss functions in cross-validated samples.\r\nWe can use the models for predictions.\r\n\r\n\r\nlibrary(caret)\r\nset.seed(123) #for reproducibility\r\n(cv_lm <-  train(life_expectancy ~ total_expenditure + \r\n             schooling + adult_mortality + infant_deaths + thinness_1_19_years +\r\n            alcohol + status, data=life_exp_new, \r\n            method=\"lm\", \r\n            trControl = trainControl(method=\"cv\", number=10)))\r\n\r\nLinear Regression \r\n\r\n1601 samples\r\n   7 predictor\r\n\r\nNo pre-processing\r\nResampling: Cross-Validated (10 fold) \r\nSummary of sample sizes: 1442, 1441, 1441, 1440, 1439, 1442, ... \r\nResampling results:\r\n\r\n  RMSE     Rsquared   MAE     \r\n  4.57687  0.7336561  3.325222\r\n\r\nTuning parameter 'intercept' was held constant at a value of TRUE\r\n\r\nHow do you interpret the above cross validated loss functions(eg.RMSE)?\r\nIf the linear model is applied to unseen data, it will predict life expectancy about 4.6 years off from the actual life expectancy of a country. That was great. Unfortunately, the model did not satisfy the assumptions. Let’s now evaluate our logistic regression\r\n\r\n\r\nlibrary(caret)\r\nset.seed(123)\r\n(cv_logit <-  train(life_exp_cat ~ total_expenditure + \r\n             schooling + adult_mortality + infant_deaths + thinness_1_19_years +\r\n            alcohol + status, data=life_exp_new, \r\n            method=\"glm\", \r\n            trControl = trainControl(method=\"cv\", number=10)))\r\n\r\nGeneralized Linear Model \r\n\r\n1601 samples\r\n   7 predictor\r\n   2 classes: 'low', 'high' \r\n\r\nNo pre-processing\r\nResampling: Cross-Validated (10 fold) \r\nSummary of sample sizes: 1441, 1441, 1441, 1441, 1441, 1441, ... \r\nResampling results:\r\n\r\n  Accuracy   Kappa    \r\n  0.8451165  0.6902183\r\n\r\nThe logistic regression model predicts the data with 84.5% accuracy. Ideally, we have to split the data into test and train set and perform the performance evaluation and tabulate results using confusion matrix functions.\r\nYes, I did it. Thank you for reading this post. In my next blog post, will write about more advanced generalized linear models(Lasso, ridge and elastic net).\r\nTrains GIFs | TenorThank you for reading this post. I hope you find this helpful. For more, subscribe to my YouTube channel and follow me on Twitter @RPy_DataScience.You can also follow me by liking R_Py Data Science Facebook page.\r\nContact\r\nPlease mention @RPy_DataScience if you tweet this post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-09-30-2020-09-30-plotting-model-coefficients-in-a-forest-plot/2020-09-30-plotting-model-coefficients-in-a-forest-plot_files/figure-html5/unnamed-chunk-23-1.png",
    "last_modified": "2023-03-31T00:26:12+02:00",
    "input_file": "2020-09-30-plotting-model-coefficients-in-a-forest-plot.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-08-10-2020-08-10-text-analysis-of-covid-publications/",
    "title": "Applying text analyses methods on 382 COVID19 journal articles",
    "description": "This blog post is a continuation of my previous blog post on applications of text mining on sampled COVI19 publications.",
    "author": [
      {
        "name": "Mihiretu Kebede(PhD)",
        "url": {}
      }
    ],
    "date": "2020-08-11",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\nLoad necessary packages\r\nFor this analysis, I need the following packages\r\n\r\n\r\nlibrary(dplyr) #for data management\r\nlibrary(ggplot2) #for plotting\r\nlibrary(tidytext) #for text mining\r\nlibrary(bib2df) #for converting bib file to data frame\r\nlibrary(wordcloud) #for plotting most frequent words\r\n\r\n\r\nData\r\nLet’s import our bibliographic data using bib2df package as described in my previous blog post\r\n\r\n\r\nlibrary(bib2df)\r\ncovid19 <- bib2df(\"covid19.bib\")\r\n\r\n\r\nAs usual, let’s have a quick glimpse of our data.\r\n\r\n\r\nglimpse(covid19)\r\n\r\nRows: 601\r\nColumns: 32\r\n$ CATEGORY     <chr> \"MISC\", \"ARTICLE\", \"ARTICLE\", \"ARTICLE\", \"MISC\"~\r\n$ BIBTEXKEY    <chr> \"Gelzinis2020\", \"Coccia2020\", \"Ataguba2020\", \"S~\r\n$ ADDRESS      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ ANNOTE       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ AUTHOR       <list> \"Gelzinis, Theresa A\", \"Coccia, Mario\", <\"Atag~\r\n$ BOOKTITLE    <chr> \"Journal of cardiothoracic and vascular anesthe~\r\n$ CHAPTER      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ CROSSREF     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ EDITION      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ EDITOR       <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA~\r\n$ HOWPUBLISHED <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ INSTITUTION  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ JOURNAL      <chr> NA, \"The Science of the total environment\", \"Gl~\r\n$ KEY          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ MONTH        <chr> \"sep\", \"aug\", \"dec\", \"sep\", \"dec\", \"nov\", \"sep\"~\r\n$ NOTE         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ NUMBER       <chr> \"9\", NA, \"1\", NA, \"1\", NA, NA, \"1\", NA, NA, \"3\"~\r\n$ ORGANIZATION <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ PAGES        <chr> \"2328--2330\", \"138474\", \"1788263\", \"312--321\", ~\r\n$ PUBLISHER    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ SCHOOL       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ SERIES       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ TITLE        <chr> \"Thoracic Anesthesia in the Coronavirus Disease~\r\n$ TYPE         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ VOLUME       <chr> \"34\", \"729\", \"13\", \"117\", \"9\", \"110\", \"83\", \"9\"~\r\n$ YEAR         <dbl> 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020,~\r\n$ DOI          <chr> \"10.1053/j.jvca.2020.05.008\", \"10.1016/j.scitot~\r\n$ ISSN         <chr> \"1532-8422 (Electronic)\", \"1879-1026 (Electroni~\r\n$ LANGUAGE     <chr> \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\"~\r\n$ PMID         <chr> \"32406428\", \"32498152\", \"32657669\", \"32546875\",~\r\n$ ABSTRACT     <chr> NA, \"This study has two goals. The first is to ~\r\n$ KEYWORDS     <chr> NA, \"Air Pollutants,Air Pollution,Betacoronavir~\r\n\r\ndim(covid19) #601 documents and 32 variables \r\n\r\n[1] 601  32\r\n\r\nOnce you read the file, it will be saved in our enviroment as data frame and we can do our analysis. As usual, the first part of the analysis is doing some descriptives and visualizing the data. We will then move to tokenization steps. Token is meaningful unit of unit of a text. This meanigful unit is represented by a word or a term. It is the unit that text analysts are interested in doing their analzsis. The process of breaking down a text to set of tokens is known as tokenization. For example\r\nText: “COVID19 is the biggest global health crisis of the modern world”. In this text, the tokens are each word. In the tokenization process, each of these words are written in a separate columns(variable) as follows.\r\nIn tokenization process, the text documents is changed to tokens and each token is counted.\r\nText\r\nCOVID19\r\nis\r\nthe\r\nbiggest\r\nglobal\r\nhealth\r\ncrisis\r\nof\r\nthe\r\nmodern\r\nworld\r\nCOVID19 is the biggest global health crisis of the modern world\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\nIn tokenization process, the text documents is changed to tokens and each token is counted. Notice “the” is mentioned twice. So, we can count it and reshuffle our table as fllows.\r\nText\r\nCOVID19\r\nis\r\nthe\r\nbiggest\r\nglobal\r\nhealth\r\ncrisis\r\nof\r\nmodern\r\nworld\r\nCOVID19 is the biggest global health crisis of the modern world\r\n1\r\n1\r\n2\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\nThe next thing after tokenization is removing irrelevant words. These are words that don’t give much information to the concept of the document other than making the grammatical structure of the document. The stp_words data has about 1150 irrelevant words. These words can be easily removed using the dyplyr ant_join function.\r\nWe have enough of theory. Let’s move to the more technical things. As we did in the previous blog, we will only need journal articiles having no missing data on their abstract variable.\r\n\r\n\r\nlibrary(dplyr)\r\ncovid19new <- covid19 %>%\r\n  filter(!is.na(ABSTRACT)) #remove all records with missing abstracts\r\n\r\n\r\n\r\n\r\ntable(covid19new$CATEGORY) #382 are journal articles with no missing abstract\r\n\r\n\r\nARTICLE    MISC \r\n    382      60 \r\n\r\n# Filter journal articles having  abstracts \r\ncovid19new <- covid19new %>% \r\n  filter(CATEGORY==\"ARTICLE\") \r\n#Check \r\ntable(covid19new$CATEGORY) #382 journal articles\r\n\r\n\r\nARTICLE \r\n    382 \r\n\r\nTokenization\r\n\r\n\r\n# select few variables from our data set\r\ndata <- covid19new %>% \r\n  select(\"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"AUTHOR\")\r\n\r\n\r\nTo start with the tokenization process, we will load stop_words data. We can also add our own stop stop words. In text mining, words on both sides of extreme frequency are no relevant. These words can be removed by creating a customized list of stop_words. Since we are doing our text analysis on journal articles, words that are obviously common in each of the abstracts can be removed. Structured abstracts usually contain “Introduction” or “background”, “Methods”, “Results” and “conclusions”. These and other words can be removed. The custom stop words can also include “covid”, since all of them are about “covid”. There is also a way to remove numbers from our corpus. For now, we will not remove all numbers.\r\n\r\n\r\ndata(stopwords)\r\ncustom_stop_words <- bind_rows(tibble(word=c(\"covid\", \"covid19\",\"covid-19\", \"sars\", \"sars\", \"cov\", \"background\", \"introduction\",\"aims\", \"objectives\", \"materials\", \"methods\", \"results\", \"conclusions\",\"textless\", \"0\", \"1\",\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\"19\", \"2019\", \"2020\", \"95\"),\r\n                                      lexicon = c(\"custom\")),\r\n                               stop_words)\r\n\r\n\r\ntidy_covid_data <- data %>% \r\n  unnest_tokens(input=ABSTRACT, output=word) %>% \r\n  anti_join(stop_words) %>% \r\n  anti_join(custom_stop_words)\r\n\r\ndim(tidy_covid_data) # 40669 tokens, 4 variables\r\n\r\n[1] 40647     4\r\n\r\ncount number of words and visualize it using barchart.\r\n\r\n\r\n tidy_covid_data %>% \r\n  count(word, sort=T) %>% \r\n   filter(n>100) %>% \r\n   mutate(word=reorder(word, n)) %>% \r\n   ggplot(aes(x=word, y=n)) + \r\n   geom_col(fill=\"#619CFF\") + coord_flip()\r\n\r\n\r\n\r\nVisualize most frequent words using wordcloud\r\n\r\n\r\nlibrary(wordcloud)\r\npal <- brewer.pal(8, \"Dark2\")\r\ntidy_covid_data %>% \r\n  count(word) %>% \r\n  with(wordcloud(word, n, max.words=700, colors = pal))\r\n\r\n\r\n\r\nRelationship between words\r\nThe relationship between words can be vizualized in a netork graphs after tokenizing using n-grams. n-grams is breaking a text in two-, three or n-number of word tokens.\r\nFor now, we will only see bigrams(two-word tokens). Two word tokens can be represented as follows\r\ntext1: “How dangerous is COVID19?”\r\ntext 2: “How dangerous is COVID19 pandemic.?”\r\nText\r\nhow dangerous\r\ndangerous is\r\nis covi19\r\ncovid19 pandemic\r\nHow dangerous is COVID19?\r\n1\r\n1\r\n1\r\n0\r\nHow dangerous is COVID19 pandemic\r\n1\r\n1\r\n1\r\n1\r\nAs we did previously, lets’s do the tokenization once again again. This time usigng bigrams.\r\n\r\n\r\ncovid_bigram <- data[, c(1,2,3)] %>% #selct only titles, abstracts and keywords column of the data. Title is saved\r\n  unnest_tokens(output=bigram, \r\n                input=ABSTRACT,\r\n                token=\"ngrams\", n=2)\r\n\r\ndim(covid_bigram) # 79404 bigrams, 3 variables\r\n\r\n[1] 79404     3\r\n\r\nLet’s see the most frequent bigrams. Notice the irrelvant words.\r\n\r\n\r\nhead(\r\n  covid_bigram %>%\r\n  count(bigram, sort = TRUE)\r\n)\r\n\r\n# A tibble: 6 x 2\r\n  bigram       n\r\n  <chr>    <int>\r\n1 covid 19   989\r\n2 of the     571\r\n3 sars cov   443\r\n4 cov 2      401\r\n5 in the     384\r\n6 of covid   212\r\n\r\nRemove useless words\r\n\r\n\r\nlibrary(tidyr)\r\n\r\ncovid_bigrams_separated <- covid_bigram %>%\r\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\r\n\r\ncovd_bigrams_filtered <- covid_bigrams_separated %>%\r\n  filter(!word1 %in% stop_words$word) %>%\r\n  filter(!word2 %in% stop_words$word) %>% \r\n  filter(!word1 %in% custom_stop_words$word) %>%\r\n  filter(!word2 %in% custom_stop_words$word)\r\n\r\ndim(covd_bigrams_filtered) # 18981 bigrams, 4 variables\r\n\r\n[1] 18963     4\r\n\r\ncovd_bigrams_filtered_counts <- covd_bigrams_filtered %>% \r\n  count(word1, word2, sort = TRUE)\r\n\r\nhead(covd_bigrams_filtered_counts)\r\n\r\n# A tibble: 6 x 3\r\n  word1       word2           n\r\n  <chr>       <chr>       <int>\r\n1 coronavirus disease        95\r\n2 acute       respiratory    78\r\n3 respiratory syndrome       75\r\n4 severe      acute          68\r\n5 syndrome    coronavirus    54\r\n6 public      health         49\r\n\r\nAs you can see from the dim( ) call, the tokenization process changed the abstract of the 382 documents to 41000 tokens. That means we have 382 observations and 41000 one word columns. In the bigram tokenization, we have about 19000 bigrams for our 382 abstracts.\r\nNow, let’s join the separated bigrams.\r\n\r\n\r\ncovid_bigrams_united <- covd_bigrams_filtered %>%\r\n  unite(bigram, word1, word2, sep = \" \")\r\n\r\ncovd_bigrams_joined_counts <- covid_bigrams_united %>% \r\n  count(bigram, sort = TRUE)\r\n\r\nhead(covd_bigrams_joined_counts)\r\n\r\n# A tibble: 6 x 2\r\n  bigram                   n\r\n  <chr>                <int>\r\n1 coronavirus disease     95\r\n2 acute respiratory       78\r\n3 respiratory syndrome    75\r\n4 severe acute            68\r\n5 syndrome coronavirus    54\r\n6 public health           49\r\n\r\nVizualize bigrams\r\nWord cloud\r\n\r\n\r\n#Let's visualize 150 most common words\r\nlibrary(wordcloud)\r\npal <- brewer.pal(8, \"Dark2\")\r\ncovid_bigrams_united %>% \r\n  count(bigram) %>% \r\n  with(wordcloud(bigram, n, max.words=150, colors = pal))\r\n\r\n\r\n\r\nThe word cloud above shows, “severe acute”, “wuhan china”, “air pollution”, “rt pcr”, “personal protective”, “cytokine storm” and “social distancing” as most common bigrams in the covid19 publications included in this analysis.\r\nNetwork graph\r\n\r\n\r\nlibrary(igraph)\r\nlibrary(ggplot2)\r\nbigram_graph <- covd_bigrams_filtered_counts %>%\r\n  filter(n > 8) %>% #words mentioned more than 8 timse\r\n  graph_from_data_frame()\r\n\r\nbigram_graph\r\n\r\nIGRAPH 21ccd7f DN-- 103 70 -- \r\n+ attr: name (v/c), n (e/n)\r\n+ edges from 21ccd7f (vertex names):\r\n [1] coronavirus->disease      acute      ->respiratory \r\n [3] respiratory->syndrome     severe     ->acute       \r\n [5] syndrome   ->coronavirus  public     ->health      \r\n [7] health     ->care         nâ         ->â           \r\n [9] mental     ->health       air        ->quality     \r\n[11] rt         ->pcr          â          ->â           \r\n[13] world      ->health       health     ->organization\r\n[15] pm         ->10           air        ->pollution   \r\n+ ... omitted several edges\r\n\r\nlibrary(ggraph)\r\nset.seed(2017)\r\n\r\nggraph(bigram_graph, layout = \"fr\") +\r\n  geom_edge_link() +\r\n  geom_node_point() +\r\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1)\r\n\r\n\r\n\r\nTerm frequency inverse document frequency (tf-idf) is a weighted numerical representation of how a certain word is important in a document. It is calculated using the following formula. tf_idf can be done for the one word columns. For now, let’s just do for bigrams instead.\r\n\\[tfidf( t, d, D ) = tf( t, d ) \\times idf( t, D )\\] \\[idf( t, D ) = log \\frac{ \\text{| } D \\text{ |} }{ 1 + \\text{| } \\{ d \\in D : t \\in d \\} \\text{ |} }\\]\r\nWhere t is the terms appearing in a document; d denotes each document; D denotes the collection of documents.\r\n\r\n\r\ncovid_bigram_tf_idf <- covid_bigrams_united %>%\r\n  count(TITLE, bigram) %>%\r\n  bind_tf_idf(bigram, TITLE, n) %>%\r\n  arrange(desc(tf_idf))\r\n\r\nhead(covid_bigram_tf_idf)\r\n\r\n# A tibble: 6 x 6\r\n  TITLE                                bigram     n    tf   idf tf_idf\r\n  <chr>                                <chr>  <int> <dbl> <dbl>  <dbl>\r\n1 Arthroplasty during the COVID-19 Pa~ arthr~     1 0.5    5.95   2.97\r\n2 Arthroplasty during the COVID-19 Pa~ safel~     1 0.5    5.95   2.97\r\n3 COVID-19 Reveals Brugada Pattern in~ bruga~     3 0.375  5.95   2.23\r\n4 Neutrophils and COVID-19: Nots, NET~ incre~     1 0.333  5.95   1.98\r\n5 Neutrophils and COVID-19: Nots, NET~ pande~     1 0.333  5.25   1.75\r\n6 COVID-19 Time Capsule 1: DATE: Marc~ time ~     2 0.286  5.95   1.70\r\n\r\ncovid_bigram_tf_idf %>%\r\n  arrange(desc(tf_idf)) %>%\r\n  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% \r\n  top_n(15) %>% \r\n  ggplot(aes(bigram, tf_idf)) +\r\n  geom_col(show.legend = FALSE, fill=\"cyan4\") +\r\n  labs(x = NULL, y = \"tf-idf\") +\r\n  coord_flip()\r\n\r\n\r\n\r\nThe above tf_idf plot shows, “athroplasyty practice”, “safely navigate”, “brugada pattern”, etc were bigrams having the highest tf_idf values.\r\nThat is all for today.\r\nContact\r\n@MihiretuKebede1\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-08-10-2020-08-10-text-analysis-of-covid-publications/2020-08-10-text-analysis-of-covid-publications_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2023-03-23T21:56:13+01:00",
    "input_file": "2020-08-10-text-analysis-of-covid-publications.knit.md",
    "preview_width": 2600,
    "preview_height": 1600
  },
  {
    "path": "posts/2020-08-04-2020-08-04-abiyahmed/",
    "title": "Digital Reactions Towards Prime Minister Abiy Ahmed's Facebook Activities",
    "description": "Let's have some fun in visualizing Facebook reactions to prime minister Abiy Ahmed's Facebook posts (June 24 to August 09). This post is an updated version. The previous analysis was only until August 5.",
    "author": [
      {
        "name": "Mihiretu Kebede(PhD)",
        "url": {}
      }
    ],
    "date": "2020-08-06",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nData\r\nInitialize\r\nRequired packages\r\n\r\nSome descriptives\r\nVisualize\r\nReshape the data\r\nHow about positive and negative reactions\r\nNegative reactions\r\nNumber of people reacting to his posts\r\nAre people getting more angry at the the prime minister after Haccalu’s death?\r\nHow about love reactions?\r\nAre people sad about his posts?\r\nInterpretation\r\nContact\r\n\r\n\r\nDisclaimer\r\nThis blog post is simply about visualizing the data. I have no political affiliation.\r\nIntroduction\r\nHaccalu Hundessa, the popular Ethiopian singer, was killed in June 29, 2020. His death sparked a widespread violence in Oromia region. Several innocent civilians were murdered and public and private properties worth of millions were vandalized. The government has immediately taken measures to restore law and order. I am sincerely hoping prime minister Abiy’s government will take significant steps in letting an independent and international investigation. As a concerned Ethiopian, I have tried to follow some of the news. To this date, the news is giving me nightmares!\r\nIn this short blog posts, I will dive into prime minister Abiy’s Facebook activity and his followers reactions using emojis. I will then compare positive (like, care, love, wow) and negative reactions(angry, sad, haha) towards his posts on Facebook. More specificically, I will show you the trend of love, anger and sad reactions.\r\nNB: classifying the reactions into positve/negative may not reflect the actual reaction of the individual to the news. It doesn’t also reflect the contents of the prime minister’s Facebbook post. In addition, internet was locked from the first of july until 23rd of July. That is clearly visible from the plots with a simple horizontal line.\r\nData\r\nI have manually collected Prime Minister Abiy Ahmed’s one month Facebook posts (From June 24 until August 09). I collected few variables: date Abiy Ahmed posted on Facebook, type of Facebook reaction (eg: Like, Love, angry, sad, etc…) and total number of reactions.\r\nInitialize\r\nRequired packages\r\n\r\n\r\nlibrary(readxl) \r\nlibrary(ggplot2) \r\nlibrary(jpeg) \r\nlibrary(directlabels) \r\nlibrary(dplyr)\r\nlibrary(data.table) \r\nlibrary(scales)\r\nlibrary(readxl)\r\nlibrary(jpeg)\r\nlibrary(ggimage)\r\n\r\n\r\n\r\n\r\nabiy_ahmed <- read_xlsx(\"F:/github/githubwebsite/_posts/2020-08-04-2020-08-04-abiyahmed/abiy_ahmed.xlsx\")\r\n# As usual \r\ndim(abiy_ahmed)\r\n\r\n[1] 154   4\r\n\r\nglimpse(abiy_ahmed)\r\n\r\nRows: 154\r\nColumns: 4\r\n$ Date_posted   <dttm> 2020-06-24, 2020-06-24, 2020-06-24, 2020-06-2~\r\n$ Reaction      <chr> \"Like\", \"Love\", \"Care\", \"Angry\", \"Haha\", \"Wow\"~\r\n$ Count         <dbl> 21000, 876, 272, 50, 64, 22, 3, 92000, 5500, 1~\r\n$ Death_Haccalu <chr> \"Before\", \"Before\", \"Before\", \"Before\", \"Befor~\r\n\r\n#abiy_ahmed <- abiy_ahmed %>% \r\n#  rename(Date_postedX.U.FEFF.Date_posted)\r\n\r\n\r\nSome descriptives\r\nIn total, about a million (932453 ) people showed some kind of reaction. On average, more than 35206(sd=20293) peple liked his posts, an average of 840 people hit the angry buttons.\r\n\r\n\r\n# Mean per reaction\r\nlibrary(dplyr)\r\nabiy_ahmed %>%  \r\n  summarize(sum=sum(Count)) # 932453\r\n\r\n# A tibble: 1 x 1\r\n     sum\r\n   <dbl>\r\n1 932453\r\n\r\nmean <- abiy_ahmed %>% \r\n  group_by(Reaction) %>% \r\n  summarize(mean=mean(Count, na.rm=T)) %>% \r\n  mutate(mean=round(mean,2)) %>% \r\n  ungroup()\r\n\r\nsd <- abiy_ahmed %>% \r\n  group_by(Reaction) %>% \r\n  summarize(sd=sd(Count, na.rm=T)) %>%\r\n  mutate(sd=round(sd,2)) %>% \r\n  ungroup()\r\n\r\nmerge(mean, sd)\r\n\r\n  Reaction     mean       sd\r\n1    Angry   840.10   656.16\r\n2     Care   670.50   441.66\r\n3     Haha   296.55   141.33\r\n4     Like 35206.52 20293.70\r\n5     Love  3014.17  2179.53\r\n6      Sad   633.43  2066.60\r\n7      Wow    90.09    94.38\r\n\r\nVisualize\r\n\r\n\r\nplot <- ggplot(abiy_ahmed, aes(x=Date_posted,\r\n                               y=Count, col=Reaction)) +\r\n  geom_point(size=2) +\r\n  geom_line(size=1) \r\n\r\nplot1 <- direct.label(plot, \"first.qp\", )\r\n\r\nplot1\r\n\r\n\r\n\r\n\r\n\r\nplot <- ggplot(abiy_ahmed, aes(x=Date_posted,\r\n                             y=Count, col=Reaction)) +\r\n  geom_point(size=2) + geom_line(size=1.5) + theme_void() + \r\n  xlab(\"Date of Facebook post\") + \r\n  ylab(\"Number of reactions\") \r\n\r\nplot1 <- direct.label(plot, \"first.qp\")\r\n\r\nplot2 <- ggbackground(plot1, \"abiy.png\") \r\nplot2\r\n\r\n\r\n\r\nReshape the data\r\nFor easier manipulation, let’s reshape our data using reshape2.\r\n\r\n\r\nlibrary(reshape2)\r\nlibrary(ggplot2)\r\nabiy_data <- melt(abiy_ahmed, id=c(\"Date_posted\", \"Reaction\", \"Death_Haccalu\"))\r\nabiy_data$Date_posted <- as.Date(abiy_data$Date_posted)\r\n\r\nggplot(abiy_data, aes(x=Date_posted, y=value, col=Reaction)) + geom_point() + xlab(\"Date of Facebook post\") + ylab(\"Number of reactions\") +\r\n  geom_line() + scale_x_date(date_breaks = \"7 day\") + theme(axis.text.x = element_text(angle = 25, vjust = 1.0, hjust = 1.0))\r\n\r\n\r\n\r\nThe graph above shows the prime minister has much more likes than the other reactions. Since there are a lot more likes than the other reactions, we will filter out the Likes to have a good picture of the other reactions. Then, we will dive more into the data to see the trends of love and angry reactions.\r\n\r\n\r\nothethan_like <-  abiy_data %>% \r\n  group_by(Date_posted) %>% \r\n  filter(Reaction !=\"Like\") \r\n\r\ndirect.label(\r\n  ggplot(othethan_like, aes(x=Date_posted, y=value, col=Reaction)) + geom_point(size=2) +\r\n  geom_line(size=1) +\r\n  xlab(\"Date of Facebook post\") + ylab(\"Number of lovely reaction\") +\r\n  ylab(\"Number of lovely reactions\") +\r\n  scale_x_date(date_breaks = \"7 day\", labels = date_format(\"%B %d\") ) + theme(axis.text.x = element_text(angle = 25, vjust = 1.0, hjust = 1.0)),\r\n  \"first.qp\",\r\n)\r\n\r\n\r\n\r\nHow about positive and negative reactions\r\nPositive reactions: “Like”, “Love”, “Wow”, “Care” Negative reactions: “Angry”, “Sad”, “Haha”\r\n\r\n\r\nabiy_pos <- abiy_data %>% \r\n  group_by(Date_posted) %>% \r\n  filter(Reaction == c(\"Like\", \"Love\", \"Wow\", \"Care\")) %>% \r\n  mutate(number_reaction=sum(value),\r\n         reaction=\"positive reaction\")  \r\n\r\nabiy_neg <- abiy_data %>% \r\n  group_by(Date_posted) %>% \r\n  filter(Reaction == c(\"Angry\", \"Sad\", \"Haha\")) %>% \r\n  mutate(number_reaction=sum(value),\r\n         reaction=\"negative reaction\")\r\n\r\nabiy_pos_neg <- rbind(abiy_pos, abiy_neg)\r\n\r\n\r\nggplot(abiy_pos_neg, aes(x=Date_posted, y=number_reaction, col=reaction)) + geom_point(size=2) +\r\n  geom_line(size=1) + scale_x_date(date_breaks = \"5 day\") + theme(axis.text.x = element_text(angle = 25, vjust = 1.0, hjust = 1.0)) +\r\n  xlab(\"Date of Facebook post\") + ylab(\"Reaction\") + \r\n  scale_x_date(date_breaks = \"7 day\", labels = date_format(\"%B %d\") ) + theme(axis.text.x = element_text(angle = 25, vjust = 1.0, hjust = 1.0))\r\n\r\n\r\n\r\nNegative reactions\r\nSince there is a lot of positive reaction, we can’t see the negative reaction very much in the above plot. Let’s just focus on the negative reactions. See, it declinined after its maximum on the 30th of June. I assume, this anger is more to the news not to his opinions.\r\n\r\n\r\nggplot(abiy_neg, aes(x=Date_posted, y=number_reaction)) + geom_point(size=2.5) +\r\n  geom_line(size=1.5, col=\"darkred\") + xlab(\"Date of Facebook post\") +\r\n  ylab(\"Number of negative reaction\") +\r\n  scale_x_date(date_breaks = \"7 day\", labels = date_format(\"%B %d\") ) + theme(axis.text.x = element_text(angle = 25, vjust = 1.0, hjust = 1.0))\r\n\r\n\r\n\r\nNumber of people reacting to his posts\r\nIt is some how declining. Are people loosing interest? I don’t know. I hope not.\r\n\r\n\r\nabiy_reaction_sum <- abiy_data %>% \r\n  group_by(Date_posted) %>% \r\n  mutate(number_reaction=sum(value),\r\n         reaction=\"total reaction\")\r\n\r\nggplot(abiy_reaction_sum, aes(x=Date_posted, y=number_reaction)) + geom_point(size=2) +\r\n  geom_line(size=1, col=\"darkgreen\") + scale_x_date(date_breaks = \"5 day\") + theme(axis.text.x = element_text(angle = 25, vjust = 1.0, hjust = 1.0))  + xlab(\"Date of Facebook post\") +\r\n  ylab(\"Total reactions\") +\r\n  scale_x_date(date_breaks = \"5 day\", labels = date_format(\"%B %d\") ) + theme(axis.text.x = element_text(angle = 25, vjust = 1.0, hjust = 1.0))\r\n\r\n\r\n\r\nAre people getting more angry at the the prime minister after Haccalu’s death?\r\nSee how the anger tops the 30th of July and then it fell down like an avalanche\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nlibrary(lubridate)\r\nlibrary(scales)\r\nangry <- abiy_data %>% \r\n  group_by(Date_posted) %>% \r\n  filter(Reaction == \"Angry\") \r\n\r\nabiy_data %>% \r\n  group_by(Death_Haccalu) %>% \r\n  filter(Reaction == \"Angry\") %>% \r\n  summarize(mean(value))\r\n\r\n# A tibble: 2 x 2\r\n  Death_Haccalu `mean(value)`\r\n  <chr>                 <dbl>\r\n1 After                  930.\r\n2 Before                 333.\r\n\r\n  ggplot(angry, aes(x=Date_posted, y=value)) + geom_point(size=2) +\r\n  geom_line(size=1.5, col=\"red\") +\r\n  xlab(\"Date of Facebook post\") + ylab(\"Number of angry reaction\") +\r\n  scale_x_date(date_breaks = \"7 day\", labels = date_format(\"%B %d\") ) + theme(axis.text.x = element_text(angle = 25, vjust = 1.0, hjust = 1.0))\r\n\r\n\r\n\r\nHow about love reactions?\r\nIt looks like people are getting less angry at his posts. But, are they loving his posts? I don’t think so. But, let’s see.\r\n\r\n\r\nlove <-  abiy_data %>% \r\n  group_by(Date_posted) %>% \r\n  filter(Reaction == c(\"Care\", \"Love\")) %>% \r\n  mutate(love_reaction=sum(value),\r\n         reaction=\"love reaction\")\r\n\r\nggplot(love, aes(x=Date_posted, y=love_reaction)) + geom_point(size=2.5) +\r\n  geom_line(size=1.5, col=\"darkgreen\") +\r\n  xlab(\"Date of Facebook post\") + ylab(\"Number of lovely reaction\") +\r\n  ylab(\"Number of lovely reactions\") +\r\n  scale_x_date(date_breaks = \"5 day\", labels = date_format(\"%B %d\") ) + theme(axis.text.x = element_text(angle = 25, vjust = 1.0, hjust = 1.0))\r\n\r\n\r\n\r\nAre people sad about his posts?\r\nWe will see the same pattern for Sad reactions on August 4. But, what happened on August 4. It is the Lebanon’s explosion. People shared their sadness about the Lebanon explosion news and shared condolences with the prime minister.\r\n\r\n\r\n  sad <- abiy_data %>% \r\n  group_by(Date_posted) %>% \r\n  filter(Reaction==\"Sad\")\r\n\r\nggplot(sad, aes(x=Date_posted, y=value)) + geom_point(size=3) +\r\n  geom_line(size=1.5, col=\"red\") +\r\n  xlab(\"Date of Facebook post\") + ylab(\"Number of sad reaction\") +\r\n  ylab(\"Number of sad reaction\") +\r\n  scale_x_date(date_breaks = \"5 day\", labels = date_format(\"%B %d\") ) + theme(axis.text.x = element_text(angle = 25, vjust = 1.0, hjust = 1.0))\r\n\r\n\r\n\r\nInterpretation\r\nThe anger and sadness sharply increased until it reached its peak on the 30th of June. Then, after three weeks of internet black out, his Facebook page became active and continued posting and I continued collecting my data. The plots above show how negative reactions have sharply declined. It seems the measures have worked very well to alleviate digital anger from the people or at least it is possible to hypothesize people were positive about the measures taken by his government. With this, I can say Abiy Ahmed has likely regained his control and his government started to properly use its teeth.\r\nPeace and love for my beautiful country and her people!\r\nI conclude my blog with a quote by Lois McMaster Bujold “The dead cannot cry out for justice. It is a duty of the living to do so for them.”\r\nContact\r\nMihiretuKebede1\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-08-04-2020-08-04-abiyahmed/abiy.png",
    "last_modified": "2023-03-29T21:03:49+02:00",
    "input_file": "2020-08-04-abiyahmed.knit.md",
    "preview_width": 2292,
    "preview_height": 1524
  },
  {
    "path": "posts/2020-08-03-2020-08-03-covid19/",
    "title": "Applying bibliometric analysis and text mining to COVID-19 publications",
    "description": "Let's have a quick overview of what on COVID-19 related records were added in PubMed on August 03/2020.",
    "author": [
      {
        "name": "Mihiretu Kebede (PhD)",
        "url": {}
      }
    ],
    "date": "2020-08-04",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\nContents\r\nPlan of attack\r\nSource of data\r\nInitialize\r\nLoad necessary packages\r\n\r\nImport the data to R: Covert the BibTeX file to data frame\r\nNow bibliometrics analysis\r\nNow, let’s use bib2df\r\nInspect the data\r\nLet’s filter out the records with no Abstract\r\nI am interested only in Journal articles\r\nText mining\r\nTokenization\r\nRemove stop words\r\nCustomize stop_words\r\nWord cloud\r\nHow are words connected to each other in the records?\r\nThe way forward\r\nContact\r\n\r\n\r\nPlan of attack\r\nSource of data\r\nLet’s do a quick PubMed search. Copy the following search terms and search on PubMed.\r\nSearch terms: (covid 19) AND ((“2020/08/02”[Date - Publication] : “3000”[Date - Publication]))\r\nAlternatively you can find the search link here.\r\nWith the above search terms, we can retrieve 601 COVID-19 related records that were indexed in PubMed on August 03, 2020.\r\nWe can now open these records using our citation manager: EndNote, Mendeley, etc.\r\nSince the file is in RIS format, we must convert it to BibTeX so that we can benefit from Bibliomterix package.\r\nI will come back in the future in another blog post to show you on how to convert RIS file to BibTeX or converting from CSV/XSL reference file to EndNote-readable format. For today, let’s just use the converted COVID19 BibTeX file. We can load convert our BibTeX file using either bibliometrix and then bib2df package.\r\nLet’s first use bibliometrix and then bib2df package to convert our COVID19 BibTeX file to data frame.\r\nInitialize\r\nSet working directory\r\nLoad necessary packages\r\n\r\n\r\nlibrary(bibliometrix) #for bibliometric analysis\r\nlibrary(dplyr) #for data management\r\nlibrary(ggplot2) #for plotting\r\nlibrary(tidytext) #for text mining\r\nlibrary(bib2df) #for converting bib file to data frame\r\nlibrary(wordcloud) #for plotting most frequent words\r\n\r\n\r\nImport the data to R: Covert the BibTeX file to data frame\r\n\r\n\r\ncovid19_bibanalysis <- convert2df(\"covid19.bib\", dbsource = \"isi\", format = \"bibtex\")\r\n\r\n\r\nConverting your isi collection into a bibliographic dataframe\r\n\r\n\r\nWarning:\r\nIn your file, some mandatory metadata are missing. Bibliometrix functions may not work properly!\r\n\r\nPlease, take a look at the vignettes:\r\n- 'Data Importing and Converting' (https://www.bibliometrix.org/vignettes/Data-Importing-and-Converting.html)\r\n- 'A brief introduction to bibliometrix' (https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html)\r\n\r\n\r\nMissing fields:  ID C1 CR \r\nDone!\r\n\r\n#As usual, isnpect the data\r\ndim(covid19_bibanalysis) #601 records, 23 variables\r\n\r\n[1] 601  25\r\n\r\nglimpse(covid19_bibanalysis) #Inspect the structure of data, variable names, etc\r\n\r\nRows: 601\r\nColumns: 25\r\n$ AU          <chr> \"GELZINIS TA\", \"COCCIA M\", \"ATAGUBA OA;ATAGUBA J~\r\n$ DE          <chr> NA, \"AIR POLLUTANTS;AIR POLLUTION;BETACORONAVIRU~\r\n$ AB          <chr> NA, \"THIS STUDY HAS TWO GOALS. THE FIRST IS TO E~\r\n$ BO          <chr> \"JOURNAL OF CARDIOTHORACIC AND VASCULAR ANESTHES~\r\n$ DI          <chr> \"10.1053/j.jvca.2020.05.008\", \"10.1016/j.scitote~\r\n$ institution <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\r\n$ SN          <chr> \"1532-8422 (ELECTRONIC)\", \"1879-1026 (ELECTRONIC~\r\n$ SO          <chr> \"JOURNAL OF CARDIOTHORACIC AND VASCULAR ANESTHES~\r\n$ LA          <chr> \"ENG\", \"ENG\", \"ENG\", \"ENG\", \"ENG\", \"ENG\", \"ENG\",~\r\n$ month       <chr> \"SEP\", \"AUG\", \"DEC\", \"SEP\", \"DEC\", \"NOV\", \"SEP\",~\r\n$ PN          <chr> \"9\", NA, \"1\", NA, \"1\", NA, NA, \"1\", NA, NA, \"3\",~\r\n$ PP          <chr> \"2328--2330\", \"138474\", \"1788263\", \"312--321\", \"~\r\n$ pmid        <chr> \"32406428\", \"32498152\", \"32657669\", \"32546875\", ~\r\n$ TI          <chr> \"THORACIC ANESTHESIA IN THE CORONAVIRUS DISEASE ~\r\n$ VL          <chr> \"34\", \"729\", \"13\", \"117\", \"9\", \"110\", \"83\", \"9\",~\r\n$ PY          <dbl> 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, ~\r\n$ DB          <chr> \"ISI\", \"ISI\", \"ISI\", \"ISI\", \"ISI\", \"ISI\", \"ISI\",~\r\n$ JI          <chr> \"J. Cardiothorac. Vasc. Anesth.\", \"Sci. Total. E~\r\n$ J9          <chr> \"J. Cardiothorac. Vasc. Anesth.\", \"Sci. Total. E~\r\n$ TC          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\r\n$ CR          <chr> \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", ~\r\n$ C1          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\r\n$ AU_UN       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\r\n$ SR_FULL     <chr> \"GELZINIS TA, 2020, J. Cardiothorac. Vasc. Anest~\r\n$ SR          <chr> \"GELZINIS TA, 2020, J. Cardiothorac. Vasc. Anest~\r\n\r\nNow bibliometrics analysis\r\n\r\n\r\nresults <-  biblioAnalysis(covid19_bibanalysis, sep = \",\") #create the object\r\noptions(width=100) #to determine width of the plot\r\n\r\ns <- summary(object=results, k=10, pase=FALSE) #to present 10 most prominent authors, journals, keywords, etc\r\n\r\n\r\n\r\nMAIN INFORMATION ABOUT DATA\r\n\r\n Timespan                              2020 : 2021 \r\n Sources (Journals, Books, etc)        206 \r\n Documents                             601 \r\n Annual Growth Rate %                  -99.67 \r\n Document Average Age                  3 \r\n Average citations per doc             0 \r\n Average citations per year per doc    0 \r\n References                            1 \r\n \r\nDOCUMENT CONTENTS\r\n Keywords Plus (ID)                    0 \r\n Author's Keywords (DE)                269 \r\n \r\nAUTHORS\r\n Authors                               594 \r\n Author Appearances                    601 \r\n Authors of single-authored docs       594 \r\n \r\nAUTHORS COLLABORATION\r\n Single-authored docs                  601 \r\n Documents per Author                  1.01 \r\n Co-Authors per Doc                    1 \r\n International co-authorships %        0 \r\n \r\n\r\nAnnual Scientific Production\r\n\r\n Year    Articles\r\n    2020      599\r\n    2021        2\r\n\r\nAnnual Percentage Growth Rate -99.67 \r\n\r\n\r\nMost Productive Authors\r\n\r\n                                                          Authors        Articles\r\n1  NA NA                                                                        3\r\n2  BENTATA Y                                                                    2\r\n3  DAUGHTON CG                                                                  2\r\n4  HERMAN JA;URITS I;KAYE AD;URMAN RD;VISWANATH O                               2\r\n5  KAPOOR I;PRABHAKAR H;MAHAJAN C                                               2\r\n6  TAHAN HM                                                                     2\r\n7  ABD EL-AZIZ TM;STOCKAND JD                                                   1\r\n8  ABDELMAKSOUD A;GOLDUST M;VESTITA M                                           1\r\n9  ABDULLAH MS;CHONG PL;ASLI R;MOMIN RN;MANI BI;METUSSIN D;CHONG VH             1\r\n10 ABDULLAH S;MANSOR AA;NAPI NNLM;MANSOR WNW;AHMED AN;ISMAIL M;RAMLY ZTA        1\r\n                                                          Authors        Articles Fractionalized\r\n1  NA NA                                                                                       3\r\n2  BENTATA Y                                                                                   2\r\n3  DAUGHTON CG                                                                                 2\r\n4  HERMAN JA;URITS I;KAYE AD;URMAN RD;VISWANATH O                                              2\r\n5  KAPOOR I;PRABHAKAR H;MAHAJAN C                                                              2\r\n6  TAHAN HM                                                                                    2\r\n7  ABD EL-AZIZ TM;STOCKAND JD                                                                  1\r\n8  ABDELMAKSOUD A;GOLDUST M;VESTITA M                                                          1\r\n9  ABDULLAH MS;CHONG PL;ASLI R;MOMIN RN;MANI BI;METUSSIN D;CHONG VH                            1\r\n10 ABDULLAH S;MANSOR AA;NAPI NNLM;MANSOR WNW;AHMED AN;ISMAIL M;RAMLY ZTA                       1\r\n\r\n\r\nTop manuscripts per citations\r\n\r\n                                                                          Paper         \r\n1  GELZINIS TA, 2020, J. Cardiothorac. Vasc. Anesth.                                    \r\n2  COCCIA M, 2020, Sci. Total. Environ.                                                 \r\n3  ATAGUBA OA, 2020, Glob. Health Action                                                \r\n4  SIGALA M, 2020, J. Bus. Res.                                                         \r\n5  LIU XH, 2020, Emerg. MICROBES \\\\& Infect.                                            \r\n6  LECHNER WV, 2020, Addict. Behav.                                                     \r\n7  CAGLIANI R, 2020, Infect. Genet. Evol. : J. Mol. Epidemiol. Evol. Genet. Infect. Dis.\r\n8  OKBA NMA, 2020, Emerg. MICROBES \\\\& Infect.                                          \r\n9  DONTHU N, 2020, J. Bus. Res.                                                         \r\n10 VAN DORP L, 2020, Infect. Genet. Evol. : J. Mol. Epidemiol. Evol. Genet. Infect. Dis.\r\n                               DOI TC TCperYear NTC\r\n1  10.1053/j.jvca.2020.05.008       0         0 NaN\r\n2  10.1016/j.scitotenv.2020.138474  0         0 NaN\r\n3  10.1080/16549716.2020.1788263    0         0 NaN\r\n4  10.1016/j.jbusres.2020.06.015    0         0 NaN\r\n5  10.1080/22221751.2020.1766383    0         0 NaN\r\n6  10.1016/j.addbeh.2020.106527     0         0 NaN\r\n7  10.1016/j.meegid.2020.104353     0         0 NaN\r\n8  10.1080/22221751.2020.1760735    0         0 NaN\r\n9  10.1016/j.jbusres.2020.06.008    0         0 NaN\r\n10 10.1016/j.meegid.2020.104351     0         0 NaN\r\n\r\n\r\nMost Relevant Sources\r\n\r\n                                       Sources        Articles\r\n1  EMERGING MICROBES \\\\& INFECTIONS                         61\r\n2  THE SCIENCE OF THE TOTAL ENVIRONMENT                     54\r\n3  CHAOS SOLITONS AND FRACTALS                              22\r\n4  MEDICAL EDUCATION ONLINE                                 21\r\n5  JOURNAL OF AFFECTIVE DISORDERS                           18\r\n6  JOURNAL OF CLINICAL ANESTHESIA                           18\r\n7  SEXUAL AND REPRODUCTIVE HEALTH MATTERS                   14\r\n8  NEUROLOGY(R) NEUROIMMUNOLOGY \\\\& NEUROINFLAMMATION       13\r\n9  INFECTIOUS DISEASES (LONDON ENGLAND)                     10\r\n10 INTEGRATIVE MEDICINE RESEARCH                            10\r\n\r\nplot(x=results, k=10, pause=FALSE) #plot the results\r\n\r\n\r\n#since all of them are indexed in August 2020, this may not be useful analysis. \r\n\r\n# Let's see sankey plots. But, before that we must remove missing values\r\nthreeFieldsPlot(covid19_bibanalysis, fields = c(\"AU\", \"DE\", \"SO\"))\r\n\r\n\r\n\r\nLook at this plot. It maps Authors with keywords and with the journals. With 61 articles indexed in PubMed in one day, a journal called, EMERGING MICROBES \\& INFECTIONS is leading the league.\r\n\r\n\r\n## Now let's do some clustering\r\n# Let's just use key words\r\ncs <- conceptualStructure(covid19_bibanalysis, field=\"DE\", \r\n                          method=\"CA\", minDegree = 4,\r\n                          stemming=FALSE, labelsize = 10, documents=2) \r\n\r\n\r\n# Let's now use Akey words\r\n# is abit dense to vizualize it here. It is very dense. \r\ncsAB <- conceptualStructure(covid19_bibanalysis, field=\"DE\", \r\n                          method=\"CA\", minDegree = 4,\r\n                          stemming=FALSE, labelsize = 10, documents=2)\r\n\r\n\r\n\r\n\r\n\r\nknitr::include_graphics(\"pic.jpg\")\r\n\r\n\r\n\r\nThe figure above shows keywords of the articles are clustered into five main categories.\r\nClinical, molecular and epidemiological keywords: the biggest part of the cluster with blue text on the figure\r\nEnvironmental monitoring cluster\r\nHumidity studies cluster\r\nMedical education cluster\r\nOrganization and administration cluster\r\nNow, let’s use bib2df\r\nIt is also possible to continue our analysis using the covid19_bibanalysisdata. However, it is good to try another package, bib2df. We will use this package to convert our BibTeX file to data frame and then from there we can perform text mining. It is also good to note that the data frame converted using bibliometrix package may not also work properly with dplyr. But, that depends with your rlang. I have experienced some errors using bibliometrix package with dplyr.\r\n\r\n\r\nlibrary(bib2df)\r\ncovid19 <- bib2df(\"covid19.bib\")\r\n\r\n\r\nInspect the data\r\n\r\n\r\nglimpse(covid19) \r\n\r\nRows: 601\r\nColumns: 32\r\n$ CATEGORY     <chr> \"MISC\", \"ARTICLE\", \"ARTICLE\", \"ARTICLE\", \"MISC\"~\r\n$ BIBTEXKEY    <chr> \"Gelzinis2020\", \"Coccia2020\", \"Ataguba2020\", \"S~\r\n$ ADDRESS      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ ANNOTE       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ AUTHOR       <list> \"Gelzinis, Theresa A\", \"Coccia, Mario\", <\"Atag~\r\n$ BOOKTITLE    <chr> \"Journal of cardiothoracic and vascular anesthe~\r\n$ CHAPTER      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ CROSSREF     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ EDITION      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ EDITOR       <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA~\r\n$ HOWPUBLISHED <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ INSTITUTION  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ JOURNAL      <chr> NA, \"The Science of the total environment\", \"Gl~\r\n$ KEY          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ MONTH        <chr> \"sep\", \"aug\", \"dec\", \"sep\", \"dec\", \"nov\", \"sep\"~\r\n$ NOTE         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ NUMBER       <chr> \"9\", NA, \"1\", NA, \"1\", NA, NA, \"1\", NA, NA, \"3\"~\r\n$ ORGANIZATION <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ PAGES        <chr> \"2328--2330\", \"138474\", \"1788263\", \"312--321\", ~\r\n$ PUBLISHER    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ SCHOOL       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ SERIES       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ TITLE        <chr> \"Thoracic Anesthesia in the Coronavirus Disease~\r\n$ TYPE         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\r\n$ VOLUME       <chr> \"34\", \"729\", \"13\", \"117\", \"9\", \"110\", \"83\", \"9\"~\r\n$ YEAR         <dbl> 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020,~\r\n$ DOI          <chr> \"10.1053/j.jvca.2020.05.008\", \"10.1016/j.scitot~\r\n$ ISSN         <chr> \"1532-8422 (Electronic)\", \"1879-1026 (Electroni~\r\n$ LANGUAGE     <chr> \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\"~\r\n$ PMID         <chr> \"32406428\", \"32498152\", \"32657669\", \"32546875\",~\r\n$ ABSTRACT     <chr> NA, \"This study has two goals. The first is to ~\r\n$ KEYWORDS     <chr> NA, \"Air Pollutants,Air Pollution,Betacoronavir~\r\n\r\n#Let's choose only some of our variabees\r\n\r\ncovid19data <- covid19 %>% \r\n  select(\"AUTHOR\", \"TITLE\", \"KEYWORDS\", \"ABSTRACT\", \"JOURNAL\",\"DOI\")\r\n\r\ndim(covid19) #601 records, 23 variables\r\n\r\n[1] 601  32\r\n\r\nwhich(!complete.cases(covid19$DOI)) # All of them have missing values\r\n\r\n[1]  87 551\r\n\r\nsum(is.na(covid19$ABSTRACT)) #159 studies have no abstract\r\n\r\n[1] 159\r\n\r\nsum(is.na(covid19$TITLE)) # 0 records have missing values\r\n\r\n[1] 0\r\n\r\nsum(is.na(covid19$DOI)) #2 studies have no doi\r\n\r\n[1] 2\r\n\r\nsum(!is.na(covid19$ABSTRACT)) #442 records have abstracts\r\n\r\n[1] 442\r\n\r\ntable(covid19$CATEGORY) #452 are journal aricles, 149 are Miscelaneous(books, conference abstracst, etc. \r\n\r\n\r\nARTICLE    MISC \r\n    452     149 \r\n\r\nLet’s filter out the records with no Abstract\r\n\r\n\r\n#Let's filter the data to retreive only the records with Abstract\r\n#As we have seen above, we have 159 records without abstracts\r\n#use dplyr to filter\r\nlibrary(dplyr)\r\ncovid19new <- covid19 %>%\r\n  filter(!is.na(ABSTRACT)) #remove all records with missing abstracts\r\nsum(!is.na(covid19new$ABSTRACT)) #442 records have abstracts\r\n\r\n[1] 442\r\n\r\n   #BINGO! We excluded the studies without Abstract. Now we, can play with text mining. \r\n\r\n\r\nI am interested only in Journal articles\r\nWe need to find out the type of these records: journal articles, books, conference abstracts, etc. We can then focus on journal articles having with abstracts included in PubMed.\r\n\r\n\r\ntable(covid19new$CATEGORY) #382 are journal articles, 60 are Miscelaneous documents\r\n\r\n\r\nARTICLE    MISC \r\n    382      60 \r\n\r\n# Filter journal articles having  abstracts \r\ncovid19new <- covid19new %>% \r\n  filter(CATEGORY==\"ARTICLE\") \r\n#Check \r\ntable(covid19new$CATEGORY) #382 journal articles\r\n\r\n\r\nARTICLE \r\n    382 \r\n\r\nText mining\r\nSince we already have our 442 studies with no missing abstract, we can conduct our text mining analysis using this data frame.\r\nThere are several r packages for conducting text mining. Most popular packages are tidytext, text2vec, quanteda, tm and stringr.\r\nFor today, I will simply follow tidytext.\r\nThere are some important steps in text mining such as tokenization, visualization, computing tf-idf statistics, n-grams, etc\r\nTerm frequency inverse document frequency (tf-idf) is a weighted numerical representation of how a certain word is important in a document. It is calculated using the following formula.\r\n\\[tfidf( t, d, D ) = tf( t, d ) \\times idf( t, D )\\]\r\n\\[idf( t, D ) = log \\frac{ \\text{| } D \\text{ |} }{ 1 + \\text{| } \\{ d \\in D : t \\in d \\} \\text{ |} }\\]\r\nWhere t is the terms appearing in a document; d denotes each document; D denotes the collection of documents.\r\n\r\n\r\n# selct few variables from our data set\r\ndata <- covid19new %>% \r\n  select(\"TITLE\", \"ABSTRACT\", \"KEYWORDS\", \"AUTHOR\")\r\n\r\n\r\nTokenization\r\nTokenization is the process of breaking a certain text into word by word columns. For example, if one abstract is written using 300 words, tokenizing the abstract will result in 300 columns for each word. This will make things easy to count words and do any further analysis.\r\n\r\ntidy_covid_data <- data %>% \r\n  unnest_tokens(input=ABSTRACT, output=word)\r\n\r\n\r\n\r\nRemove stop words\r\nStop words are words that are not very relevant to the meaning or concept of the document. For example, see this sentence. “The COVID19 pandemic is the biggest global health crisis of our time”. In this sentence, “the, is, of, our” are not relevant for the concept of this text. These words need to be removed from our analysis. To remove these words, we can use the stopwords data which is available for us. If we want to add additional stop words we can customize and create our own customized stop words.\r\n\r\n\r\ndata(\"stop_words\")\r\ntidy_covid_data <- tidy_covid_data %>% \r\n  anti_join(stop_words)\r\n\r\n\r\nCustomize stop_words\r\nUsually, abstract contains words like “background”, “introduction”, “materials”, “methods”, “results”, “conclusions”, etc. Let’s remove these words\r\n\r\n\r\n# since it is all about covid-19, we don't need covid \r\ncustom_stop_words <- bind_rows(tibble(word=c(\"covid\", \"cov\", \"background\", \"introduction\", \"materials\", \"methods\", \"results\", \"conclusions\", \"0\", \"1\",\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\"19\", \"2019\", \"2020\", \"95\"),\r\n                                      lexicon = c(\"custom\")),\r\n                               stop_words)\r\n\r\ntidy_covid_data <- tidy_covid_data %>% \r\n  anti_join(custom_stop_words)\r\n# count number of words and plot it\r\n tidy_covid_data %>% \r\n  count(word, sort=T) %>% \r\n   filter(n>100) %>% \r\n   mutate(word=reorder(word, n)) %>% \r\n   ggplot(aes(x=word, y=n)) + \r\n   geom_col(fill=\"#619CFF\") + coord_flip()\r\n\r\n\r\n\r\nWord cloud\r\n\r\n\r\nlibrary(wordcloud)\r\npal <- brewer.pal(8, \"Dark2\")\r\ntidy_covid_data %>% \r\n  count(word) %>% \r\n  with(wordcloud(word, n, max.words=300, colors = pal))\r\n\r\n\r\n\r\nHow are words connected to each other in the records?\r\nThis needs tokenizing using n-grams. This will be our next stop.\r\nThe way forward\r\nR is a powerfull open source tool for systematic reviewers. It is helpful to exclude duplicate records that your reference management software often misses. Currently, I am employing text mining applications to facilitate title/abstract screening, full text screening, clustering of studies and topic modelling. I found text mining is very interesting field and it is really appealing to learn. Looking at the current pace of medical literature, the future of systematic reviews lies on using automated tools, and leveraging text mining and machine learning algorithms. In my opinion, anyone who is interested in systematic reviews needs to consider text mining. The biggest challenge I have so far is, applying machine learning to cluster or classify abstracts is computationally intensive.The curse of high dimensionality!\r\nContact\r\nPlease mention MihiretuKebede1 if you tweet this post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-08-03-2020-08-03-covid19/pic.jpg",
    "last_modified": "2023-03-23T21:58:42+01:00",
    "input_file": "2020-08-03-covid19.knit.md"
  },
  {
    "path": "posts/2020-07-25-2020-07-25-diabetesprevalenceeurope/",
    "title": "Visualizing the prevalence of diabetes in six European countries, 1990-2017",
    "description": "This is a quick demonstration of diabetes prevalence in six European countries. The data are from the the Institute of Health Metrics and Evaluation (IHME).",
    "author": [
      {
        "name": "Mihiretu Kebede(PhD)",
        "url": {}
      }
    ],
    "date": "2020-07-25",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\nContents\r\nA step by-step guide on how to improve a simple scatter plot\r\nMy R version\r\n\r\nRequired packages\r\nLoad the data and have a closer look\r\nLet’s choose only 5 European countries with high diabetes prevalence\r\n\r\nBasic plots\r\nThe fun part\r\nUse gifski_renderer to loop or no to loop the gif\r\nLet’s tweak few things and see what happens\r\nContact\r\n\r\n\r\n\r\nA step by-step guide on how to improve a simple scatter plot\r\nMy R version\r\nSee below\r\n\r\n\r\nversion\r\n\r\n               _                           \r\nplatform       x86_64-w64-mingw32          \r\narch           x86_64                      \r\nos             mingw32                     \r\nsystem         x86_64, mingw32             \r\nstatus                                     \r\nmajor          4                           \r\nminor          1.3                         \r\nyear           2022                        \r\nmonth          03                          \r\nday            10                          \r\nsvn rev        81868                       \r\nlanguage       R                           \r\nversion.string R version 4.1.3 (2022-03-10)\r\nnickname       One Push-Up                 \r\n\r\nRequired packages\r\n\r\n\r\n#If you don't have any of these packages install them using install.packages(\"pakage)\r\nlibrary(readr) #to read csv file\r\nlibrary(dplyr) #for data manipulaion \r\nlibrary(ggplot2) # for awesome plotting\r\nlibrary(gganimate) #for animating ggplot objects\r\nlibrary(scales) # for customizing axis \r\nlibrary(lattice) #for enhancing graphics\r\nlibrary(directlabels) #for directly labeling!\r\nlibrary(transformr)\r\n\r\n\r\nLoad the data and have a closer look\r\n\r\n\r\n# The data is for all countries included in GBD studies\r\ndiabetes <- read_csv(\"Eurodiabetes.csv\") \r\ndim(diabetes) #980 observations and 9 variables. \r\n\r\n[1] 980   9\r\n\r\nstr(diabetes) \r\n\r\nspc_tbl_ [980 x 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r\n $ Location                : chr [1:980] \"Australia\" \"Australia\" \"Australia\" \"Australia\" ...\r\n $ Year                    : num [1:980] 1990 1991 1992 1993 1994 ...\r\n $ Age                     : chr [1:980] \"All ages\" \"All ages\" \"All ages\" \"All ages\" ...\r\n $ Sex                     : chr [1:980] \"Both\" \"Both\" \"Both\" \"Both\" ...\r\n $ Cause of death or injury: chr [1:980] \"Diabetes mellitus\" \"Diabetes mellitus\" \"Diabetes mellitus\" \"Diabetes mellitus\" ...\r\n $ Measure                 : chr [1:980] \"Percent of total prevalent cases\" \"Percent of total prevalent cases\" \"Percent of total prevalent cases\" \"Percent of total prevalent cases\" ...\r\n $ Value                   : num [1:980] 0.0503 0.0497 0.0492 0.0488 0.0486 ...\r\n $ Lower bound             : num [1:980] 0.0546 0.054 0.0532 0.0528 0.0526 ...\r\n $ Upper bound             : num [1:980] 0.0462 0.0459 0.0456 0.0453 0.045 ...\r\n - attr(*, \"spec\")=\r\n  .. cols(\r\n  ..   Location = col_character(),\r\n  ..   Year = col_double(),\r\n  ..   Age = col_character(),\r\n  ..   Sex = col_character(),\r\n  ..   `Cause of death or injury` = col_character(),\r\n  ..   Measure = col_character(),\r\n  ..   Value = col_double(),\r\n  ..   `Lower bound` = col_double(),\r\n  ..   `Upper bound` = col_double()\r\n  .. )\r\n - attr(*, \"problems\")=<externalptr> \r\n\r\nLet’s choose only 5 European countries with high diabetes prevalence\r\n\r\n\r\nger_au_ch <- diabetes %>% \r\n  filter(Location %in% c(\"Austria\", \"Germany\", \"Switzerland\", \"Denmark\", \"Portugal\", \"Finland\")) \r\nger_au_ch <- na.omit(ger_au_ch) #Remove missing values\r\nger_au_ch$Prev <- ger_au_ch$Value*100 #Prevalence in percent. \r\nstr(ger_au_ch) #structure of the data, variable types\r\n\r\ntibble [168 x 10] (S3: tbl_df/tbl/data.frame)\r\n $ Location                : chr [1:168] \"Austria\" \"Austria\" \"Austria\" \"Austria\" ...\r\n $ Year                    : num [1:168] 1990 1991 1992 1993 1994 ...\r\n $ Age                     : chr [1:168] \"All ages\" \"All ages\" \"All ages\" \"All ages\" ...\r\n $ Sex                     : chr [1:168] \"Both\" \"Both\" \"Both\" \"Both\" ...\r\n $ Cause of death or injury: chr [1:168] \"Diabetes mellitus\" \"Diabetes mellitus\" \"Diabetes mellitus\" \"Diabetes mellitus\" ...\r\n $ Measure                 : chr [1:168] \"Percent of total prevalent cases\" \"Percent of total prevalent cases\" \"Percent of total prevalent cases\" \"Percent of total prevalent cases\" ...\r\n $ Value                   : num [1:168] 0.0562 0.0579 0.0597 0.0614 0.0631 ...\r\n $ Lower bound             : num [1:168] 0.061 0.0627 0.0646 0.0664 0.0683 ...\r\n $ Upper bound             : num [1:168] 0.0519 0.0538 0.0555 0.0572 0.0588 ...\r\n $ Prev                    : num [1:168] 5.62 5.79 5.97 6.14 6.31 ...\r\n\r\ndim(ger_au_ch) #168 rows, 10 columns \r\n\r\n[1] 168  10\r\n\r\nis.factor(ger_au_ch$Year) #check if Year is saved as factor variable\r\n\r\n[1] FALSE\r\n\r\nger_au_ch$yearfactor <- factor(ger_au_ch$Year) #convert it to factor and save it as Yearfactor\r\n\r\nger_au_ch$Yearnumeric <- as.numeric(ger_au_ch$Year) #change it to numeric and save it as Year Numeric\r\n\r\n\r\nBasic plots\r\nSince we already have everything we need for plotting, we can start using ggplot2\r\n\r\n\r\nplot1 <- ggplot(ger_au_ch, aes(x=Yearnumeric, y=Prev, col=Location)) + \r\n  geom_line() + geom_point() + xlab(\"Year\") +\r\n  ylab(\"Prevalence of diabetes in %\") \r\nplot1\r\n\r\n\r\n\r\n\r\n\r\nplot2 <- ggplot(ger_au_ch, aes(x=Yearnumeric, y=Prev, col=Location)) + \r\n  geom_line() + xlab(\"Year\") +\r\n  ylab(\"Prevalence of diabetes in %\") \r\nplot2\r\n\r\n\r\n\r\nThe fun part\r\n\r\n\r\nlibrary(gganimate)\r\nlibrary(directlabels)\r\neuro_anim <- ggplot(ger_au_ch, aes(x=Yearnumeric, y=Prev, col=Location)) + \r\n  geom_point(size=6) + transition_time(Yearnumeric)  +\r\n  shadow_mark()  +  \r\n  scale_x_continuous(name =\"Year\",\r\n                     breaks= c(1990,1995,2000,2005, \r\n                               2010, 2015, 2020)) +\r\n  \r\n  xlab(\"Year\") +\r\n  ylab(\"Prevalence of diabetes in %\") +\r\n  labs(col=\"Country\") + \r\n  \r\n  theme(\r\n    axis.title.x = element_text(color = \"Blue\", size=15),\r\n    axis.title.y = element_text(color = \"Blue\", size=15),\r\n    axis.text.x = element_text(size = 15),\r\n    axis.text.y = element_text(size = 15),\r\n    \r\n    plot.title = element_text(size=20),\r\n    \r\n    legend.title = element_text(size = 10),\r\n    legend.text = element_text(size = 10),\r\n    legend.position = \"None\",\r\n    \r\n    text = element_text(family = \"Comics Sans MS\")\r\n  ) + ease_aes('cubic-in-out') +\r\n  geom_dl(aes(label=Location),\r\n          method=list(\"last.points\",rot=40)) \r\n\r\n\r\nUse gifski_renderer to loop or no to loop the gif\r\nLet’s see, how it looks if we assign RUE or just T in short for loop.\r\n\r\n\r\nanimate(euro_anim, renderer = gifski_renderer(loop = T), width = 700, height = 700, duration = 15) # when you assign loop=TRUE or just T, the gif starts playing again \r\n\r\n\r\n\r\nAssign loop to False or just F\r\n\r\n\r\nanimate(euro_anim, renderer = gifski_renderer(loop = F), width = 700, height = 700, duration = 15) # when you assign loop=TRUE or just F, the gif stops looping. It only plays once. Refresh if you want to see again  \r\n\r\n\r\nLet’s tweak few things and see what happens\r\n\r\n\r\neuro_anim <- ggplot(ger_au_ch, aes(x=Yearnumeric, y=Prev, col=Location)) + \r\n  geom_point(size=6) + transition_time(Yearnumeric)  +\r\n  shadow_mark()  +  \r\n  scale_x_continuous(name =\"Year\",\r\n                     breaks= c(1990,1995,2000,2005, \r\n                               2010, 2015, 2020)) +\r\n  \r\n  xlab(\"Year\") +\r\n  ylab(\"Prevalence of diabetes in %\") +\r\n  labs(col=\"Country\") + \r\n\r\n  shadow_wake(wake_length = 0.1, alpha = FALSE) +\r\n  theme(\r\n    axis.title.x = element_text(color = \"Blue\", size=15),\r\n    axis.title.y = element_text(color = \"Blue\", size=15),\r\n    axis.text.x = element_text(size = 15),\r\n    axis.text.y = element_text(size = 15),\r\n    \r\n    plot.title = element_text(size=20),\r\n    \r\n    legend.title = element_text(size = 10),\r\n    legend.text = element_text(size = 10),\r\n    legend.position = \"None\",\r\n    \r\n    text = element_text(family = \"Comics Sans MS\")\r\n  ) + ease_aes('cubic-in-out') +\r\n  geom_dl(aes(label=Location),\r\n          method=list(\"last.points\",rot=40)) \r\n\r\nanimate(euro_anim, renderer = gifski_renderer(loop = F), width = 700, height = 700, duration = 15)\r\n\r\n\r\n\r\nAnd again\r\n\r\n\r\neuro_anim3 <- ger_au_ch %>% \r\n  ggplot( aes(x=Yearnumeric, y=Prev, col=Location)) + geom_line() + geom_line() +\r\n  geom_point() +\r\n    transition_reveal(Yearnumeric) + \r\n  shadow_mark()  +  \r\n  scale_x_continuous(name =\"Year\",\r\n                     breaks= c(1990,1995,2000,2005, \r\n                               2010, 2015, 2020)) +\r\n  xlab(\"Year\") +\r\n  ylab(\"Prevalence of diabetes in %\") +\r\n  labs(col=\"Country\") + \r\n  shadow_wake(wake_length = 0.1, alpha = FALSE) +\r\n  theme(\r\n    axis.title.x = element_text(color = \"Blue\", size=15),\r\n    axis.title.y = element_text(color = \"Blue\", size=15),\r\n    axis.text.x = element_text(size = 15),\r\n    axis.text.y = element_text(size = 15),\r\n    \r\n    plot.title = element_text(size=20),\r\n    \r\n    legend.title = element_text(size = 10),\r\n    legend.text = element_text(size = 10),\r\n    legend.position = \"None\",\r\n    \r\n    text = element_text(family = \"Comics Sans MS\")\r\n  ) + ease_aes('cubic-in-out') +\r\n  geom_dl(aes(label=Location),\r\n          method=list(\"last.points\",rot=40))\r\n\r\neuro_anim3  \r\n\r\n\r\n\r\n\r\n# Shorter code\r\n ggplot(ger_au_ch, aes(x=Yearnumeric, y=Prev, col=Location)) + geom_line() + geom_line() +\r\n  geom_point() +\r\n    transition_reveal(Yearnumeric) + \r\n  shadow_mark()  +  \r\n  scale_x_continuous(name =\"Year\",\r\n                     breaks= c(1990,1995,2000,2005, \r\n                               2010, 2015, 2020)) +\r\n  xlab(\"Year\") +\r\n  ylab(\"Prevalence of diabetes in %\") +\r\n  labs(col=\"Country\") + ease_aes('cubic-in-out') +\r\n  geom_dl(aes(label=Location),\r\n          method=list(\"last.points\",rot=40))\r\n\r\n\r\n\r\nIf you would like to reproduce these codes, you can download the data from my folder.\r\nThe file name is Eurodiabetes.csv or simply click here.\r\nClick here for the codes.\r\nOnce you download the data, don’t forget to set your working directory!\r\nThat is it all for today. I hope, you like it. See you in my next post.\r\nContact\r\nPlease mention MihiretuKebede1 if you tweet this post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-07-25-2020-07-25-diabetesprevalenceeurope/2020-07-25-diabetesprevalenceeurope_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2023-03-26T23:31:40+02:00",
    "input_file": "2020-07-25-diabetesprevalenceeurope.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-07-19-animating-scatter-plots/",
    "title": "Animating scatter plots",
    "description": "Creating `Gif` for scatter plots",
    "author": [
      {
        "name": "Mihiretu Kebede(PhD)",
        "url": {}
      }
    ],
    "date": "2020-07-21",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\nAnimating a scatter plot.\r\nGif is a fancy way of data vizualization. It facilitates communication, makes your audience active.\r\nWhat do I need to do to produce gif?\r\nFirst load the packages we need.\r\nIf you don’t have these packages install them.\r\nLoad the data\r\n\r\n\r\nlibrary(readxl)\r\ntype2pre <- read_excel(\"F:/github/githubwebsite/type2GBDcompare.xls\")\r\nView(type2pre)\r\n\r\n\r\nSource of data\r\nThe data is about prevalence of type 2 diabetes for 20 countries from 1990-2017\r\n\r\n\r\n# You can downloaded such data from the IHME GBD compare websisite. You can select any indicator(prevalence, DALYs, mortality, etc). Check their website [GBD](https://vizhub.healthdata.org/gbd-compare/) \r\n\r\n#We will plot prevalence \r\nis.factor(type2pre$Year) #to check whether year is coded as numeric variable\r\n\r\n[1] FALSE\r\n\r\ntype2pre$yearfactor <- factor(type2pre$Year) #To save year as factor variable\r\ntype2pre$Yearnum <- as.numeric(type2pre$Year) #To save year as numeric variable\r\n\r\n\r\nInstall the necessary packages\r\nTo install the latest version of the packages\r\nlibrary(devtools)\r\nNB if you don’t have the necessary packages, you need to install them with the following codes\r\ndevtools::install_github(\"thomasp85/gganimate\")\r\ndevtools::install_github(\"thomasp85/transformr\") transformr is a dependency that helps you with transitions for polygons and lines.\r\nLoad the necessary packages\r\n\r\n\r\nlibrary(ggplot2) # for plotting \r\nlibrary(gganimate) #for animating your plot\r\nlibrary(scales) # for scaling your x or y-axis \r\n\r\n\r\nThe plot\r\n\r\n\r\n#Basic ggplot\r\nmyplot <- ggplot(type2pre, aes(x=Yearnum, y=Prevalence)) + \r\n  geom_point(aes(color=Location, size=3))\r\n\r\nmyplot\r\n\r\n\r\n## the gganimate part of code\r\nanimateplot <- myplot + transition_time(Yearnum) +\r\n  shadow_mark() + scale_x_continuous(limits = c(1990,2017)) + \r\n  xlab(\"Year\") + ylab(\"Prevalence per 100,000\")\r\n\r\n##NB: The shadow_mark() function is to include the previous plots in the scatter plot.Then each point moves across the variable that passess through the transition_time() function\r\n## the animation is done by passing the ggplot on the animate function of gganimate\r\nanimate(animateplot, width = 700, height = 500)\r\n\r\n\r\n\r\n*There are a lot you can do with gganimate. Google for more!\r\nReferences\r\nhttps://github.com/thomasp85/gganimate#old-api\r\nhttps://paldhous.github.io/ucb/2018/dataviz/week14.html\r\nContact\r\nPlease mention MihiretuKebede1 if you tweet this post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-07-19-animating-scatter-plots/07-19-2020-animating-scatter-plots_files/figure-html5/setup-1.png",
    "last_modified": "2023-03-26T21:32:59+02:00",
    "input_file": "07-19-2020-animating-scatter-plots.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-07-13-Animating-bar-graphs_files/",
    "title": "Animating bar graphs",
    "description": "Here we will have a quick look on how to create `Gif` or animated bar plots",
    "author": [
      {
        "name": "Mihiretu Kebede",
        "url": {}
      }
    ],
    "date": "2020-07-13",
    "categories": [
      "Data Science"
    ],
    "contents": "\r\n\r\n\r\nHello, this is my first ever blog post. Detail explanations of this data vizualization and other upcoming blogs are coming soon! Stay tuned.\r\n\r\n\r\n#Set working directory\r\n#setwd(\"F:/github/githubwebsite\")\r\n\r\nlibrary(readxl)\r\nmystepcount <- read_excel(\"F:/github/githubwebsite/mystepcount.xlsx\")\r\n\r\nstr(mystepcount)\r\n\r\ntibble [8 x 4] (S3: tbl_df/tbl/data.frame)\r\n $ Date       : chr [1:8] \"Sept_14\" \"Sept_15\" \"Sept_16\" \"Sept_17\" ...\r\n $ September  : num [1:8] 14 15 16 17 18 19 20 21\r\n $ Steps      : num [1:8] 1907 18767 10438 23128 28300 ...\r\n $ Killometres: num [1:8] 1.1 12.8 7.4 17.1 19.1 12.9 13.7 4.9\r\n\r\nlibrary(ggplot2)\r\nlibrary(gganimate)\r\nlibrary(scales)\r\ntheme_set(theme_bw())\r\n\r\nmystepcount$September <- as.character(mystepcount$September)\r\n\r\n\r\n\r\n\r\n#Plot step counts on the y-axis and days of September on the x-axis\r\nggplot(mystepcount, aes(x=September,y=Steps)) + \r\n  geom_bar(fill=\"darkturquoise\", stat=\"identity\", width=1, position=position_dodge()) + \r\n    geom_hline(yintercept = 10000, col=\"black\", size=1)\r\n\r\n\r\n\r\n\r\n\r\nanimate(\r\n  ggplot(mystepcount, aes(x=September,y=Steps)) + \r\n  geom_bar(fill=\"darkturquoise\", stat=\"identity\", width=1, position=position_dodge()) + \r\n    geom_hline(yintercept = 10000, col=\"black\", size=1) +\r\n  transition_states(September, wrap = FALSE) +\r\n  shadow_mark() +\r\n  enter_grow() +\r\n  enter_fade() + \r\n  theme(axis.text = element_text(size=16),\r\n        axis.title = element_text(size=16, face=\"bold\")) + \r\n  xlab(\"September\") +  \r\n  scale_y_discrete(name=\"Step counts\", limits=factor(c(5000,10000, 15000, 20000, 25000, 30000))) +\r\n  theme(axis.line = element_line(colour = \"black\"),\r\n          panel.border = element_blank()) ,\r\n  \r\n  duration = 5\r\n  )\r\n\r\n\r\n\r\nContact\r\nPlease mention MihiretuKebede1 if you tweet this post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-07-13-Animating-bar-graphs_files/2020-07-13-Animating-bar-graphs_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-03-26T21:38:37+02:00",
    "input_file": "2020-07-13-Animating-bar-graphs.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
